	TI	AB	DI	PY
0	Combining techniques to optimize effort predictions in software project management	"This paper tackles two questions related to software effort prediction. First, is it valuable to combine prediction techniques? Second, if so, how? Many commentators have suggested the use of more than one technique in order to support effort prediction, but to date there has been little or no empirical investigation to support this recommendation. Our analysis of effort data from a medical records information system reveals that there is little, or even negative, covariance between the accuracy of our three chosen prediction techniques, namely, expert judgment, least squares regression and case-based reasoning. This indicates that when one technique predicts poorly, one or both of the others tends to perform significantly better. This is a particularly striking result given the relative homogeneity of our data set. Consequently, searching for the single ""best"" technique, at least in this case, leads to a suboptimal prediction strategy. The challenge then becomes one of identifying a means of determining a priori which prediction technique to use. Unfortunately, despite using a range of techniques including rule induction, we were unable to identify any simple mechanism for doing so. Nevertheless, we believe this remains an important research goal. (C) 2002 Elsevier Science Inc. All rights reserved."	10.1016/S0164-1212(02)00067-5	2003, N,Y,Y
1	On using planning poker for estimating user stories	"While most studies in psychology and forecasting stress the possible hazards of group processes when predicting effort and schedule, agile software development methods recommend the use of a group estimation technique called planning poker for estimating the size of user stories and developing release and iteration plans. It is assumed that the group discussion through planning poker helps in identifying activities that individual estimators could overlook, thus providing more accurate estimates and reducing the over-optimism that is typical for expert judgment-based methods. In spite of the widespread use of agile methods, there is little empirical evidence regarding the accuracy of planning poker estimates. In order to fill this gap a study was conducted requiring 13 student teams to develop a Web-based student records information system. All teams were given the same set of user stories which had to be implemented in three Sprints. Each team estimated the stories using planning poker and the estimates provided by each team member during the first round were averaged to obtain the statistical combination for further comparison. In the same way the stories were estimated by a group of experts. The study revealed that students' estimates were over-optimistic and that planning poker additionally increased the over-optimism. On the other hand, the experts' estimates obtained through planning poker were much closer to actual effort spent and tended to be more accurate than the statistical combination of their individual estimates. The results indicate that the optimism bias caused by group discussion diminishes or even disappears as the expertise of the people involved in the group estimation process increases. (C) 2012 Elsevier Inc. All rights reserved."	10.1016/j.jss.2012.04.005	2012,N,N,N
2	Group processes in software effort estimation	"The effort required to complete software projects is often estimated, completely or partially, using the judgment of experts, whose assessment may be biased. In general, such bias as there is seems to be towards estimates that are overly optimistic. The degree of bias varies from expert to expert, and seems to depend on both Conscious and unconscious processes. One possible approach to reduce this bias towards over-optimism is to combine the judgments of several experts. This paper describes ail experiment in which experts with different backgrounds combined their estimates in group discussion. First, 20 software professionals were asked to provide individual estimates of the effort required for a software development project. Subsequently, they formed five estimation groups, each consisting of four experts. Each of these groups agreed oil a project effort estimate via the pooling of knowledge in discussion. We found that the groups submitted less optimistic estimates than the individuals. Interestingly, the group discussion-based estimates were closer to the effort expended on the actual project than the average of the individual expert estimates were, i.e., the group discussions led to better estimates than a mechanical averaging of the individual estimates. The groups' ability to identify a greater number of the activities required by the project is among the possible explanations for this reduction of bias."	10.1023/B:EMSE.0000039882.39206.5a	2004,N,Y,Y
3	Evidence-based guidelines for assessment of software development cost uncertainty	"Several studies suggest that uncertainty assessments of software development costs are strongly biased toward overconfidence, i.e., that software cost estimates typically are believed to be more accurate than they really are. This overconfidence may lead to poor project planning. As a means of improving cost uncertainty assessments, we provide evidence-based guidelines for how to assess software development cost uncertainty, based on results from relevant empirical studies. The general guidelines provided are: 1) Do not rely solely on unaided, intuition-based uncertainty assessment processes, 2) do not replace expert judgment with formal uncertainty assessment models, 3) apply structured and explicit judgment-based processes, 4) apply strategies based on an outside view of the project, 5) combine uncertainty assessments from different sources through group work, not through mechanical combination, 6) use motivational mechanisms with care and only if greater effort is likely to lead to improved assessments, and 7) frame the assessment problem to fit the structure of the relevant uncertainty information and the assessment process. These guidelines are preliminary and should be updated in response to new evidence."	10.1109/TSE.2005.128	2005,N,N,N
5	"The tool for the automatic analysis of text cohesion (TAACO): Automatic assessment of local, global, and text cohesion"	"This study introduces the Tool for the Automatic Analysis of Cohesion (TAACO), a freely available text analysis tool that is easy to use, works on most operating systems (Windows, Mac, and Linux), is housed on a user's hard drive (rather than having an Internet interface), allows for the batch processing of text files, and incorporates over 150 classic and recently developed indices related to text cohesion. The study validates TAACO by investigating how its indices related to local, global, and overall text cohesion can predict expert judgments of text coherence and essay quality. The findings of this study provide predictive validation of TAACO and support the notion that expert judgments of text coherence and quality are either negatively correlated or not predicted by local and overall text cohesion indices, but are positively predicted by global indices of cohesion. Combined, these findings provide supporting evidence that coherence for expert raters is a property of global cohesion and not of local cohesion, and that expert ratings of text quality are positively related to global cohesion."	10.3758/s13428-015-0651-7	2016,N,N,N
8	Probabilistic inversion for chicken processing lines	"We discuss an application of probabilistic inversion techniques to a model of campylobacter transmission in chicken processing lines. Such techniques are indicated when we wish to quantify a model which is new and perhaps unfamiliar to the expert community. In this case there are no measurements for estimating model parameters, and experts are typically unable to give a considered judgment. In such cases, experts are asked to quantify their uncertainty regarding variables which can be predicted by the model. The experts' distributions (after combination) are then pulled back onto the parameter space of the model, a process termed ""probabilistic inversion"". This study illustrates two such techniques, iterative proportional fitting (IPF) and PARmeter fitting for uncertain models (PARFUM). In addition, we illustrate how expert judgement on predicted observable quantities in combination with probabilistic inversion may be used for model validation and/or model criticism. (c) 2005 Elsevier Ltd. All rights reserved."	10.1016/j.ress.2005.11.054	2006
9	"Hybridising Human Judgment, AHP, Grey Theory, and Fuzzy Expert Systems for Candidate Well Selection in Fractured Reservoirs"	"The selection of appropriate wells for hydraulic fracturing is one of the most important decisions faced by oilfield engineers. It has significant implications for the future development of an oilfield in terms of its productivity and economics. In this study, we developed a fuzzy model for well selection that combines the major objective criteria with the subjective judgments of decision makers. This was done by fusing the analytic hierarchy process (AHP) method, grey theory and an advanced version of fuzzy logic theory (FLT). The AHP component was used to identify the relevant criteria involved in selecting wells for hydraulic fracturing. Grey theory was used to determine the relative importance of those criteria. Then a fuzzy expert system was applied to fuzzily process the aggregated inputs using a Type-2 fuzzy logic system. This undertakes approximate reasoning and generates recommendations for candidate wells. These techniques and technologies were hybridized by using an intercommunication job-sharing method that integrates human judgment. The proposed method was tested on data from an oilfield in Western China and finally the most appropriate candidate wells for hydraulic fracturing were ranked in order of their projected output after fracturing."	10.3390/en10040447	2017,N,Y,N
11	Classification and clinical diagnosis of cutaneous vasculitides	"The definition, diagnostic criteria and classification of systemic vasculitides, of which cutaneous vasculitides (CV) are a part, have long been discussed by the medical scientific world. The most significant contribution is due to the consensus-based criteria specifically derived by the combination of judgments from groups of experts, after accurate literature reviews and developed using consensus techniques. First of them came from the American College of Rheumatology (ACR) in 1990. In 1994 the Chapel Hill International Consensus Conference (CHCC) produced the Consensus-based Criteria essentially providing proper nomenclature for systemic vasculitis, which has been modified in 2012 by the CHCC2012. Moreover, in 2006 European League against Rheumatism and Pediatric Rheumatology European Society produced consensus criteria for the classification of childhood vasculitis. In CHCC2012 CV, affecting small vessels with a predominant skin involvement, have been included in both small vessel vasculitis and single organ vasculitis. The general characteristics of so-called CV have been described (epidemiology, clinical features, histopathology and etiopathogenesis) and, finally, the major characteristics of each clinical type of CV as well as their diagnostic criteria currently available in the literature have been reported."	0	2015,N,N,N
13	Policy capturing with ridge regression	"A policy capturing method combining human judgment with ridge regression is offered which results in superior judgment policy models. The new method (termed smart ridge regression) was tested against four others in seven judgment policy capturing applications. Performance criteria were two cross-validation indices: cross-validated multiple correlation and mean squared error of prediction of new judgments. Smart ridge regression was found to outperform ordinary least squares regression and conventional ridge regression, as well as subjective weighting and equal weighting of cues. (C) 1996 Academic Press, Inc."	10.1006/obhd.1996.0097	1996,N,N,N
14	"Use of expert knowledge to anticipate the future: Issues, analysis and directions"	"Unless an anticipation problem is routine and short-term, and objective data are plentiful, expert judgment will be needed. Risk assessment is analogous to anticipating the future, in that models need to be developed and applied to data. Since objective data are often scanty, expert knowledge elicitation (EKE) techniques have been developed for risk assessment that allow models to be developed and parametrized using expert judgment with minimal cognitive and social biases. Here, we conceptualize how EKE can be developed and applied to support anticipation of the future. Accordingly, we begin by defining EKE as a complete process, which involves considering experts as a source of data, and comprises various methods for ensuring the quality of this data, including selecting the best experts, training experts in the normative aspects of anticipation, and combining judgments from several experts, as well as eliciting unbiased estimates and constructs from experts. We detail various aspects of the papers that constitute this special issue and analyse them in terms of the stages of the EKE future-anticipation process that they address. We also identify the remaining gaps in our knowledge. Our conceptualization of EKE with the aim of supporting anticipation of the future is compared and contrasted with the extant research on judgmental forecasting."	10.1016/j.ijforecast.2016.11.001	2017,N,Y,Y
17	A hybrid econometric-AI ensemble learning model for Chinese foreign trade prediction	"Due to the complexity of economic system, the interactive effects of economic variables or factors on Chinese foreign trade make the prediction of China's foreign trade extremely difficult. To analyze the relationship between economic variables and foreign trade, this study proposes a novel nonlinear ensemble learning methodology hybridizing nonlinear econometric model and artificial neural networks (ANN) for Chinese foreign trade prediction. In this proposed learning approach, an important econometrical model, the co-integration-based error correction vector auto-regression (EC-VAR) model is first used to capture the impacts of the economic variables on Chinese foreign trade from a multivariate analysis perspective. Then an ANN-based EC-VAR model is used to capture the nonlinear patterns hidden between foreign trade and economic factors. Subsequently, for introducing the effects of irregular events on foreign trade, the text mining and expert's judgmental adjustments are also incorporated into the nonlinear ANN-based EC-VAR model. Finally, all economic variables, the outputs of linear and nonlinear EC-VAR models and judgmental adjustment model are used as another neural network inputs for ensemble prediction purpose. For illustration, the proposed ensemble learning methodology integrating econometric techniques and artificial intelligence (AI) methods is applied to Chinese export trade prediction problem."	0	2007,N,N,N
20	Bayesian network models for making maintenance decisions from data and expert judgment	"To maximize asset reliability cost-effectively, maintenance should be scheduled based on the likely deterioration of an asset. A number of types of statistical model have been proposed for predicting this but they have important practical limitations. We present a Bayesian network model that can be used for maintenance decision support to overcome these limitations. The model extends an existing statistical model of asset deterioration, but shows how i) failure data from related groups of asset can be combined, ii) data on the condition of assets available from their periodic inspection can be used iii) expert knowledge of the causes deterioration can be combined with statistical data to adjust predictions and iv) the uncertain effects of maintenance actions can be modelled. We show how the model could be used for a range of decision problems, given typical data likely to be available in practice."	0	2017,N,Y,Y
0	A review of studies on expert estimation of software development effort	"This paper provides an extensive review of studies related to expert estimation of software development effort. The main goal and contribution of the review is to support the research on expert estimation, e.g., to ease other researcher's search for relevant expert estimation studies. In addition, we provide software practitioners with useful estimation guidelines, based on the research-based knowledge of expert estimation processes. The review results suggest that expert estimation is the most frequently applied estimation strategy for software projects, that there is no substantial evidence in favour of use of estimation models, and that there are situations where we can expect expert estimates to be more accurate than formal estimation models. The following 12 expert estimation ""best practice"" guidelines are evaluated through the review: (1) evaluate estimation accuracy, but avoid high evaluation pressure; (2) avoid conflicting estimation goals; (3) ask the estimators to justify and criticize their estimates; (4) avoid irrelevant and unreliable estimation information; (5) use documented data from previous development tasks; (6) find estimation experts with relevant domain background and good estimation records; (7) Estimate top-down and bottom-up, independently of each other; (8) use estimation checklists; (9) combine estimates from different experts and estimation strategies; (10) assess the uncertainty of the estimate; (11) provide feedback on estimation accuracy and development task relations; and, (12) provide estimation training opportunities. We found supporting evidence for all 12 estimation principles, and provide suggestions on how to implement them in software organizations. (C) 2002 Elsevier Inc. All rights reserved."	10.1016/S0164-1212(02)00156-5	2004,N,Y,N
5	Computational intelligence for heart disease diagnosis: A medical knowledge driven approach	"This paper investigates a number of computational intelligence techniques in the detection of heart disease. Particularly, comparison of six well known classifiers for the well used Cleveland data is performed. Further, this paper highlights the potential of an expert judgment based (i.e., medical knowledge driven) feature selection process (termed as MFS), and compare against the generally employed computational intelligence based feature selection mechanism. Also, this article recognizes that the publicly available Cleveland data becomes imbalanced when considering binary classification. Performance of classifiers. and also the potential of MFS are investigated considering this imbalanced data issue. The experimental results demonstrate that the use of MFS noticeably improved the performance, especially in terms of accuracy, for most of the classifiers considered and for majority of the datasets (generated by converting the Cleveland dataset for binary classification). MFS combined with the computerized feature selection process (CFS) has also been investigated and showed encouraging results particularly for NaiveBayes, IBK and SMO. In summary, the medical knowledge based feature selection method has shown promise for use in heart disease diagnostics. (C) 2012 Elsevier Ltd. All rights reserved."	10.1016/j.eswa.2012.07.032	2013,N,Y,Y
8	Calibration and evaluation of five indicators of benthic community condition in two California bay and estuary habitats	"Many types of indices have been developed to assess benthic invertebrate community condition, but there have been few studies evaluating the relative performance of different index approaches. Here we calibrate and compare the performance of five indices: the Benthic Response Index (BRI), Benthic Quality Index (BQI), Relative Benthic Index (RBI), River Invertebrate Prediction and Classification System (RIVPACS), and the Index of Biotic Integrity (1131). We also examine whether index performance improves when the different indices, which rely on measurement of different properties, are used in combination. The five indices were calibrated for two geographies using 238 samples from southern California marine bays and 125 samples from polyhaline San Francisco Bay. Index performance was evaluated by comparing index assessments of 35 sites to the best professional judgment of nine benthic experts. None of the individual indices performed as well as the average expert in ranking sample condition or evaluating whether benthic assemblages exhibited evidence of disturbance. However, several index combinations outperformed the average expert. When results from both habitats were combined, two four-index combinations and a three-index combination performed best. However, performance differences among several combinations were small enough that factors such as logistics can also become a consideration in index selection. (C) 2008 Elsevier Ltd. All rights reserved."	10.1016/j.marpolbul.2008.11.007	2009,N,N,Y
9	Statistical approaches in the development of clinical practice guidelines from expert panels - The case of laminectomy in sciatica patients	"BACKGROUND. Variation in expert opinion and lack of a systematic methodology hinder the development of reliable clinical practice guidelines. However standardized protocols have been defined to quantify, combine, and summarize expert judgments. In addition, statistical methods may help to outline guidelines based on simplified models of these judgments. METHODS. TO test this hypothesis, stepwise logistic regression (SLR) and classification tree pruning (CTP) were used to predict the results of two expert panels (USA 1992 and Switzerland 1995) on laminectomy in sciatica conditions. Both panels, using the RAND-UCLA explicit method, assessed whether the procedure would be inappropriate or of potential use in 720 case scenarios combining 7 relevant factors. RESULTS. Laminectomy was rated as inappropriate in 60% and 70% of the scenarios by the US and Swiss panels, respectively. Either statistical method, in both panels, based its simplest model on the same 4 factors, as follows: imaging test results; disability; neurological findings; and conservative treatment trials (in decreasing order); the influence of 2 other factors, duration of pain and nerve root irritation, were only marginal. The correct classification rates of the models were 89% and 93% for SLR and 93% and 85% for CTP. Adopting the CTP US algorithm as a guideline would lead to consider performing laminectomy only in patients with imaging evidence of hernia, relatively severe disability, reflex abnormalities, and previous nonsurgical treatment. Adherence to the corresponding CTP Swiss algorithm would result in less restrictive conditions. CONCLUSION. The statistical techniques proved as useful instruments to structure and simplify appropriateness criteria developed by expert panels and to outline parsimonious decision models for clinical practice."	10.1097/00005650-199908000-00008	1999,N,N,N
11	Risk of surgery for subacromial impingement syndrome in relation to neck-shoulder complaints and occupational biomechanical exposures: a longitudinal study	"Objectives The aim of this longitudinal study was to evaluate the risk of surgery for subacromial impingement syndrome (SIS) in relation to neck-shoulder complaints and occupational biomechanical shoulder exposures. Methods The study was based on the Musculoskeletal Research Database at the Danish Ramazzini Centre. We linked baseline questionnaire information from 1993-2004 on neck-shoulder complaints, job titles, psychosocial work factors, body mass index, and smoking with register information on first-time surgery for SIS from 1996-2008. Biomechanical exposure measures were obtained from a job exposure matrix based on expert judgment. We applied multivariable Cox regression. Results During 280 125 person-years of follow-up among 37 402 persons, 557 first-time operations for SIS occurred. Crude surgery rates increased from 1.1 to 2.5 per 1000 person-years with increasing shoulder load. Using no neck-shoulder complaints and low shoulder load at baseline as a reference, no neck-shoulder complaints and high shoulder load showed an adjusted hazard ratio (HRadj) of 2.55 [95% confidence interval (95% CI) 1.59-4.09], while neck-shoulder complaints in combination with high shoulder load showed an HRadj of 4.52 (95% CI 2.87-7.13). Subanalyses based on 18 856 persons showed an HRadj of 5.40 (95% CI 2.88-10.11) for complaints located specifically in the shoulder in combination with high shoulder load. Conclusions Based on these findings, persons with neck-shoulder and especially shoulder complaints in combination with high shoulder load seem an obvious target group for interventions aimed at reducing exposures to prevent surgery for SIS."	10.5271/sjweh.3374	2013,N,N,N
12	Modeling hypoxia in the Chesapeake Bay: Ensemble estimation using a Bayesian hierarchical model	"Quantifying parameter and prediction uncertainty in a rigorous framework can be an important component of model skill assessment. Generally, models with lower uncertainty will be more useful for prediction and inference than models with higher uncertainty. Ensemble estimation, an idea with deep roots in the Bayesian literature, can be useful to reduce model uncertainty. It is based on the idea that simultaneously estimating common or similar parameters among models can result in more precise estimates. We demonstrate this approach using the Streeter-Phelps dissolved oxygen sag model fit to 29 years of data from Chesapeake Bay. Chesapeake Bay has a long history of bottom water hypoxia and several models are being used to assist management decision-making in this system. The Bayesian framework is particularly useful in a decision context because it can combine both expert-judgment and rigorous parameter estimation to yield model forecasts and a probabilistic estimate of the forecast uncertainty. Published by Elsevier B.V."	10.1016/j.jmarsys.2008.05.008	2009,N,N,Y
13	Life cycle assessment of fuel cell vehicles - A methodology example of input data treatment for future technologies	"Life cycle assessment (LCA) will always involve some subjectivity and uncertainty. This reality is especially true when the analysis concerns new technologies. Dealing with uncertainty can generate richer information and minimize some of the result mismatches currently encountered in the literature. As a way of analyzing future fuel cell vehicles and their potential new fuels, the Fuel Upstream Energy and Emission Model (FUEEM) developed at the University of California - Davis, pioneered two different ways to incorporate uncertainty into the analysis. First, the model works with probabilistic curves as inputs and with Monte Carlo simulation techniques to propagate the uncertainties. Second, the project involved the interested parties in the entire process, not only in the critical review phase. The objective of this paper is to present, as a case study, the tools and the methodologies developed to acquire most of the knowledge held by interested parties and to deal with their eventually conflicted - interests. The analysis calculation methodology, the scenarios, and all assumed probabilistic curves were derived from a consensus of an international expert network discussion, using existing data in the literature along with new information collected from companies. The main part of the expert discussion process uses a variant of the Delphi technique, focusing on the group learning process through the information feedback feature. A qualitative analysis indicates that a higher level of credibility and a higher quality of information can be achieved through a more participatory process. The FUEEM method works well within technical information and also in establishing a reasonable set of simple scenarios. However, for a complex combination of scenarios, it will require some improvement. The time spent in the process was the major drawback of the method and some alternatives to share this time cost are suggested."	10.1065/lca2002.02.074	2002,N,N,N
15	Knowing the crowd within: Metacognitive limits on combining multiple judgments	"We investigated how decision-makers use multiple opportunities to judge a quantity. Decision-makers undervalue the benefit of combining their own judgment with an advisor's, but theories disagree about whether this bias would apply to combining several of one's own judgments. Participants estimated percentage answers to general knowledge questions (e.g., What percent of the world's population uses the Internet?) on two occasions. In a final decision phase, they selected their first, second, or average estimate to report for each question. We manipulated the cues available for this final decision. Given cues to general theories (the labels first guess, second guess, average), participants mostly averaged, but no more frequently on trials where the average was most accurate. Given item-specific cues (numerical values of the options), metacognitive accuracy was at chance. Given both cues, participants mostly averaged and switched strategies based on whichever yielded the most accurate value on a given trial. These results indicate that underappreciation of averaging estimates does not stem only from social differences between the self and an advisor and that combining general and item-specific cues benefits metacognition. (C) 2013 Elsevier Inc. All rights reserved."	10.1016/j.jml.2013.10.002	2014,Y,Y,Y
16	Fuzzy Inference-Enhanced VC-DRSA Model for Technical Analysis: Investment Decision Aid	"To support investment decision based on technical analysis (TA), this study aims to retrieve the knowledge or rules of various indicators by a hybrid soft computing model. Although the validity of TA has been examined extensively by various statistical methods in literature, previous studies mainly explored the effectiveness of each technical indicator separately; therefore, a practical approach that may consider the inconsistency of various technical indicators simultaneously and the down-side risk of an investment decision is still underexplored. Thus, a hybrid model-by constructing a variable consistency dominance-based rough set approach (VC-DRSA) information system with the fuzzy inference-enhanced discretization of signals-is proposed, to retrieve the imprecise patterns from commonly adopted technical indicators. At the first stage, the trading signals (i.e., buy, neutral, or sell) are preprocessed in two groups: straight-forward signals and complicated signals. The straight-forward technical indicators (i.e., for signals that are decided by precise rules) are suggested by domain experts, and the buy-in signals are simulated by several trading strategies to examine the outcomes of each indicator. As for those complicated signals (i.e., for signals that require imprecise judgments with perceived feeling of domain experts to identify patterns), a fuzzy inference technique is incorporated to enhance the discretization of signals; those signals are also simulated by the aforementioned trading strategies to obtain the corresponding results. At the second stage, the trading signals generated by each technical indicator and their pertinent results from the previous stage are combined for VC-DRSA modeling to gain decision rules. To illustrate the proposed model, the weighted average index of the Taiwan stock market was examined from mid/2002 to mid/2014, and a set of decision rules with nearly 80 % classification accuracy (both in the training and the testing sets) were obtained in this empirical case. The findings suggest that several technical indicators should be considered simultaneously, and the retrieved rules (knowledge) have practical implications for investors."	10.1007/s40815-015-0058-8	2015,N,Y,Y
19	Forecasting China's foreign trade volume with a kernel-based hybrid econometric-AI ensemble learning approach	"Due to the complexity of economic system and the interactive effects between all kinds of economic variables and foreign trade, it is not easy to predict foreign trade volume. However, the difficulty in predicting foreign trade volume is usually attributed to the limitation of many conventional forecasting models. To improve the prediction performance, the study proposes a novel kernel-based ensemble learning approach hybridizing econometric models and artificial intelligence (AI) models to predict China's foreign trade volume. In the proposed approach, an important econometric model, the co-integration-based error correction vector auto-regression (EC-VAR) model is first used to capture the impacts of all kinds of economic variables on Chinese foreign trade from a multivariate linear analysis perspective. Then an artificial neural network (ANN) based EC-VAR model is used to capture the nonlinear effects of economic variables on foreign trade from the nonlinear viewpoint. Subsequently, for incorporating the effects of irregular events on foreign trade, the text mining and expert's judgmental adjustments are also integrated into the nonlinear ANN-based EC-VAR model. Finally, all kinds of economic variables, the outputs of linear and nonlinear EC-VAR models and judgmental adjustment model are used as input variables of a typical kernel-based support vector regression (SVR) for ensemble prediction purpose. For illustration, the proposed kernel-based ensemble learning methodology hybridizing econometric techniques and AI methods is applied to China's foreign trade volume prediction problem. Experimental results reveal that the hybrid econometric-AI ensemble learning approach can significantly improve the prediction performance over other linear and nonlinear models listed in this study."	10.1007/s11424-008-9062-5	2008,N,N,N
20	Quantifying ecosystem quality by modeling multi-attribute expert opinion	"The evaluation of ecosystem quality is inherently subjective, requiring decisions about which variables to notice or measure, and how these variables are integrated into a coherent evaluation. Despite the central role of human judgment, few evaluation methods address the subjectivity that is inherent in their design. There are, however, advantages to directly using opinion to create an expert system where the metric is constructed around opinion data. These advantages include stakeholder inclusion and the encouragement of a dialogue of data-driven criticism rather than subjective counter-opinion. We create an expert system to express the quality of a grassland ecosystem in Australia. We use an ensemble of bagged regression trees trained on calibrated expert preference data, to model the perceived quality of this grassland using a set of eight site variables as inputs. The model provides useful predictions of grassland quality, producing predictions similar to real expert evaluations of independent synthetic test sites not used to train the model. We apply the model to real grassland sites ranging from pristine to highly degraded, and confirm that our model orders the sites according to their degree of modification. We demonstrate that the use of too few experts produces relatively poor results, and show that for our problem the use of data from over twenty experts is appropriate. The scaling approach we used to calibrate between-expert data is shown to be an appropriate mechanism for aggregating the opinions of multiple experts. The resultant model will be useful in many contexts, and can be used by managers as a tool to evaluate real sites. It can also be integrated into ecological models of change as a means of evaluating predicted changes, for example, as a measure of utility when combined with cost estimates. The basic approach demonstrated here is applicable to any ecosystem, and we discuss the opportunities and limitations of its wider use."	10.1890/14-1485.1	2015,N,Y,Y
22	Predicting Risk in Missions under Sea Ice with Autonomous Underwater Vehicles	"Autonomous Underwater Vehicles (AUVs) have a future as effective platforms for multi-disciplinary science research and monitoring in the polar oceans. However, operation under ice may involve significant risk to the vehicle. A risk assessment and management process that balances the risk appetite of the responsible owner with the reliability of the vehicle and the probability of loss has been proposed. A critical step in the process of assessing risk is based on expert judgment of the fault history of the vehicle, and what affect faults or incidents have on the probability of loss. However, this subjective expert judgment is sensitive to the nature of sea ice cover. In contrast to the simple, yet high risk, case of operation under an ice shelf, sea ice offers a complex risk environment. Furthermore, the risk is modified by the characteristics of the support vessel, especially its ice-breaking capability. We explore how the ASPeCt sea ice characterization protocol and probability distributions of ice thickness and concentration can be used within a rigorous process to quantify risk given a range of sea ice conditions and with ships of differing ice capabilities. A solution founded on a Bayesian Belief Network approach is proposed, where the results of the expert judgment elicitation is taken as a reference. The design of the network topology captures the causal effects of the environment separately on the vehicle and on the ship, and combines these to produce the output. Complementary expert knowledge is included within the conditional probability tables of the Bayesian Belief Network. Using expert judgment on the fault history of the Autosub3 vehicle and sea ice data gathered in the Arctic and Antarctic by its predecessor, Autosub2, examples are provided of how risk is modified by the sea ice environment."	10.1109/AUV.2008.5290536	2008,N,N,N
25	Stochastic control and optimization in retail distribution: an empirical study of a Korean fashion company	"Considerable research has recently been conducted on improving the business performance of the fashion industry through supply-chain streamlining. In addition, the accuracy enhancement of sales forecasting by using statistical methods and machine learning algorithms to minimize inventory and improve profitability has been investigated. However, few studies have focused on solving the initial distribution problem to reduce logistical costs and loss of sales opportunities. This study solves the mathematical problems related to initial distributions of fashion products using stochastic control and optimization by conducting an empirical study using real data from a leading Korean fashion company. The initial distribution of a small quantity of items among numerous shops and the distribution of special sizes produced in small quantities were examined. Monte-Carlo simulations and Lebesgue's convergence theorem were considered useful for determining initial distributions of stock produced in small quantities. Furthermore, optimal initial distributions can be achieved when experience-based expert judgment is combined with mathematical modeling using stochastic control and optimization."	10.1080/00405000.2018.1478268	2019,N,N,N
27	Forecasting container throughput with big data using a partially combined framework	"This study proposes a partially-combined forecasting framework for container throughput based on big data composed of structured historical data and unstructured data. Under the proposed framework, the structured data (the original time series) is firstly decomposed into linear component and nonlinear component. Seasonal auto-regression integrated moving average model (SARIMA) is adopted to capture and forecast the linear component, and a combined model, composed of least squares support vector regression (LSSVR) and artificial neural network (GP), is applied to modeling the nonlinear component. Next, unstructured data is analyzed by an expert system. With the synthesized expert judgment, the forecasts of linear and nonlinear components are integrated into a final forecast. For the illustration and verification purpose, an empirical study is conducted with the data of Qingdao Port. The results show that the model under the proposed framework significantly outperforms its competitive rivals."	0	2015,N,N,N
28	CORRECTION OF PREDICTION MODEL OUTPUT-APPLICATION TO GENERAL CORROSION MODEL	This paper applies a newly developed methodology to calibrate the corrosion model within a structural reliability analysis. The methodology combines data from experience (measurements and expert judgment) and prediction models to adjust the structural reliability models. Two corrosion models published in the literature have been used to demonstrate the technique used for the model calibration. One model is used as a prediction for a future degradation and a second one to represent the inspection recorded data. The results of the calibration process are presented and discussed.	10.3940/rina.ijme.2014.a4.310	2014,N,N,N
31	A Review of the Application of Analytic Hierarchy Process to the Planning and Operation of Electric Power Microgrids	"This paper reviews literature that identifies the need for decision-making associated with the design and operation of electric power grids including microgrids. In particular, it examines the current applications of analytic hierarchy process (AHP) as the decision-making tool for electric power grids and microgrids. AHP is a decision making tool based on expert judgments that has been successfully applied in design and operation for the power systems. One advantage of AHP is its ability to incorporate subjective constraints. The current application of A]HP to grid connected power systems includes selection of generating units, identifying protection system vulnerability, prioritizing line maintenance, determining DC deployment and configurations, analyzing energy planning under uncertainty, locating and sizing of VAR sources, value based budgeting, choosing dispatch scenarios, forecasting loads, integrated resource planning, and determining combined active and reactive dispatch AHP has been applied to islanded microgrids for load shedding and optimizing cost. This paper concludes with proposals for expanding the use of AHP for decision-making for islanded microgrids."	0	2008,N,N,N
32	Organizing new methods for erosion and sedimentation monitoring and control	"Historic and continuing land uses at the U.S. Army's Fort Carson and the Pinon Canyon Maneuver Site (PCMS) may degrade training lands and cause erosion and increased sediment loading of local waters. A team of experts from the U.S. Geological Survey, the USDA Agricultural Research Service's Southwest Watershed Research Center, and two offices of the USDA Natural Resources Conservation Service is identifying methods and projects that can be used by Army land managers to evaluate the Army's potential contribution to the regional sediment pollution problem. Projects include site reclamation to control erosion and sediment transport; monitoring of stream flow, climate, and sediment concentrations; prediction of soil erosion; and assessment of rangeland health. Although the team has coordinated several agency projects and improved the overall approach to erosion and sedimentation monitoring and control, new research and technology will ensure additional progress in the future. This will include quantifying the impacts of military land use and training activities and maintaining this information in databases to enable the development of measurement techniques and indicators of ecosystem status, change and damage. Integrated information systems are required to combine the power of modern databases, simulation modeling, and expert judgment in designing and evaluating erosion control measures in terms of erosion and sediment transport processes. Because environmental concerns do not conform to political or administrative boundaries, the Army/ARS/USGS/NRCS team shares ideas and resources to provide both the Army and the regional community with responsible environmental management. Fort Carson's multidisciplinary team adopts the challenge of change, demonstrates new directions in managing highly complex challenges, and validates the federal government's effectiveness and ability to be efficient."	0	2000,N,N,N