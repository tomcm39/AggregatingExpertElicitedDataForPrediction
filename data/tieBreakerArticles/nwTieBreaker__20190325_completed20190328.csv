,TI,AB,DI,PY,4,Statistical correction of judgmental point forecasts and decisions,"In many organizations point estimates labelled as 'forecasts' are produced by human judgment rather than statistical methods. However, when these estimates are subject to asymmetric loss they are, in fact, decisions because they involve the selection of a value with the objective of minimizing loss. While there are often considerable advantages in using judgment to arrive at these decisions the psychological demands of the task may mean that the resulting decisions are sub-optimal when compared with those resulting from a normative decision model. In these circumstances a combination of statistical methods and judgment may be superior. This paper suggests a procedure which involves the statistical correction of the original decision to obtain a forecast and the subsequent use of a mathematical model to identify the theoretically optimal decision in the light of this forecast. The application of the procedure to the monthly decisions of a manufacturing company suggests that it may offer the potential for achieving substantial improvements In many practical contexts. Copyright (C) 1996 Elsevier Science Ltd",10.1016/0305-0483(96)00028-X,1996,06,An application of judgment analysis to examination marking in psychology,"Statistical combinations of specific measures have been shown to be superior to expert judgment in several fields. In this study, judgment analysis was applied to examination marking to investigate factors that influenced marks awarded and contributed to differences between first and second markers. Seven markers in psychology rated 551 examination answers on seven 'aspects' for which specific assessment criteria had been developed to support good practice in assessment. The aspects were: addressing the question, covering the area, understanding, evaluation, development of argument, structure and organization, and clarity. Principal-components analysis indicated one major factor and no more than two minor factors underlying the seven aspects. Aspect ratings were used to predict overall marks, using multiple regression to 'capture' the marking policies of individual markers. These varied from marker to marker in terms of the numbers of aspect ratings that made independent contributions to the prediction of overall marks and the extent to which aspect ratings explained the variance in overall marks. The number of independently predictive aspect ratings, and the amount of variance in overall marks explained by aspect ratings, were consistently higher for first markers (question setters) than for second markers. Co-markers' overall marks were then used as an external criterion to test the extent to which a simple model consisting of the sum of the aspect ratings improved on overall marks in the prediction of co-markers marks. The model significantly increased the variance in co-markers' marks accounted for, but only for second markers, who had not taught the material and had not set the question. Further research is needed to develop the criteria and especially to establish the reliability and validity of specific aspects of assessment. The present results support the view that, for second markers at least, combined measures of specific aspects of examination answers may help to improve the reliability of marking.",10.1348/000712602760146233,2002,07,Identification of more risks can lead to increased over-optimism of and over-confidence in software development effort estimates,"Software professionals are, on average, over-optimistic about the required effort usage and over-confident about the accuracy of their effort estimates. A better understanding of the mechanisms leading to the over-optimism and over-confidence may enable better estimation processes and, as a consequence, better managed software development projects We hypothesize that there are situations where more work on risk identification leads to increased over-optimism and over-confidence in software development effort estimates, instead of the Intended improvement of realism. Four experiments with software professionals are conducted to test the hypothesis All four experiments provide results in support of the hypothesis. Possible explanations of the counter-intuitive finding relate to results from cognitive science on ""illusion-of-control"", ""cognitive accessibility"", ""the peak-end rule"" and ""risk as feeling"" Thorough work on risk identification is essential for many purposes and our results should not lead to less emphasis on this activity Our results do, however, suggest that it matters how risk identification and judgment-based effort estimation processes are combined A simple approach for better combination of risk identification work and effort estimation is suggested (C) 2009 Elsevier B V All rights reserved",10.1016/j.infsof.2009.12.002,2010,010,Bayesian reanalysis of the challenger O-ring data,"A Bayesian forecasting model is developed to quantify uncertainty about the postflight state of a field-joint primary O-ring (not damaged or damaged), given the O-ring temperature at the time of launch of the space shuttle Challenger in 1986. The crux of this problem is the enormous extrapolation that must be performed: 23 previous shuttle flights were launched at temperatures between 53 degrees F and 81 degrees F, but the next launch is planned at 31 degrees F. The fundamental advantage of the Bayesian model is its theoretic structure, which remains correct over the entire sample space of the predictor and that affords flexibility of implementation. A novel approach to extrapolating the input elements based on expert judgment is presented; it recognizes that extrapolation is equivalent to changing the conditioning of the model elements. The prior probability of O-ring damage can be assessed subjectively by experts following a nominal-interacting process in a group setting. The Bayesian model can output several posterior probabilities of O-ring damage, each conditional on the given temperature and on a different strength of the temperature effect hypothesis. A lower bound on, or a value of, the posterior probability can be selected for decision making consistently with expert judgment, which encapsulates engineering information, knowledge, and experience. The Bayesian forecasting model is posed as a replacement for the logistic regression and the nonparametric approach advocated in earlier analyses of the Challenger O-ring data. A comparison demonstrates the inherent deficiency of the generalized linear models for risk analyses that require (1) forecasting an event conditional on a predictor value outside the sampling interval, and (2) combining empirical evidence with expert judgment.",10.1111/j.1539-6924.2008.01081.x,2008,012,Accuracy gains of adding vote expectation surveys to a combined forecast of US presidential election outcomes,"In averaging forecasts within and across four-component methods (i.e. polls, prediction markets, expert judgment and quantitative models), the combined PollyVote provided highly accurate predictions for the US presidential elections from 1992 to 2012. This research note shows that the PollyVote would have also outperformed vote expectation surveys, which prior research identified as the most accurate individual forecasting method during that time period. Adding vote expectations to the PollyVote would have further increased the accuracy of the combined forecast. Across the last 90 days prior to the six elections, a five-component PollyVote (i.e. including vote expectations) would have yielded a mean absolute error of 1.08 percentage points, which is 7% lower than the corresponding error of the original four-component PollyVote. This study thus provides empirical evidence in support of two major findings from forecasting research. First, combining forecasts provides highly accurate predictions, which are difficult to beat for even the most accurate individual forecasting method available. Second, the accuracy of a combined forecast can be improved by adding component forecasts that rely on different data and different methods than the forecasts already included in the combination.",10.1177/2053168015570416,2015,115,Weighted Diagnostic Criteria for Developmental Dysplasia of the Hip,"Objective To establish clinical diagnostic criteria for developmental dysplasia of the hip (DDH) that model the practices of expert clinicians. Study design Of 23 clinical criteria for the diagnosis of DDH, ranked in order of diagnostic importance by international consensus, the 7 most highly ranked were placed in all possible combinations to create unique case vignettes. Twenty-six experts rated 52 vignettes for the presence of DDH. We modeled the data to determine which of the 7 criteria were associated with a clinician's opinion that the vignette represented DDH. From the resulting regression coefficients, for each vignette we calculated a probability of DDH. An independent panel rated the same vignettes using a visual analog scale response. We correlated the visual analog scale ratings with probabilities derived from the model. Results Our model identified 4 of 7 criteria as predictive of DDH (P < .001): Ortolani/Barlow test (beta = 3.26), limited abduction (beta = 1.48), leg length discrepancy (beta = 0.74), and first-degree family history of DDH (beta = 1.39). There was substantial correlation between the probability of DDH predicted by the model and that derived from an independent expert panel (r = 0.73; P < .001). Conclusion Weighted clinical criteria for inferring the likelihood of DDH produced consistent results in the judgment of 2 separate groups of experts. Using these weights, nonexperts could establish the probability of DDH in a manner approaching the practice of clinical experts.",10.1016/j.jpeds.2014.08.023,2014,116,Life-cycle performance of structures: combining expert judgment and results of inspection,"Current bridge management systems base decisions on the results of visual inspections. These systems consider visual inspection results as accurate and disregard any further information available. In the present study, the result of each inspection is considered as a random variable, dependent of a wide range of factors, that can be integrated with other sources of information, including expert judgment and results of other inspections. The combination of different sources of information results in reliable posterior information and allows more accurate predictions of future deterioration. In the present paper, performance of an existing structure is obtained in terms of the condition index, which describes the effects of deterioration as can be seen by an inspector, and the safety index, which measures the safety margin of the structure. The reduction in uncertainty associated with periodical inspections is considered through updating of performance profiles. The updating of the condition index is direct, as new information on this parameter is collected by the inspector. In terms of safety, however, only indirect information is collected and the uncertainty reduction associated with an inspection is significantly lower. Several realistic examples show the impact of inspections on the predicted life-cycle performance of structures.",0,2008,118,Development of a Group Judgment Process for Forecasts of Health Care Innovations,"IMPORTANCE Health care costs have increased substantially over the past few decades, in part owing to the development and diffusion of new medical treatments. Forecasting potential future technologic innovations can allow for more informed planning. OBJECTIVE To assess the predictive validity of a structured formal method for forecasting future technologic innovations in health care. DESIGN, SETTING, AND PARTICIPANTS This pilot study combined an untested, unvalidated combination of a consensus process and group judgment process to evaluate forecasts made in 2001 for technologic innovations by 2021 in Alzheimer disease (AD) and cardiovascular disease (CVD). Six experts in AD and 7 experts in CVD composed the judgment group. The study was conducted in 2017-2018. MAIN OUTCOMES AND MEASURES Year 2001 forecasts for 2021 that were judged by experts as being close to correct, directionally correct, or not correct, as well as innovations that occurred since 2001 that were not predicted. RESULTS Four forecasts of innovations in AD, each considered to be between 30% and 40% likely to be achieved by 2021, were judged to be close to correct. One forecast was considered to be directionally correct, with a likelihood of occurrence of 40%, in that it was overoptimistic. One innovation that occurred was missed: new imaging techniques (amyloid beta plaque and tau tangle positron emission tomographic imaging). Five forecasts of CVD innovations were considered to be at least 50% likely to occur by 2021, and of these, 2 were judged to be close to correct, 1 was judged as being directionally correct, and 2 were judged as being not correct (although in one of these forecasts, the overarching innovation has been achieved but with a different noninvasive imaging modality). Of 7 additional forecasts considered to be less likely to be achieved by 2021, 4 were judged to be close to correct and 3 were judged as being directionally correct. Two innovations occurred but were missed: transcatheter aortic valve replacement and cardiac resynchronization therapy. Across both conditions, 15 of 17 innovations forecasted were judged to be close to correct or directionally correct, 2 were judged to be incorrect, and there were 3 missed innovations. CONCLUSIONS AND RELEVANCE Expert elicitation provided a useful, but not fully accurate, lens into future innovation.",10.1001/jamanetworkopen.2018.5108,2018,019,A Novel Hybrid ABC-PSO Algorithm for Effort Estimation of Software Projects Using Agile Methodologies,"In modern software development processes, software effort estimation plays a crucial role. The success or failure of projects depends greatly on the accuracy of effort estimation and schedule results. Many studies focused on proposing novel models to enhance the accuracy of predicted results; however, the question of accurate estimation of effort has been a challenging issue with regards to researchers and practitioners, especially when it comes to projects using agile methodologies. This study aims at introducing a novel formula based on team velocity and story point factors. The parameters of this formula are then optimized by employing swarm optimization algorithms. We also propose an improved algorithm combining the advantages of the artificial bee colony and particle swarm optimization algorithms. The experimental results indicated that our approaches outperformed methods in other studies in terms of the accuracy of predicted results.",10.1515/jisys-2016-0294,2018,021,Probabilistic Inversion Techniques in Quantitative Risk Assessment for Power System Load Forecasting,"Expert judgment is frequently used to assess parameter values in quantitative risk assessment. Experts can however only be expected to assess observable quantities, not abstract model parameters. This means that we need a method for translating expert assessed uncertainties on model outputs into uncertainties on model parameter values. So we use Probabilistic Inversion (PI) method. The probability distribution on model parameters obtained in this way can be used in a variety of ways, but in particular in an uncertainty analysis or as a Bayes prior. In this paper probabilistic inversion problems are first defined, existing algorithms for solving such problems are also discussed and the algorithms based on iterative algorithms are introduced. Those computational algorithms have proven successful in various projects. Such techniques are indicated when we wish to quantify a model which is new and perhaps unfamiliar to the expert community. There are no measurements for estimating model parameters, and experts are typically unable to give a considered judgment. In such cases, experts are asked to quantify their uncertainty regarding variables which can be predicted by the model. Applications to power system load forecasting in NingXia province of China is discussed. This study illustrates two such techniques, Iterative Proportional Fitting (IPF) and PARmeter Fitting for Uncertain Models (PARFUM) which provide useful tools for the practicing quantities risk assessment. In addition, we also illustrate how expert judgment on predicted observable quantities in combination with probabilistic inversion may be used for model validation.",10.1109/ICINFA.2008.4608092,2008,122,Reliability of vibration predictions in civil engineering applications,"The reliability of vibration predictions distinguishes itself from other reliability problems because of the highly non-linear behavior of the underlying models. Over the last two years, a combination of four institutes in the Netherlands has studied the reliability in this type of predictions. For the sake of comparison, besides sophisticated computational prediction models, also simple empirical models and expert judgment was analyzed. The paper describes the experimental set-up and the results of the project. Conclusions are drawn about the reliability of the predictions and the reduction that may be achieved from an increase in model sophistication.",0,2003,023,An expert system to evaluate TCE sites for natural attenuation,An expert judgment site-screening tool has been developed to evaluate natural attenuation as a remedial option for sites with trichloroethene (TCE) contamination of ground water. This site-screening tool combines a causative model for natural attenuation and expert knowledge within a probabilistic causal inference framework known as a Bayesian Belief Network (BBN). The resulting expert system can be used to screen sites and to evaluate evidence about the adequacy of natural attenuation for a site or portion of a site. Comparisons between sites and between experts can help to identify key natural attenuation processes and information needs for more reliable prediction of natural attenuation.,0,1998,00,Reducing Overconfidence in the Interval Judgments of Experts,"Elicitation of expert opinion is important for risk analysis when only limited data are available. Expert opinion is often elicited in the form of subjective confidence intervals; however, these are prone to substantial overconfidence. We investigated the influence of elicitation question format, in particular the number of steps in the elicitation procedure. In a 3-point elicitation procedure, an expert is asked for a lower limit, upper limit, and best guess, the two limits creating an interval of some assigned confidence level (e.g., 80%). In our 4-step interval elicitation procedure, experts were also asked for a realistic lower limit, upper limit, and best guess, but no confidence level was assigned; the fourth step was to rate their anticipated confidence in the interval produced. In our three studies, experts made interval predictions of rates of infectious diseases (Study 1, n = 21 and Study 2, n = 24: epidemiologists and public health experts), or marine invertebrate populations (Study 3, n = 34: ecologists and biologists). We combined the results from our studies using meta-analysis, which found average overconfidence of 11.9%, 95% CI [3.5, 20.3] (a hit rate of 68.1% for 80% intervals)-a substantial decrease in overconfidence compared with previous studies. Studies 2 and 3 suggest that the 4-step procedure is more likely to reduce overconfidence than the 3-point procedure (Cohen's d = 0.61, [0.04, 1.18]).",10.1111/j.1539-6924.2009.01337.x,2010,12,Using Bayesian networks to model expected and unexpected operational losses,"This report describes the use of Bayesian networks (BNs) to model statistical loss distributions in financial operational risk scenarios. Its focus is on modeling ""long"" tail, or unexpected, loss events using mixtures of appropriate loss frequency and severity distributions where these mixtures are conditioned on causal variables that model the capability or effectiveness of the underlying controls process. The use of causal modeling is discussed from the perspective of exploiting local expertise about process reliability and formally connecting this knowledge to actual or hypothetical statistical phenomena resulting from the process. This brings the benefit of supplementing sparse data with expert judgment and transforming qualitative knowledge about the process into quantitative predictions. We conclude that BNs can help combine qualitative data from experts and quantitative data from historical loss databases in a principled way and as such they go some way in meeting the requirements of the draft Basel II Accord (Basel, 2004) for an advanced measurement approach (AMA).",10.1111/j.1539-6924.2005.00641.x,2005,03,"Scenarios, uncertainty and conditional forecasts of the world population","Current official population forecasts differ little from those that Whelpton made 50 years ago either in the cohort-component methodology used or in the arguments used to motivate the assumptions. However, Whelpton produced some of the most erroneous forecasts of this century. This suggests that current forecasters should ensure that they give users an assessment of the uncertainty of their forecasts. We show how simple statistical methods can be combined with expert judgment to arrive at an overall predictive distribution for the future population. We apply the methods to a world population forecast that was made in 1994. Accepting that point forecast, we find that the probability is only about 2% that the world population in the year 2030 will be less than the low scenario of 8317 million. The probability that the world population will exceed the high scenario of 10736 million is about 13%. Similarly, the probability is only about 51% that the high-low interval of a recent United Nations (UN) forecast will contain the true population in the year 2025. Even if we consider the UN high-low intervals as conditional on the possible future policies of its member states, they appear to have a relatively small probability of encompassing the future population.",10.1111/1467-985X.00046,1997,04,Role thinking: Standing in other people's shoes to forecast decisions in conflicts,"When forecasting decisions in conflict situations, experts are often advised to figuratively stand in the other person's shoes. We refer to this as ""role thinking"", because, in practice, the advice is to think about how other protagonists will view the situation in order to predict their decisions. We tested the effect of role thinking on forecast accuracy. We obtained 101 role-thinking forecasts of the decisions that would be made in nine diverse conflicts from 27 Naval postgraduate students (experts) and 107 role-thinking forecasts from 103 second-year organizational behavior students (novices). The accuracy of the novices' forecasts was 33% and that of the experts' was 31%; both were little different from chance (guessing), which was 28%. The small improvement in accuracy from role-thinking strengthens the finding from earlier research that it is not sufficient to think hard about a situation in order to predict the decisions which groups of people will make when they are in conflict. Instead, it is useful to ask groups of role players to simulate the situation. When groups of novice participants adopted the roles of protagonists in the aforementioned nine conflicts and interacted with each other, their group decisions predicted the actual decisions with an accuracy of 60%. (C) 2010 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.",10.1016/j.ijforecast.2010.05.001,2011,05,Methods for assessing uncertainty in fundamental assumptions and associated models for cancer risk assessment,"The distributional approach for uncertainty analysis in cancer risk assessment is reviewed and extended. The method considers a combination of bioassay study results, targeted experiments, and expert judgment regarding biological mechanisms to predict a probability distribution for uncertain cancer risks. Probabilities are assigned to alternative model components, including the determination of human carcinogenicity, mode of action, the dosimetry measure for exposure, the mathematical form of the dose-response relationship, the experimental data set(s) used to fit the relationship, and the formula used for interspecies extrapolation. Alternative software platforms for implementing the method are considered, including Bayesian belief networks (BBNs) that facilitate assignment of prior probabilities, specification of relationships among model components, and identification of all output nodes on the probability tree. The method is demonstrated using the application of Evans, Sielken, and co-workers for predicting cancer risk from formaldehyde inhalation exposure. Uncertainty distributions are derived for maximum likelihood estimate (MLE) and 95th percentile upper confidence limit (UCL) unit cancer risk estimates, and the effects of resolving selected model uncertainties on these distributions are demonstrated, considering both perfect and partial information for these model components. A method for synthesizing the results of multiple mechanistic studies is introduced, considering the assessed sensitivities and selectivities of the studies for their targeted effects. A highly simplified example is presented illustrating assessment of genotoxicity based on studies of DNA damage response caused by naphthalene and its metabolites. The approach can provide a formal mechanism for synthesizing multiple sources of information using a transparent and replicable weight-of-evidence procedure.",10.1111/j.1539-6924.2008.01134.x,2008,07,Probabilistic Inversion in Priority Setting of Emerging Zoonoses,"This article presents methodology of applying probabilistic inversion in combination with expert judgment in priority setting problem. Experts rank scenarios according to severity. A linear multi-criteria analysis model underlying the expert preferences is posited. Using probabilistic inversion, a distribution over attribute weights is found that optimally reproduces the expert rankings. This model is validated in three ways. First, consistency of expert rankings is checked, second, a complete model fitted using all expert data is found to adequately reproduce observed expert rankings, and third, the model is fitted to subsets of the expert data and used to predict rankings in out-of-sample expert data.",10.1111/j.1539-6924.2010.01378.x,2010,18,Can cancer researchers accurately judge whether preclinical reports will reproduce?,"There is vigorous debate about the reproducibility of research findings in cancer biology. Whether scientists can accurately assess which experiments will reproduce original findings is important to determining the pace at which science self-corrects. We collected forecasts from basic and preclinical cancer researchers on the first 6 replication studies conducted by the Reproducibility Project: Cancer Biology (RP: CB) to assess the accuracy of expert judgments on specific replication outcomes. On average, researchers forecasted a 75% probability of replicating the statistical significance and a 50% probability of replicating the effect size, yet none of these studies successfully replicated on either criterion (for the 5 studies with results reported). Accuracy was related to expertise: experts with higher h-indices were more accurate, whereas experts with more topic-specific expertise were less accurate. Our findings suggest that experts, especially those with specialized knowledge, were overconfident about the RP: CB replicating individual experiments within published reports; researcher optimism likely reflects a combination of overestimating the validity of original studies and underestimating the difficulties of repeating their methodologies.",10.1371/journal.pbio.2002212,2017,110,Judgmental selection of forecasting models,"In this paper, we explored how judgment can be used to improve the selection of a forecasting model. We compared the performance of judgmental model selection against a standard algorithm based on information criteria. We also examined the efficacy of a judgmental model-build approach, in which experts were asked to decide on the existence of the structural components (trend and seasonality) of the time series instead of directly selecting a model from a choice set. Our behavioral study used data from almost 700 participants, including forecasting practitioners. The results from our experiment suggest that selecting models judgmentally results in performance that is on par, if not better, to that of algorithmic selection. Further, judgmental model selection helps to avoid the worst models more frequently compared to algorithmic selection. Finally, a simple combination of the statistical and judgmental selections and judgmental aggregation significantly outperform both statistical and judgmental selections.",10.1016/j.jom.2018.05.005,2018,1