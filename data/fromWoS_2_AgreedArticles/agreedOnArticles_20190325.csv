,TI,AB,DI,PY,tcmRelated,gcgRelated,ngrRelated,nwRelated,reviewer,2ndReviewer,agree,include
0,"Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical-statistical controversy","Given a data set about an individual or a group (e.g., interviewer ratings, life history or demographic facts, test results, self-descriptions), there are two modes of data combination for a predictive or diagnostic purpose. The clinical method relies on human judgment that is based on informal contemplation and, sometimes, discussion with others (e.g., case conferences). The mechanical method involves a formal, algorithmic, objective procedure (e.g., equation) to reach the decision. Empirical comparisons of the accuracy of the two methods (136 studies over a wide range of predictands) show that the mechanical method is almost invariably equal to or superior to the clinical method: Common antiactuarial arguments are rebutted, possible causes of widespread resistance to the comparative research are offered, and policy implications of the statistical method's superiority are discussed.",10.1037/1076-8971.2.2.293,1996,1,1.0,0.0,0.0,gcg,1.0,1,1
1,Combining probability distributions from experts in risk analysis,"This paper concerns the combination of experts' probability distributions in risk analysis, discussing a variety of combination methods and attempting to highlight the important conceptual and practical issues to be considered in designing a combination process in practice. The role of experts is important because their judgments can provide valuable information, particularly in view of the limited availability of ""hard data"" regarding many important uncertainties in risk analysis. Because uncertainties are represented in terms of probability distributions in probabilistic risk analysis (PRA), we consider expert information in terms of probability distributions. The motivation for the use of multiple experts is simply the desire to obtain as much information as possible. Combining experts' probability distributions summarizes the accumulated information for risk analysts and decision-makers. Procedures for combining probability distributions are often compartmentalized as mathematical aggregation methods or behavioral approaches, and we discuss both categories. However, an overall aggregation process could involve both mathematical and behavioral aspects, and no single process is best in all circumstances. An understanding of the pros and cons of different methods and the key issues to consider is valuable in the design of a combination process for a specific PRA. The output, a ""combined probability distribution,"" can ideally be viewed as representing a summary of the current state of expert opinion regarding the uncertainty of interest.",10.1111/j.1539-6924.1999.tb00399.x,1999,1,0.0,0.0,1.0,nw,1.0,1,1
2,"HUMAN FACIAL BEAUTY - AVERAGENESS, SYMMETRY, AND PARASITE RESISTANCE","It is hypothesized that human faces judged to be attractive by people possess two features-averageness and symmetry-that promoted adaptive mate selection in human evolutionary history by way of production of offspring with parasite resistance. Facial composites made by combining individual faces are judged to be attractive, and more attractive than the majority of individual faces. The composites possess both symmetry and averageness of features. Facial averageness may reflect high individual protein heterozygosity and thus an array of proteins to which parasites must adapt. Heterozygosity may be an important defense of long-lived hosts against parasites when it occurs in portions of.the genome that do not code for the essential features of complex adaptations. In this case heterozygosity can create a hostile microenvironment for parasites without disrupting adaptation. Facial bilateral symmetry is hypothesized to affect positive beauty judgments because symmetry is a certification of overall phenotypic quality and developmental health, which may be importantly influenced by parasites. Certain secondary sexual traits are influenced by testosterone, a hormone that reduces immunocompetence. Symmetry and size of the secondary sexual traits of the face (e.g., cheek bones) are expected to correlate positively and advertise immunocompetence honestly and therefore to affect positive beauty judgments. Facial attractiveness is predicted to correlate with attractive, nonfacial secondary sexual traits; other predictions from the view that parasite-driven selection led to the evolution of psychological adaptations of human beauty perception are discussed. The view that human physical attractiveness and judgments about human physical attractiveness evolved in the context of parasite-driven selection leads to the hypothesis that both adults and children have a species-typical adaptation to the problem of identifying and favoring healthy individuals and avoiding parasite-susceptible individuals. It is proposed that this adaptation guides human decisions about nepotism and reciprocity in relation to physical attractiveness.",10.1007/BF02692201,1993,0,0.0,0.0,0.0,ngr,0.0,1,0
3,Eliciting Expert Knowledge in Conservation Science,"Expert knowledge is used widely in the science and practice of conservation because of the complexity of problems, relative lack of data, and the imminent nature of many conservation decisions. Expert knowledge is substantive information on a particular topic that is not widely known by others. An expert is someone who holds this knowledge and who is often deferred to in its interpretation. We refer to predictions by experts of what may happen in a particular context as expert judgments. In general, an expert-elicitation approach consists of five steps: deciding how information will be used, determining what to elicit, designing the elicitation process, performing the elicitation, and translating the elicited information into quantitative statements that can be used in a model or directly to make decisions. This last step is known as encoding. Some of the considerations in eliciting expert knowledge include determining how to work with multiple experts and how to combine multiple judgments, minimizing bias in the elicited information, and verifying the accuracy of expert information. We highlight structured elicitation techniques that, if adopted, will improve the accuracy and information content of expert judgment and ensure uncertainty is captured accurately. We suggest four aspects of an expert elicitation exercise be examined to determine its comprehensiveness and effectiveness: study design and context, elicitation design, elicitation method, and elicitation output. Just as the reliability of empirical data depends on the rigor with which it was acquired so too does that of expert knowledge.",10.1111/j.1523-1739.2011.01806.x,2012,1,0.0,0.0,1.0,nw,1.0,1,1
4,Do humans optimally integrate stereo and texture information for judgments of surface slant?,"An optimal linear system for integrating Visual cues to 3D surface geometry weights Cues ill inverse proportion to their uncertainty. The problem of integrating texture and stereo information for judgments of planar surface slant provides a strong test of optimality in human perception. Since the accuracy of slant from texture judgments changes by an order of magnitude from low to high slants, optimality predicts corresponding changes in cue weights as a function Of Surface slant. Furthermore, Since humans show significant individual differences in their abilities to use both texture and stereo information for judgments of 3D Surface geometry, the problem admits the stronger test that individual differences in subjects' thresholds for discriminating slant from the individual cues should predict individual differences in cue weights. We tested both predictions by measuring slant discrimination thresholds and stereo/texture cue weights as a function of surface slant for multiple subjects. The results bear Out both predictions of optimality, with the exception of an apparent slight under-weighting Of texture information. This may be accounted for by factors specific to the Stimuli used to isolate stereo information in the experiments. Taken together, the results are consistent with the hypothesis that humans optimally combine the two cues to surface slant, with Cue weights proportional to the subjective reliability of the cues. (C) 2003 Elsevier Ltd. All rights reserved.",10.1016/S0042-6989(03)00458-9,2003,0,0.0,0.0,0.0,gcg,0.0,1,0
5,A review of studies on expert estimation of software development effort,"This paper provides an extensive review of studies related to expert estimation of software development effort. The main goal and contribution of the review is to support the research on expert estimation, e.g., to ease other researcher's search for relevant expert estimation studies. In addition, we provide software practitioners with useful estimation guidelines, based on the research-based knowledge of expert estimation processes. The review results suggest that expert estimation is the most frequently applied estimation strategy for software projects, that there is no substantial evidence in favour of use of estimation models, and that there are situations where we can expect expert estimates to be more accurate than formal estimation models. The following 12 expert estimation ""best practice"" guidelines are evaluated through the review: (1) evaluate estimation accuracy, but avoid high evaluation pressure; (2) avoid conflicting estimation goals; (3) ask the estimators to justify and criticize their estimates; (4) avoid irrelevant and unreliable estimation information; (5) use documented data from previous development tasks; (6) find estimation experts with relevant domain background and good estimation records; (7) Estimate top-down and bottom-up, independently of each other; (8) use estimation checklists; (9) combine estimates from different experts and estimation strategies; (10) assess the uncertainty of the estimate; (11) provide feedback on estimation accuracy and development task relations; and, (12) provide estimation training opportunities. We found supporting evidence for all 12 estimation principles, and provide suggestions on how to implement them in software organizations. (C) 2002 Elsevier Inc. All rights reserved.",10.1016/S0164-1212(02)00156-5,2004,0,0.0,0.0,1.0,nw,1.0,0,0
6,Intuitions about combining opinions: Misappreciation of the averaging principle,"Averaging estimates is an effective way to improve accuracy when combining expert judgments, integrating group members' judgments, or using advice to modify personal judgments. If the estimates of two judges ever fall on different sides of the truth, which we term bracketing, averaging must outperform the average judge for convex loss functions, such as mean absolute deviation (MAD). We hypothesized that people often hold incorrect beliefs about averaging, falsely concluding that the average of two judges' estimates would be no more accurate than the average judge. The experiments confirmed that this misconception was common across a range of tasks that involved reasoning from summary data (Experiment 1), from specific instances (Experiment 2), and conceptually (Experiment 3). However, this misconception decreased as observed or assumed bracketing rate increased (all three studies) and when bracketing was made more transparent (Experiment 2). Experiment 4 showed that flawed inferential rules and poor extensional reasoning abilities contributed to the misconception. We conclude by describing how people may face few opportunities to learn the benefits of averaging and how misappreciating averaging contributes to poor intuitive strategies for combining estimates.",10.1287/mnsc.1050.0459,2006,1,0.0,0.0,1.0,nw,1.0,1,1
7,Use (and abuse) of expert elicitation in support of decision making for public policy,"The elicitation of scientific and technical judgments from experts, in the form of subjective probability distributions, can be a valuable addition to other forms of evidence in support of public policy decision making. This paper explores when it is sensible to perform such elicitation and how that can best be done. A number of key issues are discussed, including topics on which there are, and are not, experts who have knowledge that provides a basis for making informed predictive judgments; the inadequacy of only using qualitative uncertainty language; the role of cognitive heuristics and of overconfidence; the choice of experts; the development, refinement, and iterative testing of elicitation protocols that are designed to help experts to consider systematically all relevant knowledge when they make their judgments; the treatment of uncertainty about model functional form; diversity of expert opinion; and when it does or does not make sense to combine judgments from different experts. Although it may be tempting to view expert elicitation as a low-cost, low-effort alternative to conducting serious research and analysis, it is neither. Rather, expert elicitation should build on and use the best available research and analysis and be undertaken only when, given those, the state of knowledge will remain insufficient to support timely informed assessment and decision making.",10.1073/pnas.1319946111,2014,1,0.0,1.0,0.0,ngr,1.0,1,1
8,Bayesian forecasting for complex systems using computer simulators,"Although computer models are often used for forecasting future outcomes of complex systems, the uncertainties in such forecasts are not usually treated formally. We describe a general Bayesian approach for using a computer model or simulator of a complex system to forecast system outcomes. The approach is based on constructing beliefs derived from a combination of expert judgments and experiments on the computer model. These beliefs, which are systematically updated as we make runs of the computer model, are used for either Bayesian or Bayes linear forecasting for the system. Issues of design and diagnostics are described in the context of forecasting. The methodology is applied to forecasting for an active hydrocarbon reservoir.",10.1198/016214501753168370,2001,1,0.0,0.0,1.0,nw,1.0,1,1
9,Landslide susceptibility assessment by bivariate methods at large scales: Application to a complex mountainous environment,"Statistical assessment of landslide susceptibility has become a major topic of research in the last decade. Most progress has been accomplished on producing susceptibility maps at meso-scales (1:50,000-1:25,000). At 1:10,000 scale, which is the scale of production of most regulatory landslide hazard and risk maps in Europe, few tests on the performance of these methods have been performed. This paper presents a procedure to identify the best variables for landslide susceptibility assessment through a bivariate technique (weights of evidence, WOE) and discusses the best way to minimize conditional independence (CI) between the predictive variables. Indeed, violating Cl can severely bias the simulated maps by over- or under-estimating landslide probabilities. The proposed strategy includes four steps: (i) identification of the best response variable (RV) to represent landslide events, (ii) identification of the best combination of predictive variables (PVs) and neo-predictive variables (nPVs) to increase the performance of the statistical model, (iii) evaluation of the performance of the simulations by appropriate tests, and (iv) evaluation of the statistical model by expert judgment. The study site is the north-facing hillslope of the Barcelonnette Basin (France), affected by several types of landslides and characterized by a complex morphology. Results indicate that bivariate methods are powerful to assess landslide susceptibility at 1: 10,000 scale. However, the method is limited from a geomorphological viewpoint when RVs and PVs are complex or poorly informative. It is demonstrated that expert knowledge has still to be introduced in statistical models to produce reliable landslide susceptibility maps. (c) 2007 Elsevier B.V. All rights reserved.",10.1016/j.geomorph.2007.02.020,2007,0,0.0,0.0,0.0,gcg,0.0,1,0
10,Reducing Overconfidence in the Interval Judgments of Experts,"Elicitation of expert opinion is important for risk analysis when only limited data are available. Expert opinion is often elicited in the form of subjective confidence intervals; however, these are prone to substantial overconfidence. We investigated the influence of elicitation question format, in particular the number of steps in the elicitation procedure. In a 3-point elicitation procedure, an expert is asked for a lower limit, upper limit, and best guess, the two limits creating an interval of some assigned confidence level (e.g., 80%). In our 4-step interval elicitation procedure, experts were also asked for a realistic lower limit, upper limit, and best guess, but no confidence level was assigned; the fourth step was to rate their anticipated confidence in the interval produced. In our three studies, experts made interval predictions of rates of infectious diseases (Study 1, n = 21 and Study 2, n = 24: epidemiologists and public health experts), or marine invertebrate populations (Study 3, n = 34: ecologists and biologists). We combined the results from our studies using meta-analysis, which found average overconfidence of 11.9%, 95% CI [3.5, 20.3] (a hit rate of 68.1% for 80% intervals)-a substantial decrease in overconfidence compared with previous studies. Studies 2 and 3 suggest that the 4-step procedure is more likely to reduce overconfidence than the 3-point procedure (Cohen's d = 0.61, [0.04, 1.18]).",10.1111/j.1539-6924.2009.01337.x,2010,0,1.0,0.0,0.0,gcg,1.0,0,0
11,Automatic detection of learner's affect from conversational cues,"We explored the reliability of detecting a learner's affect from conversational features extracted from interactions with AutoTutor, an intelligent tutoring system (ITS) that helps students learn by holding a conversation in natural language. Training data were collected in a learning session with AutoTutor, after which the affective states of the learner were rated by the learner, a peer, and two trained judges. Inter-rater reliability scores indicated that the classifications of the trained judges were more reliable than the novice judges. Seven data sets that temporally integrated the affective judgments with the dialogue features of each learner were constructed. The first four datasets corresponded to the judgments of the learner, a peer, and two trained judges, while the remaining three data sets combined judgments of two or more raters. Multiple regression analyses confirmed the hypothesis that dialogue features could significantly predict the affective states of boredom, confusion, flow, and frustration. Machine learning experiments indicated that standard classifiers were moderately successful in discriminating the affective states of boredom, confusion, flow, frustration, and neutral, yielding a peak accuracy of 42% with neutral (chance = 20%) and 54% without neutral (chance = 25%). Individual detections of boredom, confusion, flow, and frustration, when contrasted with neutral affect, had maximum accuracies of 69, 68, 71, and 78%, respectively (chance = 50%). The classifiers that operated on the emotion judgments of the trained judges and combined models outperformed those based on judgments of the novices (i.e., the self and peer). Follow-up classification analyses that assessed the degree to which machine-generated affect labels correlated with affect judgments provided by humans revealed that human-machine agreement was on par with novice judges (self and peer) but quantitatively lower than trained judges. We discuss the prospects of extending AutoTutor into an affect-sensing ITS.",10.1007/s11257-007-9037-6,2008,0,0.0,0.0,0.0,ngr,0.0,1,0
12,Soft computing system for bank performance prediction,"This paper presents a soft computing based bank performance prediction system. It is an ensemble system whose constituent models are a multilayered feed forward neural network trained with backpropagation (MLFF-BP), a probabilistic neural network (PNN) and a radial basis function neural network (RBFN), support vector machine (SVM), classification and regression trees (CART) and a fuzzy rule based classifier. Further, principal component analysis (PCA) based hybrid neural networks, viz. PCA-MLFF-BP, PCA-PNN and PCA-RBF are also included as constituents of the ensemble. Moreover, GRNN and PNN were trained with a genetic algorithm to optimize the smoothing factors. Two ensembles (i) simple majority voting based and (ii) weightage based are implemented. This system predicts the performance of a bank in the coming financial year based on its previous 2-years' financial data. Ten-fold cross-validation is performed in the training sessions and results are validated with an independent production set. It is demonstrated that the ensemble is able to yield lower Type I and Type II errors compared to its constituent models. Further, the ensemble also outperformed an earlier study [P. G. Swicegood, Predicting poor bank profitability: a comparison of neural network, discriminant analysis and professional human judgement, Ph.D. Thesis, Department of Finance, Florida State University, 1998] that used multivariate discriminant analysis (MDA), MLFF-BP and human judgment. (C) 2007 Elsevier B. V. All rights reserved.",10.1016/j.asoc.2007.02.001,2008,0,0.0,0.0,0.0,nw,0.0,1,0
13,Toward a ground-motion logic tree for probabilistic seismic hazard assessment in Europe,"The Seismic Hazard Harmonization in Europe (SHARE) project, which began in June 2009, aims at establishing new standards for probabilistic seismic hazard assessment in the Euro-Mediterranean region. In this context, a logic tree for ground-motion prediction in Europe has been constructed. Ground-motion prediction equations (GMPEs) and weights have been determined so that the logic tree captures epistemic uncertainty in ground-motion prediction for six different tectonic regimes in Europe. Here we present the strategy that we adopted to build such a logic tree. This strategy has the particularity of combining two complementary and independent approaches: expert judgment and data testing. A set of six experts was asked to weight pre-selected GMPEs while the ability of these GMPEs to predict available data was evaluated with the method of Scherbaum et al. (Bull Seismol Soc Am 99:3234-3247, 2009). Results of both approaches were taken into account to commonly select the smallest set of GMPEs to capture the uncertainty in ground-motion prediction in Europe. For stable continental regions, two models, both from eastern North America, have been selected for shields, and three GMPEs from active shallow crustal regions have been added for continental crust. For subduction zones, four models, all non-European, have been chosen. Finally, for active shallow crustal regions, we selected four models, each of them from a different host region but only two of them were kept for long periods. In most cases, a common agreement has been also reached for the weights. In case of divergence, a sensitivity analysis of the weights on the seismic hazard has been conducted, showing that once the GMPEs have been selected, the associated set of weights has a smaller influence on the hazard.",10.1007/s10950-012-9281-z,2012,0,0.0,0.0,1.0,nw,1.0,0,0
14,Hailfinder: A Bayesian system for forecasting severe weather,"Hailfinder is a Bayesian system that combines meteorological data and models with expert judgment, based on both experience and physical understanding, to forecast severe weather in Northeastern Colorado. The system is based on a model, known as a belief network (BN), that has recently emerged as the basis of some powerful intelligent systems. Hailfinder is the first such system to apply these Bayesian models in the realm of meteorology, a field that has served as the basis of many past investigations of probabilistic forecasting. The design of Hailfinder provides a variety of insights to designers of other BN-based systems, regardless of their fields of application.",10.1016/0169-2070(95)00664-8,1996,1,0.0,0.0,1.0,nw,1.0,1,1
15,Analyzing the Climate Sensitivity of the HadSM3 Climate Model Using Ensembles from Different but Related Experiments,"Global climate models (GCMs) contain imprecisely defined parameters that account, approximately, for subgrid-scale physical processes. The response of a GCM to perturbations in its parameters, which is crucial for quantifying uncertainties in simulations of climate change, can-in principle-be assessed by simulating the GCM many times. In practice, however, such ""perturbed physics'' ensembles are small because GCMs are so expensive to simulate. Statistical tools can help in two ways. First, they can be used to combine ensembles from different but related experiments, increasing the effective number of simulations. Second, they can be used to describe the GCM's response in ways that cannot be extracted directly from the ensemble(s). The authors combine two experiments to learn about the response of the Hadley Centre Slab Climate Model version 3 (HadSM3) climate sensitivity to 31 model parameters. A Bayesian statistical framework is used in which expert judgments are required to quantify the relationship between the two experiments; these judgments are validated by detailed diagnostics. The authors identify the entrainment rate coefficient of the convection scheme as the most important single parameter and find that this interacts strongly with three of the large-scale-cloud parameters.",10.1175/2008JCLI2533.1,2009,0,0.0,0.0,0.0,ngr,0.0,1,0
16,A universal model of esthetic perception based on the sensory coding of natural stimuli,"Philosophers have pointed out that there is a close relation between the esthetics of art and the beauty of natural scenes. Supporting this similarity at the experimental level, we have recently shown that visual art and natural scenes share fractal-like, scale-invariant statistical properties. Moreover, evidence from neurophysiological experiments shows that the visual system uses an efficient (sparse) code to process optimally the statistical properties of natural stimuli. In the present work, a hypothetical model of esthetic perception is described that combines both lines of evidence. Specifically, it is proposed that an artist creates a work of art so that it induces a specific resonant state in the visual system. This resonant state is thought to be based on the adaptation of the visual system to natural scenes. The proposed model is universal and predicts that all human beings share the same general concept of esthetic judgment. The model implies that esthetic perception, like the coding of natural stimuli, depends on stimulus form rather than content, depends on higher-order statistics of the stimuli, and is non-intuitive to cognitive introspection. The model accommodates the central tenet of neuroesthetic theory that esthetic perception reflects fundamental functional properties of the nervous system.",10.1163/156856808782713780,2008,0,0.0,0.0,0.0,ngr,0.0,1,0
17,A Bayesian Network Modeling Approach to Forecasting the 21st Century Worldwide Status of Polar Bears,"To inform the U.S. Fish and Wildlife Service decision, whether or not to list polar bears as threatened under the Endangered Species Act (ESA), we projected the status of the world's polar bears (Ursus maritimus) for decades centered on future years 2025, 2050, 2075, and 2095. We defined four ecoregions based on current and projected sea ice conditions: seasonal ice, Canadian Archipelago, polar basin divergent, and polar basin convergent ecoregions. We incorporated general circulation model projections of future sea ice into a Bayesian network (BN) model structured around the factors considered in ESA decisions. This first-generation BN model combined empirical data, interpretations of data, and professional judgments of one polar bear expert into a probabilistic framework that identifies causal links between environmental stressors and polar bear responses. We provide guidance regarding steps necessary to refine the model, including adding inputs from other experts. The BN model projected extirpation of polar bears from the seasonal ice and polar basin divergent ecoregions, where approximate to 2/3 of the world's polar bears currently occur, by mid century. Projections were less dire in other ecoregions. Decline in ice habitat was the overriding factor driving the model outcomes. Although this is a first-generation model, the dependence of polar bears on sea ice is universally accepted, and the observed sea ice decline is faster than models suggest. Therefore, incorporating judgments of multiple experts in a final model is not expected to fundamentally alter the outlook for polar bears described here.",10.1029/180GM14,2008,0,0.0,0.0,1.0,nw,1.0,0,0
18,A Stability Bias in Human Memory: Overestimating Remembering and Underestimating Learning,"The dynamics of human memory are complex and often unintuitive, but certain features-such as the fact that studying results in learning-seem like common knowledge. In 12 experiments, however, participants who were told they would be allowed to study a list of word pairs between I and 4 times and then take a cued-recall test predicted little or no learning across trials, notwithstanding their large increases in actual learning. When queried directly, the participants espoused the belief that studying results in learning, but they showed little evidence of that belief in the actual task. These findings, when combined with A. Koriat, R. A. Bjork, L. Sheffer, and S. K. Bar's (2004) research on judgments of forgetting, suggest a stability bias in human memory-that is, a tendency to assume that the accessibility of one's memories will remain relatively stable over time rather than benefiting from future learning or suffering from future forgetting.",10.1037/a0017350,2009,0,0.0,0.0,0.0,nw,0.0,1,0
19,The statistical structure of human speech sounds predicts musical universals,"The similarity of musical scales and consonance judgments across human populations has no generally accepted explanation. Here we present evidence that these aspects of auditory perception arise from the statistical structure of naturally occurring periodic sound stimuli. An analysis of speech sounds, the principal source of periodic sound stimuli in the human acoustical environment, shows that the probability distribution of amplitude-frequency combinations in human utterances predicts both the structure of the chromatic scale and consonance ordering. These observations suggest that what we hear is determined by the statistical relationship between acoustical stimuli and their naturally occurring sources, rather than by the physical parameters of the stimulus per se.",10.1523/JNEUROSCI.23-18-07160.2003,2003,0,0.0,0.0,0.0,ngr,0.0,1,0
20,USE OF PROBABILISTIC EXPERT JUDGMENT IN UNCERTAINTY ANALYSIS OF CARCINOGENIC POTENCY,"A new approach to characterizing the state of knowledge about carcinogenic potency is described. In this approach, the carcinogenic risk posed by a specific dose is characterized by a probability distribution, indicating the relative likelihood of different risk estimates. The approach utilizes expert judgment and a probability tree and is illustrated in a case study of chloroform exposure. Experts in cancer biology/toxicology, pharmacokinetics, and dose-response modeling were identified by a panel of science-policy specialists. In a workshop, experts reviewed the chloroform data, received training in probability elicitation, and constructed a consensual probability tree based on biological theories of cancer causation. Distributions of carcinogenic risk were developed based on the probability tree, chloroform data, judgmental probabilities provided by the experts, and classical statistical techniques. Risk distributions varied considerably between experts, with some predicting essentially no risk from 100 ppb chloroform in drinking water while others have at least some probability on risks generally considered of regulatory significance. Estimated human risk was much lower when extrapolating from liver tumors in animals than from kidney tumors. Issues of scientific disagreement leading to different risk distributions between experts are discussed. The resulting risk distributions are compared to standard EPA risk calculations for the same exposure scenario as well as to the expert judgments of epidemiologists about cancer risks of chlorinated drinking water. Issues in combining expert judgments are discussed, and several alternative methods are presented. Strengths and weaknesses of the distributional approach are discussed. (C) 1994 Academic Press, Inc.",10.1006/rtph.1994.1034,1994,1,0.0,1.0,0.0,ngr,1.0,1,1
21,The limits of forecasting methods in anticipating rare events,"In this paper we review methods that aim to aid the anticipation of rare, high-impact, events. We evaluate these methods according to their ability to yield well-calibrated probabilities or point forecasts for such events. We first identify six factors that can lead to poor calibration and then examine how successful the methods are in mitigating these factors. We demonstrate that all the extant forecasting methods including the use of expert judgment, statistical forecasting. Delphi and prediction markets contain fundamental weaknesses. We contrast these methods with a non-forecasting method that is intended to aid planning for the future scenario planning. We conclude that all the methods are problematic for aiding the anticipation of rare events and that the only remedies are to either (i) to provide protection for the organization against the occurrence of negatively-valenced events whilst allowing the organization to benefit from the occurrence of positively-valenced events, or (ii) to provide conditions to challenge one's own thinking and hence improve anticipation. We outline how components of devil's advocacy and dialectical inquiry can be combined with Delphi and scenario planning to enhance anticipation of rare events. (C) 2009 Elsevier Inc. All rights reserved.",10.1016/j.techfore.2009.10.008,2010,0,0.0,0.0,1.0,nw,1.0,0,0
22,2015 Guidelines of the Taiwan Society of Cardiology and the Taiwan Hypertension Society for the Management of Hypertension,"It has been almost 5 years since the publication of the 2010 hypertension guidelines of the Taiwan Society of Cardiology (TSOC). There is new evidence regarding the management of hypertension, including randomized controlled trials, non-randomized trials, post-hoc analyses, subgroup analyses, retrospective studies, cohort studies, and registries. More recently, the European Society of Hypertension (ESH) and the European Society of Cardiology (ESC) published joint hypertension guidelines in 2013. The panel members who were appointed to the Eighth Joint National Committee (JNC) also published the 2014 JNC report. Blood pressure (BP) targets have been changed; in particular, such targets have been loosened in high risk patients. The Executive Board members of TSOC and the Taiwan Hypertension Society (THS) aimed to review updated information about the management of hypertension to publish an updated hypertension guideline in Taiwan. We recognized that hypertension is the most important risk factor for global disease burden. Management of hypertension is especially important in Asia where the prevalence rate grows faster than other parts of the world. In most countries in East Asia, stroke surpassed coronary heart disease (CHD) in causing premature death. A diagnostic algorithm was proposed, emphasizing the importance of home BP monitoring and ambulatory BP monitoring for better detection of night time hypertension, early morning hypertension, white-coat hypertension, and masked hypertension. We disagreed with the ESH/ESH joint hypertension guidelines suggestion to loosen BP targets to <140/90 mmHg for all patients. We strongly disagree with the suggestion by the 2014 JNC report to raise the BP target to <150/90 mmHg for patients between 60-80 years of age. For patients with diabetes, CHD, chronic kidney disease who have proteinuria, and those who are receiving antithrombotic therapy for stroke prevention, we propose BP targets of <130/80 mmHg in our guidelines. BP targets are <140/90 mmHg for all other patient groups, except for patients >= 80 years of age in whom a BP target of <150/90 mmHg would be optimal. For the management of hypertension, we proposed a treatment algorithm, starting with life style modification (LSM) including S-ABCDE (Sodium restriction, Alcohol limitation, Body weight reduction, Cigarette smoke cessation, Diet adaptation, and Exercise adoption). We emphasized a low-salt strategy instead of a no-salt strategy, and that excessively aggressive sodium restriction to <2.0 gram/day may be harmful. When drug therapy is considered, a strategy called ""PROCEED"" was suggested (Previous experience, Risk factors, Organ damage, Contraindications or unfavorable conditions, Expert's or doctor's judgment, Expenses or cost, and Delivery and compliance issue). To predict drug effects in lowering BP, we proposed the ""Rule of 10"" and ""Rule of 5"". With a standard dose of any one of the 5 major classes of anti-hypertensive agents, one can anticipate approximately a 10-mmHg decrease in systolic BP (SBP) (Rule of 10) and a 5-mmHg decrease in diastolic BP (DBP) (Rule of 5). When doses of the same drug are doubled, there is only a 2-mmHg incremental decrease in SBP and a 1-mmHg incremental decrease in DBP. Preferably, when 2 drugs with different mechanisms are to be taken together, the decrease in BP is the sum of the decrease of the individual agents (approximately 20 mmHg in SBP and 10 mmHg in DBP). Early combination therapy, especially single-pill combination (SPC), is recommended. When patient's initial treatment cannot get BP to targeted goals, we have proposed an adjustment algorithm, ""AT GOALs"" (Adherence, Timing of administration, Greater doses, Other classes of drugs, Alternative combination or SPC, and LSM + Laboratory tests). Treatment of hypertension in special conditions, including treatment of resistant hypertension, hypertension in women, and perioperative management of hypertension, were also mentioned. The TSOC/THS hypertension guidelines provide the most updated information available in the management of hypertension. The guidelines are not mandatory, and members of the task force fully realize that treatment of hypertension should be individualized to address each patient's circumstances. Ultimately, the decision of the physician decision remains of the utmost importance in hypertension management. Copyright (C) 2014 Elsevier Taiwan LLC and the Chinese Medical Association. All rights reserved.",10.1016/j.jcma.2014.11.005,2015,0,0.0,0.0,0.0,ngr,0.0,1,0
23,"A map of human impacts to a ""pristine"" coral reef ecosystem, the PapahAnaumokuAkea Marine National Monument","Effective and comprehensive regional-scale marine conservation requires fine-grained data on the spatial patterns of threats and their overlap. To address this need for the PapahAnaumokuAkea Marine National Monument (Monument) in Hawaii, USA, spatial data on 14 recent anthropogenic threats specific to this region were gathered or created, including alien species, bottom fishing, lobster trap fishing, ship-based pollution, ship strike risks, marine debris, research diving, research equipment installation, research wildlife sacrifice, and several anthropogenic climate change threats i.e., increase in ultraviolet (UV) radiation, seawater acidification, the number of warm ocean temperature anomalies relevant to disease outbreaks and coral bleaching, and sea level rise. These data were combined with habitat maps and expert judgment on the vulnerability of different habitat types in the Monument to estimate spatial patterns of current cumulative impact at 1 ha (0.01 km(2)) resolution. Cumulative impact was greatest for shallow reef areas and peaked at Maro Reef, where 13 of the 14 threats overlapped in places. Ocean temperature variation associated with disease outbreaks was found to have the highest predicted impact overall, followed closely by other climate-related threats, none of which have easily tractable management solutions at the regional scale. High impact threats most tractable to regional management relate to ship traffic. Sensitivity analyses show that the results are robust to both data availability and quality. Managers can use these maps to (1) inform management and surveillance priorities based on the ranking of threats and their distributions, (2) guide permitting decisions based on cumulative impacts, and (3) choose areas to monitor for climate change effects. Furthermore, this regional analysis can serve as a case study for managers elsewhere interested in assessing and mapping region-specific cumulative human impacts.",10.1007/s00338-009-0490-z,2009,0,0.0,0.0,0.0,ngr,0.0,1,0
24,Probabilistic risk analysis in subsurface hydrology,"We present a general framework for probabilistic risk assessment (PRA) of subsurface contamination. PRA provides a natural venue for the rigorous quantification of structural (model) and parametric uncertainties inherent in predictions of subsurface flow and transport. A typical PRA starts by identifying relevant components of a subsurface system (e.g., a buried solid-waste tank, an aquitard, a remediation effort) and proceeds by using uncertainty quantification techniques to estimate the probabilities of their failure. These probabilities are then combined by means of fault-tree analyses to yield probabilistic estimates of the risk of system failure (e.g., aquifer contamination). Since PRA relies on subjective probabilities, it is ideally suited for assimilation of expert judgment and causal relationships.",10.1029/2007GL029245,2007,1,0.0,0.0,1.0,nw,1.0,1,1
25,A convolutional neural network approach for objective video quality assessment,"This paper describes an application of neural networks in the field of objective measurement method designed to automatically assess the perceived quality of digital videos. This challenging issue aims to emulate human judgment and to replace very,complex and time consuming subjective quality assessment. Several metrics have been proposed in literature to tackle this issue. They are based on a general framework that combines different stages, each of them addressing complex problems. The ambition of this paper is not to present a global perfect quality metric but rather to focus on an original way to use neural networks in such a framework in the context of reduced reference (RR) quality metric. Especially, we point out the interest of such a tool for combining features and pooling them in order to compute quality scores. The proposed approach solves some problems inherent to objective metrics that should predict subjective quality score obtained using the single stimulus continuous quality evaluation (SSCQE) method. This latter has been adopted by video quality expert group (VQEG) in its recently finalized reduced referenced and no reference (RRNR-TV) test plan. The originality of such approach compared to previous attempts to use neural networks for quality assessment, relies on the use of a convolutional neural network (CNN) that allows a continuous time scoring of the video. Objective features are extracted on a frame-by-frame basis on both the reference and the distorted sequences; they are derived from a perceptual-based representation and integrated along the temporal axis using a time-delay neural network (TDNN). Experiments conducted on different MPEG-2 videos, with bit rates ranging 2-6 Mb/s, show the effectiveness of the proposed approach to get a plausible model of temporal pooling from the human vision system (HVS) point of view. More specifically, a linear correlation criteria, between objective and subjective scoring, up to 0.92 has been obtained on a set of typical TV videos.",10.1109/TNN.2006.879766,2006,0,0.0,0.0,0.0,gcg,0.0,1,0
26,The relative importance of the face and body in judgments of human physical attractiveness,"A number of traits have been proposed to be important in human mate choice decisions. However, relatively little work has been conducted to determine the relative importance of these traits. In this study, we assessed the relative importance of the face and body in judgments of human physical attractiveness. One hundred twenty-seven men and 133 women were shown images of 10 individuals of the opposite sex. Participants rated the images for their attractiveness for either a short-term relationship or a long-term relationship. Images of the face and the body were rated independently before participants were shown and asked to rate the combined face and body images. Face ratings were found to be the best predictor of the ratings of combined images for both sexes and for both relationship types. Females showed no difference in ratings between short- and long-term conditions, but male ratings of female bodies became relatively more important for a short-term relationship compared with a long-term relationship. Results suggest that faces and bodies may be signaling different information about potential mates. (C) 2009 Elsevier Inc. All rights reserved.",10.1016/j.evolhumbehav.2009.06.005,2009,0,0.0,0.0,1.0,nw,1.0,0,0
27,Combining forecasts: An application to elections,"We summarize the literature on the effectiveness of combining forecasts by assessing the conditions under which combining is most valuable. Using data on the six US presidential elections from 1992 to 2012, we report the reductions in error obtained by averaging forecasts within and across four election forecasting methods: poll projections, expert judgment, quantitative models, and the Iowa Electronic Markets. Across the six elections, the resulting combined forecasts were more accurate than any individual component method, on average. The gains in accuracy from combining increased with the numbers of forecasts used, especially when these forecasts were based on different methods and different data, and in situations involving high levels of uncertainty. Such combining yielded error reductions of between 16% and 59%, compared to the average errors of the individual forecasts. This improvement is substantially greater than the 12% reduction in error that had been reported previously for combining forecasts. (C) 2013 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.",10.1016/j.ijforecast.2013.02.005,2014,1,0.0,0.0,1.0,nw,1.0,1,1
28,Aggregating Probability Distributions,"This chapter is concerned with the aggregation of probability distributions in decision and risk analysis. Experts often provide valuable information regarding important uncertainties in decision and risk analyses because of the limited availability of hard data to use in those analyses. Multiple experts are often consulted in order to obtain as much information as possible, leading to the problem of how to combine or aggregate their information. Information may also be obtained from other sources such as forecasting techniques or scientific models. Because uncertainties are typically represented in terms of probability distributions, we consider expert and other information in terms of probability distributions. We discuss a variety of models that lead to specific combination methods. The output of these methods is a combined probability distribution, which can be viewed as representing a summary of the current state of information regarding the uncertainty of interest. After presenting the models and methods, we discuss empirical evidence on the performance of the methods. In the conclusion, we highlight important conceptual and practical issues to be considered when designing a combination process for use in practice.",10.1017/CBO9780511611308.010,2007,1,0.0,0.0,0.0,gcg,0.0,0,0
29,Understanding and predicting the combined effects of climate change and land-use change on freshwater macroinvertebrates and fish,"Climate change and land-use change are having substantial impacts on biodiversity world-wide, but few studies have considered the impact of these factors together. If the combined effects of climate and land-use change are greater than the effects of each threat individually, current conservation management strategies may be inefficient and/or ineffective. This is particularly important with respect to freshwater ecosystems because freshwater biodiversity has declined faster than either terrestrial or marine biodiversity over the last three decades. This is the first study to model the independent and combined effects of climate change and land-use change on freshwater macroinvertebrates and fish. Using a case study in south-east Queensland, Australia, we built a Bayesian belief network populated with a combination of field data, simulations, existing models and expert judgment. Different land-use and climate scenarios were used to make predictions on how the richness of freshwater macroinvertebrates and fish is likely to respond in future. We discovered little change in richness averaged across the region, but identified important impacts and effects at finer scales. High nutrients and high runoff as a result of urbanization combined with high nutrients and high water temperature as a result of climate change and were the leading drivers of potential declines in macroinvertebrates and fish at fine scales. Synthesis and applications. This is the first study to separate out the constituent drivers of impacts on biodiversity that result from climate change and land-use change. Mitigation requires management actions that reduce in-stream nutrients, slows terrestrial runoff and provides shade, to improve the resilience of biodiversity in streams. Encouragingly, the restoration of riparian habitats is identified as an important buffering tool that can mitigate the negative effects of climate change and land-use change.",10.1111/1365-2664.12236,2014,1,0.0,1.0,0.0,ngr,1.0,1,1
30,Using Bayesian networks to model expected and unexpected operational losses,"This report describes the use of Bayesian networks (BNs) to model statistical loss distributions in financial operational risk scenarios. Its focus is on modeling ""long"" tail, or unexpected, loss events using mixtures of appropriate loss frequency and severity distributions where these mixtures are conditioned on causal variables that model the capability or effectiveness of the underlying controls process. The use of causal modeling is discussed from the perspective of exploiting local expertise about process reliability and formally connecting this knowledge to actual or hypothetical statistical phenomena resulting from the process. This brings the benefit of supplementing sparse data with expert judgment and transforming qualitative knowledge about the process into quantitative predictions. We conclude that BNs can help combine qualitative data from experts and quantitative data from historical loss databases in a principled way and as such they go some way in meeting the requirements of the draft Basel II Accord (Basel, 2004) for an advanced measurement approach (AMA).",10.1111/j.1539-6924.2005.00641.x,2005,1,0.0,0.0,0.0,gcg,0.0,0,0
31,Computational intelligence for heart disease diagnosis: A medical knowledge driven approach,"This paper investigates a number of computational intelligence techniques in the detection of heart disease. Particularly, comparison of six well known classifiers for the well used Cleveland data is performed. Further, this paper highlights the potential of an expert judgment based (i.e., medical knowledge driven) feature selection process (termed as MFS), and compare against the generally employed computational intelligence based feature selection mechanism. Also, this article recognizes that the publicly available Cleveland data becomes imbalanced when considering binary classification. Performance of classifiers. and also the potential of MFS are investigated considering this imbalanced data issue. The experimental results demonstrate that the use of MFS noticeably improved the performance, especially in terms of accuracy, for most of the classifiers considered and for majority of the datasets (generated by converting the Cleveland dataset for binary classification). MFS combined with the computerized feature selection process (CFS) has also been investigated and showed encouraging results particularly for NaiveBayes, IBK and SMO. In summary, the medical knowledge based feature selection method has shown promise for use in heart disease diagnostics. (C) 2012 Elsevier Ltd. All rights reserved.",10.1016/j.eswa.2012.07.032,2013,0,0.0,0.0,1.0,nw,1.0,0,0
32,Modeling and forecasting industrial end-use natural gas consumption,"Forecasting industrial end-use natural gas consumption is an important prerequisite for efficient system operation and a basis for planning decisions. This paper presents a novel prediction model that provides forecasting in a medium-term horizon (1-3 years) with a very high resolution (days) based on a decomposition approach. The forecast is obtained by the combination of three different components: one that captures the trend of the time series, a seasonal component based on the Linear Hinges Model, and a transitory component to estimate daily variations using explanatory variables. The flexibility of the model allows describing demand patterns in a very wide range of historical profiles. Furthermore, the proposed method combines a very simple representation of the forecasting model, which allows the expert to integrate judgmental analysis and adjustment of the statistical forecast, with accuracy and high computational efficiency. Realistic case studies are provided. (C) 2007 Elsevier B.V All rights reserved.",10.1016/j.eneco.2007.01.015,2007,1,0.0,0.0,1.0,nw,1.0,1,1
33,Combining techniques to optimize effort predictions in software project management,"This paper tackles two questions related to software effort prediction. First, is it valuable to combine prediction techniques? Second, if so, how? Many commentators have suggested the use of more than one technique in order to support effort prediction, but to date there has been little or no empirical investigation to support this recommendation. Our analysis of effort data from a medical records information system reveals that there is little, or even negative, covariance between the accuracy of our three chosen prediction techniques, namely, expert judgment, least squares regression and case-based reasoning. This indicates that when one technique predicts poorly, one or both of the others tends to perform significantly better. This is a particularly striking result given the relative homogeneity of our data set. Consequently, searching for the single ""best"" technique, at least in this case, leads to a suboptimal prediction strategy. The challenge then becomes one of identifying a means of determining a priori which prediction technique to use. Unfortunately, despite using a range of techniques including rule induction, we were unable to identify any simple mechanism for doing so. Nevertheless, we believe this remains an important research goal. (C) 2002 Elsevier Science Inc. All rights reserved.",10.1016/S0164-1212(02)00067-5,2003,1,0.0,0.0,0.0,ngr,0.0,0,0
34,"Learning, prediction and causal Bayes nets","Recent research in cognitive and developmental psychology on acquiring and using causal knowledge uses the causal Bayes net formalism, which simultaneously represents hypotheses about causal relations, probability relations, and effects of interventions. The formalism provides new normative standards for reinterpreting experiments on human judgment, offers a precise interpretation of mechanisms, and allows generalizations of existing theories of causal learning. Combined with hypotheses about learning algorithms, the formalism makes predictions about inferences in many experimental designs beyond the classical, Pavlovian cue --> effect design.",10.1016/S1364-6613(02)00009-8,2003,0,0.0,0.0,0.0,gcg,0.0,1,0
35,Translating Video Content to Natural Language Descriptions,"Humans use rich natural language to describe and communicate visual perceptions. In order to provide natural language descriptions for visual content, this paper combines two important ingredients. First, we generate a rich semantic representation of the visual content including e.g. object and activity labels. To predict the semantic representation we learn a CRF to model the relationships between different components of the visual input. And second, we propose to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language. For this we exploit the power of a parallel corpus of videos and textual descriptions and adapt statistical machine translation to translate between our two languages. We evaluate our video descriptions on the TACoS dataset [23], which contains video snippets aligned with sentence descriptions. Using automatic evaluation and human judgments we show significant improvements over several baseline approaches, motivated by prior work. Our translation approach also shows improvements over related work on an image description task.",10.1109/ICCV.2013.61,2013,0,0.0,0.0,0.0,ngr,0.0,1,0
36,Addressing equifinality and uncertainty in eutrophication models,"[1] Large simulation models of eutrophication processes are commonly used to aid scientific understanding and to guide management decisions. Confidence in models for these purposes depends on uncertainty in model equations (structural uncertainty) and on effects of input uncertainties (model parameters, initial conditions, and forcing functions) on model outputs. Our objective herein is to illustrate two strategies, a generalized likelihood uncertainty estimation (GLUE) approach combined with a simple Monte Carlo sampling scheme and a Bayesian methodological framework along with Markov Chain Monte Carlo (MCMC) simulations, for elucidating the propagation of uncertainty in the high-dimensional parameter spaces of mechanistic eutrophication models. We examine the ability of the two approaches to offer insights into the degree of information about model inputs that the data contain, to quantify the correlation structure among parameter estimates, and to obtain predictions along with uncertainty bounds for modeled output variables. Our analysis is based on a four-state-variable (phosphate-detritus-phytoplankton-zooplankton) model and the mesotrophic Lake Washington (Washington State, United States) as a case study. Scientific knowledge, expert judgment, and observational data were used to formulate prior probability distributions and characterize the uncertainty pertaining to 14 model parameters. Despite the conceptual differences for addressing model equifinality, that is, wide ranges of parameter values subject to complex multivariate relationships that result in plausible observed behaviors and produce equivalently accurate predictions, we found that the two strategies provided fairly consistent estimates of the posterior parameter correlation structure and output uncertainty. Nonetheless, our analysis also shows that MCMC can more efficiently quantify the joint probability distribution of model parameters and make inference about this distribution. The latter finding can be explained by the basic idea underlying the MCMC methodology, that is, the configuration of a Markov process whose stationary distribution approximates the joint posterior distribution of all the stochastic model nodes; as a result, Monte Carlo samples are not drawn from the prior parameter space, and problems of wide or highly correlated prior distributions can be overcome. Finally, our study stresses the lack of perfect simulators of natural system dynamics and introduces two statistical formulations that can explicitly account for the discrepancy between mathematical models and environmental systems.",10.1029/2007WR005862,2008,0,0.0,0.0,0.0,gcg,0.0,1,0
37,Are We Wise About the Wisdom of Crowds? The Use of Group Judgments in Belief Revision,"Recent research has advanced our understanding of how people use the advice of others to update their beliefs. Because groups and teams play a significant role in organizations and collectively are wiser than their individual members, it is important to understand their influence on belief revision as well. I report the results of four studies examining intuitions about group wisdom and the informational influence of groups. In their overt assessments, experimental participants rated larger groups as more accurate than smaller groups and discriminated more between them when group size was salient. When provided advice, participants relied more on groups than individuals to update their beliefs, but were only modestly sensitive to group size. Most were suboptimal in the use of that advice, overweighting their initial beliefs and underweighting the more valid judgment of the group. Thus although acknowledged in principle, the wisdom of crowds is only shallowly manifest in observed behavior.",10.1287/mnsc.1090.1031,2009,0,0.0,0.0,0.0,ngr,0.0,1,0
38,Uncertainty analysis using evidence theory - confronting level-1 and level-2 approaches with data availability and computational constraints,"Dempster-Shafer Theory of Evidence (DST), as an alternative or complementary approach to the representation of uncertainty, is gradually being explored with complex practical applications beyond purely algebraic examples. This paper reviews literature documenting such complex applications and studies its applicability from the point of view of the nature and amount of data that is typically available in industrial risk analysis: medium-size frequential observations for aleatory components, small noised datasets for model parameters and expert judgment for other components. On the basis of a simple flood model encoding typical risk analysis features, different approaches to quantify uncertainty in DST are reviewed and benchmarked in that perspective: (i) combining all sources of uncertainty under a single-level DST model; (ii) separating aleatory and epistemic uncertainties, respectively, modeled with a first probabilistic layer and a second one under DST. Methods for handling data in probabilistic studies such as Kolmogorov-Smirnov tests and quantile-quantile plots are transferred to the domain of DST. We illustrate how data availability guides the choice of the settings and how results and sensitivity analyses can be interpreted in the domain of DST, concluding with recommendations for industrial practice. (C) 2010 Elsevier Ltd. All rights reserved.",10.1016/j.ress.2010.01.005,2010,0,0.0,0.0,0.0,ngr,0.0,1,0
39,On using planning poker for estimating user stories,"While most studies in psychology and forecasting stress the possible hazards of group processes when predicting effort and schedule, agile software development methods recommend the use of a group estimation technique called planning poker for estimating the size of user stories and developing release and iteration plans. It is assumed that the group discussion through planning poker helps in identifying activities that individual estimators could overlook, thus providing more accurate estimates and reducing the over-optimism that is typical for expert judgment-based methods. In spite of the widespread use of agile methods, there is little empirical evidence regarding the accuracy of planning poker estimates. In order to fill this gap a study was conducted requiring 13 student teams to develop a Web-based student records information system. All teams were given the same set of user stories which had to be implemented in three Sprints. Each team estimated the stories using planning poker and the estimates provided by each team member during the first round were averaged to obtain the statistical combination for further comparison. In the same way the stories were estimated by a group of experts. The study revealed that students' estimates were over-optimistic and that planning poker additionally increased the over-optimism. On the other hand, the experts' estimates obtained through planning poker were much closer to actual effort spent and tended to be more accurate than the statistical combination of their individual estimates. The results indicate that the optimism bias caused by group discussion diminishes or even disappears as the expertise of the people involved in the group estimation process increases. (C) 2012 Elsevier Inc. All rights reserved.",10.1016/j.jss.2012.04.005,2012,0,0.0,1.0,0.0,ngr,1.0,0,0
40,No-reference image blur assessment using multiscale gradient,"The increasing number of demanding consumer video applications, as exemplified by cell phone and other low-cost digital cameras, has boosted interest in no-reference objective image and video quality assessment (QA) algorithms. In this paper, we focus on no-reference image and video blur assessment. We consider natural scenes statistics models combined with multi-resolution decomposition methods to extract reliable features for QA. The algorithm is composed of three steps. First, a probabilistic support vector machine (SVM) is applied as a rough image quality evaluator. Then the detail image is used to refine the blur measurements. Finally, the blur information is pooled to predict the blur quality of images. The algorithm is tested on the LIVE Image Quality Database and the Real Blur Image Database; the results show that the algorithm has high correlation with human judgments when assessing blur distortion of images.",10.1186/1687-5281-2011-3,2011,0,0.0,0.0,0.0,nw,0.0,1,0
41,ECOLOGICAL VULNERABILITY IN WILDLIFE: AN EXPERT JUDGMENT AND MULTICRITERIA ANALYSIS TOOL USING ECOLOGICAL TRAITS TO ASSESS RELATIVE IMPACT OF POLLUTANTS,"Nature development in The Netherlands often is planned on contaminated soils and sediments of former agricultural land and in floodplain areas; however, this contamination may present a risk to wildlife species desired at those nature development sites. Specific risk assessment methods are needed, because toxicological information is lacking for most wildlife species. The vulnerability of a species is a combination of its potential exposure, sensitivity to the type of pollutant, and recovery capacity. We developed a new method to predict ecological vulnerability in wildlife using autecological information. The analysis results in an ordinal ranking of vulnerable species. The method was applied to six representative contaminants: copper and zinc (essential metals, low to medium toxicity), cadmium (nonessential metal, high toxicity), DDT (persistent organic pesticide, high toxicity), chlorpyrifos (persistent organophosphate insecticide, high toxicity), and ivermectin (persistent veterinary pharmaceutical, low to medium toxicity). High vulnerability to the essential metals copper and zinc was correlated with soil and sediment habitat preference of a species and with r-strategy (opportunistic strategy suited for unstable environments). Increased vulnerability to the bioaccumulating substances cadmium and DDT was correlated with higher position of a species in the food web and with life span and K-strategy (equilibrium strategy suited for stable environments). Vulnerability to chlorpyrifos and ivermectin was high for species with a preference for soil habitats. The ecological vulnerability analysis has potential to further our abilities in risk assessment.",10.1897/08-626.S1,2009,0,0.0,0.0,0.0,nw,0.0,1,0
42,Differential Diagnosis of Children with Suspected Childhood Apraxia of Speech,"Purpose: The gold standard for diagnosing childhood apraxia of speech (CAS) is expert judgment of perceptual features. The aim of this study was to identify a set of objective measures that differentiate CAS from other speech disorders. Method: Seventy-two children (4-12 years of age) diagnosed with suspected CAS by community speech-language pathologists were screened. Forty-seven participants underwent diagnostic assessment including presence or absence of perceptual CAS features. Twenty-eight children met two sets of diagnostic criteria for CAS (American Speech-Language-Hearing Association, 2007b; Shriberg, Potter, & Strand, 2009); another 4 met the CAS criteria with comorbidity. Fifteen were categorized as non-CAS with phonological impairment, submucous cleft, or dysarthria. Following this, 24 different measures from the diagnostic assessment were rated by blinded raters. Multivariate discriminant function analysis was used to identify the combination of measures that best predicted expert diagnoses. Results: The discriminant function analysis model, including syllable segregation, lexical stress matches, percentage phonemes correct from a polysyllabic picture-naming task, and articulatory accuracy on repetition of /peteke/, reached 91% diagnostic accuracy against expert diagnosis. Conclusions: Polysyllabic production accuracy and an oral motor examination that includes diadochokinesis may be sufficient to reliably identify CAS and rule out structural abnormality or dysarthria. Testing with a larger unselected sample is required.",10.1044/2014_JSLHR-S-12-0358,2015,0,0.0,0.0,1.0,nw,1.0,0,0
43,Coherence and correspondence criteria for rationality: Experts' estimation of risks of sexually transmitted infections,"The aim of this study is to examine both coherence and correspondence criteria for rationality in experts' judgments of risk. We investigated biases in risk estimation for sexually transmitted infections (STIs) predicted by fuzzy-trace theory, i.e., that specific errors would occur despite experts' knowledge of correct responses. One hundred twenty professionals with specific knowledge of STI risks in adolescents were administered a survey questionnaire to test predictions concerning: knowledge deficits (producing underestimation of risks); gist-based representation of risk categories (producing overestimation of condom effectiveness); retrieval failure for risk knowledge (producing lower risk estimates); and processing interference in combining risk estimates (producing biases in post-test diagnosis of infection). Retrieval was manipulated by asking estimation questions that ""unpacked"" the STI category into infection types or did not specify infection types. Other questions differentiated processing biases from knowledge deficits or retrieval failure by directly providing requisite knowledge. Experts' knowledge of STI transmission and infection risks was verified empirically. Nevertheless, under predictable conditions, they misestimated risk, overestimated the effectiveness of condoms, and also suffered from processing biases. When questions provided better retrieval supports (unpacked format), risk estimates improved. Biases were linked to gist representations, retrieval failures, and processing errors, as opposed to knowledge about STIs. Results support fuzzy-trace theory's dual-process assumptions that different types of errors are dissociated from one another, and separate failures of coherence and correspondence among the same sample of experts. Copyright (c) 2005 John Wiley & Sons, Ltd.",10.1002/bdm.493,2005,0,0.0,0.0,0.0,ngr,0.0,1,0
44,A DECOMPOSITION OF THE CORRELATION-COEFFICIENT AND ITS USE IN ANALYZING FORECASTING SKILL,"Estimates of several components of forecasting skill can be obtained by combining a skill-score decomposition developed by Allan Murphy with techniques for decomposing correlation coefficients that have been employed in research on human judgment. The decomposition of the correlation coefficient requires knowledge of the information or ""cues"" used by the forecaster. When the cues are known, it is possible to estimate the effects of uncertainty and the forecaster's consistency and use of the cues.",10.1175/1520-0434(1990)005<0661:ADOTCC>2.0.CO;2,1990,0,0.0,0.0,0.0,gcg,0.0,1,0
45,Prediction of musical affect using a combination of acoustic structural cues,"This study explores whether musical affect attribution can be predicted by a linear combination of acoustical structural cues. To that aim, a database of sixty musical audio excerpts was compiled and analyzed at three levels: judgments of affective content by subjects; judgments of structural content by musicological experts (i.e., ""manual structural cues""), and extraction of structural content by an auditory-based computer algorithm (called: acoustical structural cues). In Study 1, an affect space was constructed with Valence (gay-sad), Activity (tender-bold) and Interest (exciting-boring) as the main dimensions, using the responses of a hundred subjects. In Study 11 manual and acoustical structural cues were analyzed and compared. Manual structural cues such as loudness and articulation could be accounted for in terms of a combination of acoustical structural cues. In Study 111, the subjective responses of eight individual subjects were analyzed using the affect space obtained in Study 1, and modeled in terms of the structural cues obtained in Study 11, using linear regression modeling. This worked better for the Activity dimension than for the Valence dimension, while the Interest dimension could not be accounted for. Overall, manual structural cues worked better than acoustical structural cues. In a final assessment study, a selected set of acoustical structural cues was used for building prediction models. The results indicate that musical affect attribution can partly be predicted using a combination of acoustical structural cues. Future research may focus on non-linear approaches, elaboration of dataset and subjects, and refinement of acoustical structural cue extraction.",10.1080/09298210500123978,2005,0,0.0,0.0,0.0,ngr,0.0,1,0
46,Using expert judgment and stakeholder values to evaluate adaptive management options,"This paper provides an example of a practical integration of probabilistic policy analysis and multi-stakeholder decision methods at a hydroelectric facility in British Columbia, Canada. A structured decision-making framework utilizing the probabilistic judgments of experts, a decision tree, and a Monte Carlo simulation provided insight to a decision to implement an experimental flow release program. The technical evaluation of the expected costs and benefits of the program were integrated into the multi-stakeholder decision process. The framework assessed the magnitude of the uncertainty, its potential to affect water management decisions, the predictive ability of the experiment, the value of the expected costs and benefits, and the preferences of stakeholders for alternative outcomes. As a result of the analysis, the initial experimental design was revised, and a multi-stakeholder group reached consensus on a program of experimental flow releases to test the response of salmonids to flow. The approach treats adaptive management as a policy alternative within a broader decision problem, and it demonstrates the utility of combining expert judgment processes and stakeholder values with adaptive management to improve the likelihood that proposed experimental approaches will deliver net value to society.",0,2004,1,0.0,0.0,1.0,nw,1.0,1,1
47,Improving predictive accuracy with a combination of human intuition and mechanical decision aids,"This study examines the intuitive combination of human judgment and mechanical prediction under varied information conditions, As expected, mechanical prediction outperformed human intuition when based on the same information, but a combined approach was best when judges had access to relevant information not captured by the model (information asymmetry), The model was useful for differentiating between the event outcomes (improved slope), while eliminating the bias caused by base-rate neglect. Human intuition was useful for incorporating relevant information outside the scope of the model, resulting in improved slope and reduced judgment scatter. The addition of irrelevant information was detrimental to judgment accuracy, causing an increase in bias and a reduction in slope. These results provide insight into hour and when combining mechanical prediction and human intuition is likely to result in improved accuracy. (C) 1998 Academic Press.",10.1006/obhd.1998.2809,1998,0,0.0,0.0,1.0,nw,1.0,0,0
48,The Neuroscience of Implicit Moral Evaluation and Its Relation to Generosity in Early Childhood,"Despite cultural and individual variation, humans are a judgmental bunch [1]. There is accumulating evidence for early social and moral evaluation as shown by research with infants and children documenting the notion that some behaviors are perceived as right and others are perceived as wrong [2]. Moreover, social interactions are governed by a concern for fairness and others' well-being [3, 4]. However, although generosity increases between infancy and late childhood, it is less clear what mechanisms guide this change [5]. Early predispositions toward prosociality are thought to arise in concert with the social and cultural environment, developing into adult morality, a complex incorporation of emotional, motivational, and cognitive processes [6, 7]. Using EEG combined with eye tracking and behavioral sharing, we investigated, for the first time, the temporal neurodynamics of implicit moral evaluation in 3- to 5-year-old children. Results show distinct early automatic attentional (EPN) and later cognitively controlled (N2, LPP) patterns of neural response while viewing characters engaging in helping and harming behaviors. Importantly, later (LPP), but not early (EPN), waveforms predicted actual generosity. These results shed light on theories of moral development by documenting the respective contribution of automatic and cognitive neural processes underpinning social evaluation and directly link these neural computations to prosocial behavior in children.",10.1016/j.cub.2014.11.002,2015,0,0.0,0.0,0.0,nw,0.0,1,0
49,"Scenarios, uncertainty and conditional forecasts of the world population","Current official population forecasts differ little from those that Whelpton made 50 years ago either in the cohort-component methodology used or in the arguments used to motivate the assumptions. However, Whelpton produced some of the most erroneous forecasts of this century. This suggests that current forecasters should ensure that they give users an assessment of the uncertainty of their forecasts. We show how simple statistical methods can be combined with expert judgment to arrive at an overall predictive distribution for the future population. We apply the methods to a world population forecast that was made in 1994. Accepting that point forecast, we find that the probability is only about 2% that the world population in the year 2030 will be less than the low scenario of 8317 million. The probability that the world population will exceed the high scenario of 10736 million is about 13%. Similarly, the probability is only about 51% that the high-low interval of a recent United Nations (UN) forecast will contain the true population in the year 2025. Even if we consider the UN high-low intervals as conditional on the possible future policies of its member states, they appear to have a relatively small probability of encompassing the future population.",10.1111/1467-985X.00046,1997,1,0.0,0.0,0.0,gcg,0.0,0,0
50,Calibration and evaluation of five indicators of benthic community condition in two California bay and estuary habitats,"Many types of indices have been developed to assess benthic invertebrate community condition, but there have been few studies evaluating the relative performance of different index approaches. Here we calibrate and compare the performance of five indices: the Benthic Response Index (BRI), Benthic Quality Index (BQI), Relative Benthic Index (RBI), River Invertebrate Prediction and Classification System (RIVPACS), and the Index of Biotic Integrity (1131). We also examine whether index performance improves when the different indices, which rely on measurement of different properties, are used in combination. The five indices were calibrated for two geographies using 238 samples from southern California marine bays and 125 samples from polyhaline San Francisco Bay. Index performance was evaluated by comparing index assessments of 35 sites to the best professional judgment of nine benthic experts. None of the individual indices performed as well as the average expert in ranking sample condition or evaluating whether benthic assemblages exhibited evidence of disturbance. However, several index combinations outperformed the average expert. When results from both habitats were combined, two four-index combinations and a three-index combination performed best. However, performance differences among several combinations were small enough that factors such as logistics can also become a consideration in index selection. (C) 2008 Elsevier Ltd. All rights reserved.",10.1016/j.marpolbul.2008.11.007,2009,0,0.0,0.0,1.0,nw,1.0,0,0
51,Using hidden multi-state Markov models with multi-parameter volcanic data to provide empirical evidence for alert level decision-support,"For the purposes of eruption forecasting and hazard mitigation, a volcanic crisis may be represented as a staged progression of states of unrest, each with its own timescale and likelihood of transition to other states (or to climactic eruption). If the state conditions can be interpreted physically, e.g., in terms of advancing materials failure, this knowledge could be used directly to inform decisions on alert level setting. A multi-state Markov process provides one simple model for defining states and for estimating rates of switching between states. However, for eruptive processes, such states are not directly observable and must be inferred from latent markers, such as seismic activity, gas output, deformation rates, etc., some of which may be contradictory, Interpretations of uncertain data will be liable to error, so a model is needed which can simultaneously estimate both elements: the transition likelihood of a hidden process and the probabilities of state misclassification. We describe the concept and underlying principles of continuous-time hidden Markov models and demonstrate them in a decision-support context with a preliminary working implementation using MULTIMO data. Where multi-parameter streams of raw, processed or conditioned data of different kinds are available, these can be input in near real-time to appropriate hidden multi-state Markov models, the outputs of each providing their own objective analyses of eruptive state in probabilistic terms. These separate, multiple indicators of state can then be input into a Bayesian Belief Network framework for weighing and combining them as different strands of evidence, together with other observations, data, interpretations and expert judgment. (c) 2005 Elsevier B.V. All rights reserved.",10.1016/j.jvolgeores.2005.08.010,2006,0,0.0,0.0,0.0,ngr,0.0,1,0
52,Assessing the needs of patients in pain: A matter of opinion?,"Study Design. A prospective cohort study including patients with nonspecific spinal pain was performed. Objectives. To investigate whether the use of expert judgment in routine practice can provide a basis for reliable decision making concerning the need for intervention in patients with spinal pain and their ability to benefit from treatment. Summary of Background Data. A wide range of instruments and techniques are used to assess and treat patients with spinal pain. Many instruments are used without being clinimetrically tested. Methods. A questionnaire concerning the patients' need of treatment and their potential to assimilate it was sent to experts in the health care arena: physicians, physical therapists, social insurance officers. The experts included were those connected with patients participating in a larger outcome study. Two cohorts of patients (sample 1, n = 217; sample 2, n = 257) were followed for 6 and 12 months, during which time the patients' health and work status were mapped. Results. No acceptable agreement was found between any of the experts' ratings of patients' needs and potential for rehabilitation. Logistic regression showed that the experts' judgments were based almost solely on the age of thr: patient. The prediction analyses showed that the most consistent predictor of the patients' status at the B-month follow-up assessment was the patients' own belief in the existence of effective treatments and their perceived ability for learning to cope with the condition. Conclusions. Expert judgment as exercised in routine practice cannot be used as basis for reliable decision making concerning the need of the patient with spinal pain for intervention and the patient's ability to benefit from treatment.",10.1097/00007632-200011010-00015,2000,0,0.0,0.0,0.0,ngr,0.0,1,0
53,On the use of the analytic hierarchy process in the aggregation of expert judgments,"Expert judgments are involved in many aspects of scientific research, either formally or informally. In order to combine the different opinions elicited, simple aggregation methods have often been used with the result that expert biases, interexpert dependencies and other factors which might affect the judgments of the experts are often ignored. A more comprehensive approach, based on the analytic hierarchy process, is proposed in this paper to account for the large variety of factors influencing the experts. A structured hierarchy is constructed to decompose the overall problem in the elementary factors that can influence the expert's judgments. The importance of the different elements of the hierarchy is then assessed by pairwise comparison. The overall approach is simple, presents a systematic character and offers a good degree of flexibility. The approach provides the decision maker with a tool to quantitatively measure the significance of the judgments provided by the different experts involved in the elicitation. The resulting values can be used as weights in an aggregation scheme such as, for example, the simple weighted averaging scheme. Two applications of the approach are presented with reference to case studies of formal expert judgment elicitation previously analyzed in literature: the elicitation of the pressure increment in the containment building of the Sequoyah nuclear power plant following reactor vessel breach, and the prediction of the future changes in precipitation in the vicinity of Yucca Mountain. (C) 1996 Elsevier Science Limited.",10.1016/0951-8320(96)00060-9,1996,1,0.0,0.0,1.0,nw,1.0,1,1
54,Optic variables used to judge future ball arrival position in expert and novice soccer players,"Although many studies have looked at the perceptual-cognitive strategies used to make anticipatory judgments in sport, few have examined the informational invariants that our visual system may be attuned to. Using immersive interactive virtual reality to simulate the aerodynamics of the trajectory of a ball with and without sidespin, the present study examined the ability of expert and novice soccer players to make judgments about the ball's future arrival position. An analysis of their judgment responses showed how participants were strongly influenced by the ball's trajectory. The changes in trajectory caused by sidespin led to erroneous predictions about the ball's future arrival position. An analysis of potential informational variables that could explain these results points to the use of a first-order compound variable combining optical expansion and optical displacement.",10.3758/APP.71.3.515,2009,0,0.0,0.0,0.0,nw,0.0,1,0
55,Group processes in software effort estimation,"The effort required to complete software projects is often estimated, completely or partially, using the judgment of experts, whose assessment may be biased. In general, such bias as there is seems to be towards estimates that are overly optimistic. The degree of bias varies from expert to expert, and seems to depend on both Conscious and unconscious processes. One possible approach to reduce this bias towards over-optimism is to combine the judgments of several experts. This paper describes ail experiment in which experts with different backgrounds combined their estimates in group discussion. First, 20 software professionals were asked to provide individual estimates of the effort required for a software development project. Subsequently, they formed five estimation groups, each consisting of four experts. Each of these groups agreed oil a project effort estimate via the pooling of knowledge in discussion. We found that the groups submitted less optimistic estimates than the individuals. Interestingly, the group discussion-based estimates were closer to the effort expended on the actual project than the average of the individual expert estimates were, i.e., the group discussions led to better estimates than a mechanical averaging of the individual estimates. The groups' ability to identify a greater number of the activities required by the project is among the possible explanations for this reduction of bias.",10.1023/B:EMSE.0000039882.39206.5a,2004,0,0.0,1.0,0.0,ngr,1.0,0,0
56,Current and future pavement maintenance prioritization based on rapid visual condition evaluation,"Satisfactory maintenance of its highway network is essential for any nation's economic growth. A pavement management system (PMS) formulated according to specific needs and resources of a particular highway maintenance agency would assure satisfactory pavement performance with minimal maintenance cost. Since the collection of detailed pavement condition data is extremely costly and time-consuming, innovative approaches for rapid data collection is in increasing demand among highway agencies with limited PMS budgets. A time-saving and effective data collection approach based on subjective judgment is introduced by the writers for rating predominant distress types found in asphaltic pavements. Inclusion of both severity and extent ratings of distresses is expected to provide a strong basis for eventual maintenance cost computations. The mathematical techniques of fuzzy sets are used to deal with the subjectivity associated with human judgment of distress severity and extent. In addition, the relative importance of each distress type with respect to maintenance is also utilized in the determination of the combined condition index. Several fuzzy aggregation and ranking approaches are explored and the one with the highest computational efficiency is employed for ranking pavement sections with respect to rehabilitation needs. Finally, a fuzzy pavement condition forecasting model is also developed by incorporating subjective probability assessments regarding pavement condition deterioration rates, in the Markov transition process. Specific transition probability matrices for different distress types are used in this approach to overcome the deficiencies of the traditional PCI approach. The potential applicability of the methodology is tested on the major pavement network of Sri Lanka and its effectiveness and the execution ease are demonstrated.",10.1061/(ASCE)0733-947X(2001)127:2(116),2001,0,0.0,0.0,0.0,ngr,0.0,1,0
57,Fuzzy decision support system for demand forecasting with a learning mechanism,"In this paper, a new decision support system for demand forecasting DSS-DF is presented. A demand forecast is generated in DSS-DF by combining four forecasts values. Two of them are obtained independently, one by a customer and the other by a market expert. They represent subjective judgments on future demand, given as linguistic values, such as ""demand is around a certain value"" or ""demand is not lower than a certain value"", etc. Two additional forecasts are crisp values, obtained using conventional statistical methods, one using time-series analysis based on decomposition (TSAD), and the other using an auto regressive integrated moving average (ARMA) model. The combination of these four forecast values into one improved forecast is made by applying fuzzy IF-THEN rules. A modified Mamdani-style inference is used, which enables reasoning with fuzzy inputs. A new learning mechanism is developed and incorporated into the DSS_DF to adapt the rule bases that combine the individual forecasted values. The rule bases are adapted taking into consideration the performance of each of the forecast methods recorded in the past. The application of DSS_DF is demonstrated by an illustrative example. The forecasts obtained by DSS_DF are compared with results procured by applying the conventional TSAD and ARMA methods separately. The results obtained are encouraging and indicate that combining forecasts obtained by different methods may be beneficial. (C) 2006 Elsevier B.V. All rights reserved.",10.1016/j.fss.2006.03.011,2006,1,1.0,0.0,0.0,gcg,1.0,1,1
58,ViS(3): an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices,"Algorithms for video quality assessment (VQA) aim to estimate the qualities of videos in a manner that agrees with human judgments of quality. Modern VQA algorithms often estimate video quality by comparing localized space-time regions or groups of frames from the reference and distorted videos, using comparisons based on visual features, statistics, and/or perceptual models. We present a VQA algorithm that estimates quality via separate estimates of perceived degradation due to (1) spatial distortion and (2) joint spatial and temporal distortion. The first stage of the algorithm estimates perceived quality degradation due to spatial distortion; this stage operates by adaptively applying to groups of spatial video frames the two strategies from the most apparent distortion algorithm with an extension to account for temporal masking. The second stage of the algorithm estimates perceived quality degradation due to joint spatial and temporal distortion; this stage operates by measuring the dissimilarity between the reference and distorted videos represented in terms of two-dimensional spatiotemporal slices. Finally, the estimates obtained from the two stages are combined to yield an overall estimate of perceived quality degradation. Testing on various video-quality databases demonstrates that our algorithm performs well in predicting video quality and is competitive with current state-of-the-art VQA algorithms. (C) 2014 SPIE and IS&T",10.1117/1.JEI.23.1.013016,2014,0,0.0,0.0,0.0,ngr,0.0,1,0
59,Predicting Project Velocity in XP Using a Learning Dynamic Bayesian Network Model,"Bayesian networks, which can combine sparse data, prior assumptions, and expert judgment into a single causal model, have already been used to build software effort prediction models. We present such a model of an Extreme Programming environment and show how it can learn from project data in order to make quantitative effort predictions and risk assessments without requiring any additional metrics collection program. The model's predictions are validated against a real-world industrial project, with which they are in good agreement.",10.1109/TSE.2008.76,2009,0,0.0,0.0,0.0,gcg,0.0,1,0
60,A model to predict fish quality from instrumental features,"Sensorial evaluation of fishes, using a well-defined scheme including several qualitative attributes, can give a reliable quantitative evaluation of freshness (quality index method, QIM). The introduction of artificial instruments, mimicking human senses, seems a promising approach to obtain a comparable judgment with trained panels one. The outputs of colour, texture and electronic nose measurements can be compared and combined by data fusion, to construct an artificial quality index (AQI), describing the quality of fish at least as well as the QIM predicted. In this paper, the application of such approach to build a fish freshness indicator of sardine fishes is illustrated. (c) 2005 Published by Elsevier B.V.",10.1016/j.snb.2005.06.028,2005,0,0.0,0.0,0.0,gcg,0.0,1,0
61,Predicting the Future as Bayesian Inference: People Combine Prior Knowledge With Observations When Estimating Duration and Extent,"Predicting the future is a basic problem that people have to solve every day and a component of planning, decision making, memory, and causal reasoning. In this article, we present 5 experiments testing a Bayesian model of predicting the duration or extent of phenomena from their current state. This Bayesian model indicates how people should combine prior knowledge with observed data. Comparing this model with human judgments provides constraints on possible algorithms that people might use to predict the future. In the experiments, we examine the effects of multiple observations, the effects of prior knowledge, and the difference between independent and dependent observations, using both descriptions and direct experience of prediction problems. The results indicate that people integrate prior knowledge and observed data in a way that is consistent with our Bayesian model, ruling out some simple heuristics for predicting the future. We suggest some mechanisms that might lead to more complete algorithmic-level accounts.",10.1037/a0024899,2011,0,0.0,0.0,0.0,nw,0.0,1,0
62,Analysis of cognitive performance in schizophrenia patients and healthy individuals with unsupervised clustering models,"Currently, assignment of cognitive test results to particular cognitive domains is guided by theoretical considerations and expert judgments which may vary. More objective means of classification may advance understanding of the relationships between test performance and the cognitive functions probed. We examined whether ""atheoretical"" analyses of cognitive test data can help identify potential hidden structures in cognitive performance. Novel data-mining methods which ""let the data talk"" without a priori theoretically bound constraints were used to analyze neuropsychological test results of 75 schizophrenia patients and 57 healthy individuals. The analyses were performed on the combined sample to maximize the ""atheoretical"" approach and allow it to reveal different structures of cognition in patients and controls. Analyses used unsupervised clustering methods, including hierarchical clustering, self-organizing maps (SOM), k-means and supermagnetic clustering (SPC). The model revealed two major clusters containing accuracy and reaction time measures respectively. The sensitivity (75% versus 52%) and specificity (95% versus 77%) of these clusters for diagnosing schizophrenia differed. Downstream branching was influenced by stimulus domain. Predictions arising from this ""atheoretical"" model are supported by evidence from published studies. This preliminary study suggests that appropriate application of data-mining methods may contribute to investigation of cognitive functions. (c) 2007 Elsevier Ireland Ltd. All rights reserved.",10.1016/j.psychres.2007.06.009,2008,0,0.0,0.0,0.0,nw,0.0,1,0
63,Daily simulation of ozone and fine particulates over New York State: findings and challenges,"This study investigates the potential utility of the application of a photochemical modeling system in providing simultaneous forecasts of ozone (O-3) and fine particulate matter (PM2.5) over New York State. To this end, daily simulations from the Community Multiscale Air Quality (CMAQ) model for three extended time periods during 2004 and 2005 have been performed, and predictions were compared with observations of ozone and total and speciated PM2.5. Model performance for 8-h daily maximum O-3 was found to be similar to other forecasting systems and to be better than that for the 24-h-averaged total PM2.5. Both pollutants exhibited no seasonal differences in model performance. CMAQ simulations successfully captured the urban-rural and seasonal differences evident in observed total and speciated PM2.5 concentrations. However, total PM2.5 mass was strongly overestimated in the New York City metropolitan area, and further analysis of speciated observations and model predictions showed that most of this overprediction stems from organic aerosols and crustal material. An analysis of hourly speciated data measured in Bronx County, New York, suggests that a combination of uncertainties in vertical mixing, magnitude, and temporal allocation of emissions and deposition processes are all possible contributors to this overprediction in the complex urban area. Categorical evaluation of CMAQ simulations in terms of exceeding two different threshold levels of the air quality index (AQI) again indicates better performance for ozone than PM2.5 and better performance for lower exceedance thresholds. In most regions of New York State, the routine air quality forecasts based on observed concentrations and expert judgment show slightly better agreement with the observed distributions of AQI categories than do CMAQ simulations. However, CMAQ shows skill similar to these routine forecasts in terms of capturing the AQI tendency, that is, in predicting changes in air quality conditions. Overall, the results presented in this study reveal that additional research and development is needed to improve CMAQ simulations of PM2.5 concentrations over New York State, especially for the New York City metropolitan area. On the other hand, because CMAQ simulations capture urban-rural concentration gradients and day-to-day fluctuations in observed air quality despite systematic overpredictions in some areas, it would be useful to develop tools that combine CMAQ's predictive capability in terms of spatial concentration gradients and AQI tendencies with real-time observations of ambient pollutant levels to generate forecasts with higher temporal and spatial resolutions ( e. g., county level) than those of techniques based exclusively on monitoring data.",10.1175/JAM2520.1,2007,0,0.0,0.0,0.0,gcg,0.0,1,0
64,The thick and the thin of it: Contextual effects in body perception,"This research investigated how and when exposure to different body contexts alters body ideals used in judgment. University students judged the width and pleasantness of human figures, with context manipulated by presenting mostly narrow or mostly wide forms. In Experiment 1, silhouettes were presented simultaneously on the page. In Experiment 2, figures were presented successively on a computer screen as detailed pictures. In both experiments, width ratings were consistent with A. Parducci's (1995) range-frequency theory, which predicted the same figure is judged thinner in the wide context. Pleasantness ratings were well described by an ideal-point model that used a gaussian similarity function, with the ideal shifting to a narrower width in the narrow context. The assimilative shifts in ideals with mere exposure to a small set of contextual stimuli supports the hypothesis that mass media presentations of ultrathin individuals may lead women to adopt a ""thin ideal"" in judging themselves and others. Although exposure to narrow or wide sets of figures had no consistent effect on measures of body satisfaction, individual differences in body satisfaction were related to ideal-point shifts. Women who were dissatisfied with their own bodies consistently used thin ideals to judge body images and were insensitive to the contextual manipulation of body image.",10.1207/s15324834basp2703_3,2005,0,0.0,0.0,0.0,ngr,0.0,1,0
65,Combining statistical and judgmental forecasts via a web-based tourism demand forecasting system,"This paper introduces a web-based tourism demand forecasting system (TDFS) that is designed to forecast the demand for Hong Kong tourism, as measured by tourist arrivals, total and sectoral tourist expenditures, and the demand for hotel rooms. The TDFS process comprises three stages - preliminary data analysis, the generation of quantitative forecasts and judgmental adjustments - which correspond to the three key system components: the data module, the quantitative forecasting module and the judgmental forecasting module, respectively. These stages (modules) interact with one another. This paper focuses on a recent case study that illustrates the functional ability of the TDFS as a support system, providing accurate forecasts of the demand for Hong Kong tourism. Specifically, the quantitative forecasts are generated by the autoregressive distributed lag model, then adjusted by a panel of experts comprising postgraduate students and academic staff. The results show that this combination of quantitative and judgmental forecasts improves the overall forecasting accuracy. (C) 2012 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.",10.1016/j.ijforecast.2011.12.003,2013,1,0.0,1.0,0.0,ngr,1.0,1,1
66,A study on predicting hazard-factors for safe driving,This paper proposes an algorithm for detecting objects representing potential hazards to drivers based on the combination of local information derived from optical flows and global information obtained from the host vehicle's status. The algorithm uses artificial neural networks to infer the degree of danger posed by moving objects in dynamic images taken with a vehicle-mounted camera. This approach allows more flexible adaptation of the algorithm to many drivers with dissimilar characteristics. Experiments were conducted with both miniature vehicles in a virtual environment and real vehicles in a real driving situation using video images of multiple moving objects. The results show that the algorithm can infer hazardous situations similar to the judgments made by human drivers. The proposed algorithm provides the foundation for constructing a practical driving assistance system.,10.1109/TIE.2007.891651,2007,0,0.0,0.0,0.0,gcg,0.0,1,0
67,Role thinking: Standing in other people's shoes to forecast decisions in conflicts,"When forecasting decisions in conflict situations, experts are often advised to figuratively stand in the other person's shoes. We refer to this as ""role thinking"", because, in practice, the advice is to think about how other protagonists will view the situation in order to predict their decisions. We tested the effect of role thinking on forecast accuracy. We obtained 101 role-thinking forecasts of the decisions that would be made in nine diverse conflicts from 27 Naval postgraduate students (experts) and 107 role-thinking forecasts from 103 second-year organizational behavior students (novices). The accuracy of the novices' forecasts was 33% and that of the experts' was 31%; both were little different from chance (guessing), which was 28%. The small improvement in accuracy from role-thinking strengthens the finding from earlier research that it is not sufficient to think hard about a situation in order to predict the decisions which groups of people will make when they are in conflict. Instead, it is useful to ask groups of role players to simulate the situation. When groups of novice participants adopted the roles of protagonists in the aforementioned nine conflicts and interacted with each other, their group decisions predicted the actual decisions with an accuracy of 60%. (C) 2010 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.",10.1016/j.ijforecast.2010.05.001,2011,0,1.0,0.0,0.0,gcg,1.0,0,0
68,An illusion predicted by V1 population activity implicates cortical topography in shape perception,"Mammalian primary visual cortex (V1) is topographically organized such that the pattern of neural activation in V1 reflects the location and spatial extent of visual elements in the retinal image, but it is unclear whether this organization contributes to visual perception. We combined computational modeling, voltage-sensitive dye imaging (VSDI) in behaving monkeys and behavioral measurements in humans to investigate whether the large-scale topography of V1 population responses influences shape judgments. Specifically, we used a computational model to design visual stimuli that had the same physical shape, but were predicted to elicit variable V1 response spread. We confirmed these predictions with VSDI. Finally, we designed a behavioral task in which human observers judged the shapes of these stimuli and found that their judgments were systematically distorted by the spread of V1 activity. This illusion suggests that the topographic pattern of neural population responses in visual cortex contributes to visual perception.",10.1038/nn.3517,2013,0,0.0,0.0,0.0,gcg,0.0,1,0
69,A framework of integrating gene relations from heterogeneous data sources: an experiment on Arabidopsis thaliana,"One of the most important goals of biological investigation is to uncover gene functional relations. In this study we propose a framework for extraction and integration of gene functional relations from diverse biological data sources, including gene expression data, biological literature and genomic sequence information. We introduce a two-layered Bayesian network approach to integrate relations from multiple sources into a genome-wide functional network. An experimental study was conducted on a test-bed of Arabidopsis thaliana. Evaluation of the integrated network demonstrated that relation integration could improve the reliability of relations by combining evidence from different data sources. Domain expert judgments on the gene functional clusters in the network confirmed the validity of our approach for relation integration and network inference.",10.1093/bioinformatics/btl345,2006,0,0.0,0.0,0.0,ngr,0.0,1,0
70,Evidence-based guidelines for assessment of software development cost uncertainty,"Several studies suggest that uncertainty assessments of software development costs are strongly biased toward overconfidence, i.e., that software cost estimates typically are believed to be more accurate than they really are. This overconfidence may lead to poor project planning. As a means of improving cost uncertainty assessments, we provide evidence-based guidelines for how to assess software development cost uncertainty, based on results from relevant empirical studies. The general guidelines provided are: 1) Do not rely solely on unaided, intuition-based uncertainty assessment processes, 2) do not replace expert judgment with formal uncertainty assessment models, 3) apply structured and explicit judgment-based processes, 4) apply strategies based on an outside view of the project, 5) combine uncertainty assessments from different sources through group work, not through mechanical combination, 6) use motivational mechanisms with care and only if greater effort is likely to lead to improved assessments, and 7) frame the assessment problem to fit the structure of the relevant uncertainty information and the assessment process. These guidelines are preliminary and should be updated in response to new evidence.",10.1109/TSE.2005.128,2005,0,0.0,1.0,0.0,ngr,1.0,0,0
71,Statistical approaches in the development of clinical practice guidelines from expert panels - The case of laminectomy in sciatica patients,"BACKGROUND. Variation in expert opinion and lack of a systematic methodology hinder the development of reliable clinical practice guidelines. However standardized protocols have been defined to quantify, combine, and summarize expert judgments. In addition, statistical methods may help to outline guidelines based on simplified models of these judgments. METHODS. TO test this hypothesis, stepwise logistic regression (SLR) and classification tree pruning (CTP) were used to predict the results of two expert panels (USA 1992 and Switzerland 1995) on laminectomy in sciatica conditions. Both panels, using the RAND-UCLA explicit method, assessed whether the procedure would be inappropriate or of potential use in 720 case scenarios combining 7 relevant factors. RESULTS. Laminectomy was rated as inappropriate in 60% and 70% of the scenarios by the US and Swiss panels, respectively. Either statistical method, in both panels, based its simplest model on the same 4 factors, as follows: imaging test results; disability; neurological findings; and conservative treatment trials (in decreasing order); the influence of 2 other factors, duration of pain and nerve root irritation, were only marginal. The correct classification rates of the models were 89% and 93% for SLR and 93% and 85% for CTP. Adopting the CTP US algorithm as a guideline would lead to consider performing laminectomy only in patients with imaging evidence of hernia, relatively severe disability, reflex abnormalities, and previous nonsurgical treatment. Adherence to the corresponding CTP Swiss algorithm would result in less restrictive conditions. CONCLUSION. The statistical techniques proved as useful instruments to structure and simplify appropriateness criteria developed by expert panels and to outline parsimonious decision models for clinical practice.",10.1097/00005650-199908000-00008,1999,0,0.0,0.0,1.0,nw,1.0,0,0
72,Automatic feature identification and graphical support in rule-based forecasting: A comparison,"We examined automatic feature identification and graphical support in rule-based expert systems for forecasting. The rule-based expert forecasting system (RBEFS) includes predefined rules to automatically identify features of a time series and selects the extrapolation method to be used. The system call also integrate managerial judgment using a graphical interface that allows a user to view alternate extrapolation methods two at a time. The use of the RBEFS led to a significant improvement in accuracy compared to equal-weight combinations of forecasts. Further improvement were achieved with the user interface. For 6-year ahead ex ante forecasts, the rule-based expert forecasting system has a median absolute percentage error (MdAPE) 15% less than that of equally weighted combined forecasts and a 33% improvement over the random walk. The user adjusted forecasts had a MdAPE 20% less than that of the expert system. The results of the system are also compared to those of an earlier rule-based expert system which required human judgments about some features of the time series data. The results of the comparison of the two rule-based expert systems showed no significant differences between them.",10.1016/S0169-2070(96)00682-6,1996,0,0.0,0.0,1.0,nw,1.0,0,0
73,Statistical correction of judgmental point forecasts and decisions,"In many organizations point estimates labelled as 'forecasts' are produced by human judgment rather than statistical methods. However, when these estimates are subject to asymmetric loss they are, in fact, decisions because they involve the selection of a value with the objective of minimizing loss. While there are often considerable advantages in using judgment to arrive at these decisions the psychological demands of the task may mean that the resulting decisions are sub-optimal when compared with those resulting from a normative decision model. In these circumstances a combination of statistical methods and judgment may be superior. This paper suggests a procedure which involves the statistical correction of the original decision to obtain a forecast and the subsequent use of a mathematical model to identify the theoretically optimal decision in the light of this forecast. The application of the procedure to the monthly decisions of a manufacturing company suggests that it may offer the potential for achieving substantial improvements In many practical contexts. Copyright (C) 1996 Elsevier Science Ltd",10.1016/0305-0483(96)00028-X,1996,0,0.0,1.0,0.0,ngr,1.0,0,0
74,"The tool for the automatic analysis of text cohesion (TAACO): Automatic assessment of local, global, and text cohesion","This study introduces the Tool for the Automatic Analysis of Cohesion (TAACO), a freely available text analysis tool that is easy to use, works on most operating systems (Windows, Mac, and Linux), is housed on a user's hard drive (rather than having an Internet interface), allows for the batch processing of text files, and incorporates over 150 classic and recently developed indices related to text cohesion. The study validates TAACO by investigating how its indices related to local, global, and overall text cohesion can predict expert judgments of text coherence and essay quality. The findings of this study provide predictive validation of TAACO and support the notion that expert judgments of text coherence and quality are either negatively correlated or not predicted by local and overall text cohesion indices, but are positively predicted by global indices of cohesion. Combined, these findings provide supporting evidence that coherence for expert raters is a property of global cohesion and not of local cohesion, and that expert ratings of text quality are positively related to global cohesion.",10.3758/s13428-015-0651-7,2016,0,0.0,1.0,0.0,ngr,1.0,0,0
75,Neurobiological Mechanisms Underlying the Blocking Effect in Aversive Learning,"Current theories of classical conditioning assume that learning depends on the predictive relationship between events, not just on their temporal contiguity. Here we employ the classic experiment substantiating this reasoning-the blocking paradigm-in combination with functional magnetic resonance imaging (fMRI) to investigate whether human amygdala responses in aversive learning conform to these assumptions. In accordance with blocking, we demonstrate that significantly stronger behavioral and amygdala responses are evoked by conditioned stimuli that are predictive of the unconditioned stimulus than by conditioned stimuli that have received the same pairing with the unconditioned stimulus, yet have no predictive value. When studying the development of this effect, we not only observed that it was related to the strength of previous conditioned responses, but also that predictive compared with nonpredictive conditioned stimuli received more overt attention, as measured by fMRI-concurrent eye tracking, and that this went along with enhanced amygdala responses. We furthermore observed that prefrontal regions play a role in the development of the blocking effect: ventromedial prefrontal cortex (subgenual anterior cingulate) only exhibited responses when conditioned stimuli had to be established as nonpredictive for an outcome, whereas dorsolateral prefrontal cortex also showed responses when conditioned stimuli had to be established as predictive. Most importantly, dorsolateral prefrontal cortex connectivity to amygdala flexibly switched between positive and negative coupling, depending on the requirements posed by predictive relationships. Together, our findings highlight the role of predictive value in explaining amygdala responses and identify mechanisms that shape these responses in human fear conditioning.",10.1523/JNEUROSCI.1210-12.2012,2012,0,0.0,0.0,0.0,gcg,0.0,1,0
76,Predicting Future Sensorimotor States Influences Current Temporal Decision Making,"Accurate motor execution is achieved by estimating future sensory states via a forward model of limb dynamics. In the current experiment, we probed the time course over which state estimation evolves during movement planning by combining a bimanual arm crossing movement with a temporal order judgment (TOJ) task. Human participants judged which of two successive vibrotactile stimuli delivered to each index finger arrived first as they were preparing to either cross or uncross their hands. TOJ error rate was found to systematically vary in a time- and direction-dependent manner. When planning to cross the hands, error rate systematically increased as the vibrotactile stimuli were delivered closer in time to the onset of the movement. By contrast, planning to uncross the hands led to a gradual reduction in error rate as movement planning progressed. In both cases, these changes occurred before the actual alteration in hand configuration. We suggest that these systematic changes in error represent an interaction between the evolving state estimation processes and decisions regarding the timing of successive events.",10.1523/JNEUROSCI.0037-11.2011,2011,0,0.0,0.0,0.0,ngr,0.0,1,0
77,Multi-criteria decision analysis for supporting the selection of medical devices under uncertainty,"Innovative approaches to the assessment and management of medical technologies use a combination of health technology assessment (HTA) and operations research methods, specifically multiple-criteria decision analysis (MCDA). The purpose of this article is to develop methodological support and provide a theoretical justification for decision support in the selection of medical devices under conditions of uncertainty, using MRI systems as an example. The goal of the method application has been formulated as follows: determine a ranked list of MRI systems for contributory health organisations administered by regional authorities (regional hospitals) in the Czech Republic. An analytic hierarchy process (AHP) and the Delphi method were used to identify experts' preferences and for consensus building. The expert group was selected based on eight complex-valued criteria, and each expert was given a weighting factor. A set of 13 MRI systems and the 14 key default specifications that play the most important roles when hospitals select MRIs for purchase were defined. Strong conformity (W >= 0.6, p < 0.05) within the experts' judgments was revealed. A prediction regarding alternatives, weights and changes in priority vectors over the,following 8 years has been provided. The developed approach is useful in decision support when selecting medical devices under conditions of uncertainty by hospitals. (C) 2015 Elsevier B.V. and Association of European Operational Research Societies (EURO) within the International Federation of Operational Research Societies (IFORS). All rights reserved.",10.1016/j.ejor.2015.05.075,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
78,Development of an Antibiotic Spectrum Score Based on Veterans Affairs Culture and Susceptibility Data for the Purpose of Measuring Antibiotic De-Escalation: A Modified Delphi Approach,"OBJECTIVE. Development of a numerical score to measure the microbial spectrum of antibiotic regimens (spectrum score) and method to identify antibiotic de-escalation events based on application of the score. DESIGN. Web-based modified Delphi method. PARTICIPANTS. Physician and pharmacist antimicrobial stewards practicing in the United States recruited through infectious diseases-focused listservs. METHODS. Three Delphi rounds investigated: organisms and antibiotics to include in the spectrum score, operationalization of rules for the score, and de-escalation measurement. A 4-point ordinal scale was used to score antibiotic susceptibility for organism-antibiotic domain pairs. Antibiotic regimen scores, which represented combined activity of antibiotics in a regimen across all organism domains, were used to compare antibiotic spectrum administered early (day 2) and later (day 4) in therapy. Changes in spectrum score were calculated and compared with Delphi participants' judgments on de-escalation with 20 antibiotic regimen vignettes and with non-Delphi steward judgments on de-escalation of 300 pneumonia regimen vignettes. Method sensitivity and specificity to predict expert de-escalation status were calculated. RESULTS. Twenty-four participants completed all Delphi rounds. Expert support for concepts utilized in metric development was identified. For vignettes presented in the Delphi, the sign of change in score correctly classified de-escalation in all vignettes except those involving substitution of oral antibiotics. The sensitivity and specificity of the method to identify de-escalation events as judged by non-Delphi stewards were 86.3% and 96.0%, respectively. CONCLUSIONS. Identification of de-escalation events based on an algorithm that measures microbial spectrum of antibiotic regimens generally agreed with steward judgments of de-escalation status.",10.1086/677633,2014,0,0.0,0.0,0.0,ngr,0.0,1,0
79,Two Reasons to Make Aggregated Probability Forecasts More Extreme,"When aggregating the probability estimates of many individuals to form a consensus probability estimate of an uncertain future event, it is common to combine them using a simple weighted average. Such aggregated probabilities correspond more closely to the real world if they are transformed by pushing them closer to 0 or 1. We explain the need for such transformations in terms of two distorting factors: The first factor is the compression of the probability scale at the two ends, so that random error tends to push the average probability toward 0.5. This effect does not occur for the median forecast, or, arguably, for the mean of the log odds of individual forecasts. The second factor-which affects mean, median, and mean of log odds-is the result of forecasters taking into account their individual ignorance of the total body of information available. Individual confidence in the direction of a probability judgment (high/low) thus fails to take into account the wisdom of crowds that results from combining different evidence available to different judges. We show that the same transformation function can approximately eliminate both distorting effects with different parameters for the mean and the median. And we show how, in principle, use of the median can help distinguish the two effects.",10.1287/deca.2014.0293,2014,1,0.0,0.0,1.0,nw,1.0,1,1
80,Risk of surgery for subacromial impingement syndrome in relation to neck-shoulder complaints and occupational biomechanical exposures: a longitudinal study,"Objectives The aim of this longitudinal study was to evaluate the risk of surgery for subacromial impingement syndrome (SIS) in relation to neck-shoulder complaints and occupational biomechanical shoulder exposures. Methods The study was based on the Musculoskeletal Research Database at the Danish Ramazzini Centre. We linked baseline questionnaire information from 1993-2004 on neck-shoulder complaints, job titles, psychosocial work factors, body mass index, and smoking with register information on first-time surgery for SIS from 1996-2008. Biomechanical exposure measures were obtained from a job exposure matrix based on expert judgment. We applied multivariable Cox regression. Results During 280 125 person-years of follow-up among 37 402 persons, 557 first-time operations for SIS occurred. Crude surgery rates increased from 1.1 to 2.5 per 1000 person-years with increasing shoulder load. Using no neck-shoulder complaints and low shoulder load at baseline as a reference, no neck-shoulder complaints and high shoulder load showed an adjusted hazard ratio (HRadj) of 2.55 [95% confidence interval (95% CI) 1.59-4.09], while neck-shoulder complaints in combination with high shoulder load showed an HRadj of 4.52 (95% CI 2.87-7.13). Subanalyses based on 18 856 persons showed an HRadj of 5.40 (95% CI 2.88-10.11) for complaints located specifically in the shoulder in combination with high shoulder load. Conclusions Based on these findings, persons with neck-shoulder and especially shoulder complaints in combination with high shoulder load seem an obvious target group for interventions aimed at reducing exposures to prevent surgery for SIS.",10.5271/sjweh.3374,2013,0,0.0,0.0,1.0,nw,1.0,0,0
81,Using multi criteria decision making in analysis of alternatives for selection of enabling technology,"In September 2009 the U.S. Government Accountability Office (GAO) reported, Defense Acquisitions: Many Analyses of Alternatives Have Not Provided a Robust Assessment of Weapon System Options [U.S. Government Accountability Office, Pub. No. GAO-09-665, 2009, p. 1]. In their focused review of 32 Acquisition Category I programs, it was found that 10 did not conduct an Analysis of Alternatives (AoA), but rather focused on an already selected weapon system solution. Prior to Milestone A, the Department of Defense (DoD) requires that service sponsors conduct an Analysis of Alternatives (AoA). The AoA is an analytical comparison of multiple alternatives to be completed prior to committing and investing costly resources to one project or decision. Typically, however, sponsors will circumvent the process in an effort to save money or schedule, and capability requirements are proposed that are so specific that they effectively eliminate all but the preferred concepts, practically ignoring other alternatives. Decision making is one of the most challenging parts of Systems Engineering. How one feeds the decision making process is key to eliminating long term waste. About three-quarters of a programs total life cycle cost is influenced by decisions made before it is approved to start development[U.S. Government Accountability Office, Pub. No. GAO-09-665, 2009, pp. 2]. This study evaluates the positive benefits of defining the problem domain prior to expeditiously turning to the solution domain. The goal in any decision making process is to provide the decision maker with the ability to look into the future, and to make the best possible decision based on past and present information and future predictions. There is a need for approaches that combine available quantitative data with the more subjective knowledge of experts. Decision theory techniques have been successfully used for contrasting expert judgments and making educated choices for many years. For a successful analysis, one must focus on selection of specific criteria that are key performance drivers that can lead to an informed selection of the enabling technology. Understanding the requirements of the end state goal is key to a successful analysis and should also assist in the selection of key performance parameters. A case study example is presented to demonstrate a third tier AoA identifying an enabling technology using the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) while successfully accounting for tacit knowledge of expert practitioners. (c) 2012 Wiley Periodicals, Inc.",10.1002/sys.21233,2013,0,0.0,0.0,0.0,gcg,0.0,1,0
82,Modeling hypoxia in the Chesapeake Bay: Ensemble estimation using a Bayesian hierarchical model,"Quantifying parameter and prediction uncertainty in a rigorous framework can be an important component of model skill assessment. Generally, models with lower uncertainty will be more useful for prediction and inference than models with higher uncertainty. Ensemble estimation, an idea with deep roots in the Bayesian literature, can be useful to reduce model uncertainty. It is based on the idea that simultaneously estimating common or similar parameters among models can result in more precise estimates. We demonstrate this approach using the Streeter-Phelps dissolved oxygen sag model fit to 29 years of data from Chesapeake Bay. Chesapeake Bay has a long history of bottom water hypoxia and several models are being used to assist management decision-making in this system. The Bayesian framework is particularly useful in a decision context because it can combine both expert-judgment and rigorous parameter estimation to yield model forecasts and a probabilistic estimate of the forecast uncertainty. Published by Elsevier B.V.",10.1016/j.jmarsys.2008.05.008,2009,0,0.0,0.0,1.0,nw,1.0,0,0
83,A neural timing model of visual threshold,"A theory of visual threshold-versus-intensity curves is presented. The theory is based on a model of spike generation by idealized visual neurons. The spike generation model relates the interarrival time statistics of the neural spikes to the statistics of weighted photon summation within the receptive field of the spiking neuron, the spike generation model is combined with the psychophysical assumption that the observer monitors a neuron which is ideally matched to the target in both scale and location. The resulting model specifies how the interarrival time distribution depends on the incremental flash intensity, area, and duration of the threshold stimulus and on the background luminance. A further psychophysical assumption is then added to the model: that the observer uses a neural timing model in order to discriminate test trials from blanks. On the basis of the complete model, a theory of threshold-versus-intensity functions is derived, The model observer exhibits threshold behavior that mimics many aspects of the threshold judgments of human observers. In particular, the theoretical threshold functions exhibit: (1) at low background levels, absolute thresholds which depend inversely on test area and duration: (2) at moderate background levels, de Vries-Rose behavior if and only if the test flash is sufficiently small and brief; and (3) in the limit of high background levels, asymptotic Weber behavior. The theory also accounts for changes in the shape of the threshold-versus-intensity curve as test flash area (or spatial frequency) and duration are altered, It is argued that empirical failures to find Weber behavior at high backgrounds under some stimulus conditions (high spatial frequency or small targets, brief targets) may be due to the occurrence of photoreceptor saturation at background levels that are below the range in which Weber behavior would be predicted by the (nonsaturating) model presented here. (C) 1996 Academic Press, Inc.",10.1006/jmps.1996.0001,1996,0,0.0,0.0,0.0,nw,0.0,1,0
84,Visual motion aftereffects arise from a cascade of two isomorphic adaptation mechanisms,"Prolonged exposure to a moving stimulus can substantially alter the perceived velocity ( both speed and direction) of subsequently presented stimuli. Here, we show that these changes can be parsimoniously explained with a model that combines the effects of two isomorphic adaptation mechanisms, one nondirectional and one directional. Each produces a pattern of velocity biases that serves as an observable ""signature"" of the corresponding mechanism. The net effect on perceived velocity is a superposition of these two signatures. By examining human velocity judgments in the context of different adaptor velocities, we are able to separate these two signatures. The model. fits the data well, successfully predicts subjects' behavior in an additional experiment using a nondirectional adaptor, and is in agreement with a variety of previous experimental results. As such, the model provides a unifying explanation for the diversity of motion aftereffects.",10.1167/9.9.9,2009,0,0.0,0.0,0.0,gcg,0.0,1,0
85,Location memory in the real world: Category adjustment effects in 3-dimensional space,"The ability to remember spatial locations is critical to human functioning, both in an evolutionary and in an everyday sense. Yet spatial memories and judgments often show systematic errors and biases. Bias has been explained by models such as the Category Adjustment model (CAM), in which fine-grained and categorical information about locations are combined in a Bayesian manner (Huttenlocher, Hedges, & Duncan, 1991). However, experiments testing this model have largely used locations contained in simple geometric shapes and, more recently, 2D scenes. Do the results generalize to location memory in the complex natural world, as they should if the CAM is to provide an over-arching framework for thinking about spatial memory? Here, this issue is addressed using a novel extension of the location memory paradigm that allows for testing of location memory in an everyday, 3D environment. The results support two predictions of the CAM: that memory for locations is biased toward central values, and that the magnitude of error increases with the retention interval. (C) 2013 Elsevier B.V. All rights reserved.",10.1016/j.cognition.2013.02.016,2013,0,0.0,0.0,0.0,gcg,0.0,1,0
86,A Behavioral Probabilistic Risk Assessment Framework for Managing Autonomous Underwater Vehicle Deployments,"The deployment of a deep-diving long-range autonomous underwater vehicle (AUV) is a complex operation that requires the use of a risk-informed decision-making process. Operational risk assessment is heavily dependent on expert subjective judgment. Expert judgments can be elicited either mathematically or behaviorally. During mathematical elicitation experts are kept separate and provide their assessment individually. These are then mathematically combined to create a judgment that represents the group view. The limitation with this approach is that experts do not have the opportunity to discuss different views and thus remove bias from their assessment. In this paper, a Bayesian behavioral approach to estimate and manage AUV operational risk is proposed. At an initial workshop, behavioral aggregation, that is, reaching agreement on the distributions of risks for faults or incidents, is followed by an agreed upon initial estimate of the likelihood of success of the proposed risk mitigation methods. Postexpedition, a second workshop assesses the new data and compares observed to predicted risk, thus updating the prior estimate using Bayes' rule. This feedback further educates the experts and assesses the actual effectiveness of the mitigation measures. Applying this approach to an AUV campaign in ice-covered waters in the Arctic showed that the maximum error between the predicted and the actual risk was 9% and that the experts' assessments of the effectiveness of risk mitigation led to a maximum of 24% in risk reduction.",10.1175/JTECH-D-12-00005.1,2012,1,0.0,1.0,0.0,ngr,1.0,1,1
87,Incorporating subjective and stochastic uncertainty in an interactive multi-objective groundwater calibration framework,"The interactive multi-objective genetic algorithm (IMOGA) combines traditional optimization with an interactive framework that considers the subjective knowledge of hydro-geological experts in addition to quantitative calibration measures such as calibration errors and regularization to solve the groundwater inverse problem. The IMOGA is inherently a deterministic framework and identifies multiple large-scale parameter fields (typically head and transmissivity data are used to identify transmissivity fields). These large-scale parameter fields represent the optimal trade-offs between the different criteria (quantitative and qualitative) used in the IMOGA. This paper further extends the IMOGA to incorporate uncertainty both in the large-scale trends as well as the small-scale variability (which can not be resolved using the field data) in the parameter fields. The different parameter fields identified by the IMOGA represent the uncertainty in large-scale trends, and this uncertainty is modeled using a Bayesian approach where calibration error, regularization, and the expert's subjective preference are combined to compute a likelihood metric for each parameter field. Small-scale (stochastic) variability is modeled using a geostatistical approach and added onto the large-scale trends identified by the IMOGA. This approach is applied to the Waste Isolation Pilot Plant (WIPP) case-study. Results, with and without expert interaction, are analyzed and the impact that expert judgment has on predictive uncertainty at the WIPP site is discussed. It is shown that for this case, expert interaction leads to more conservative solutions as the expert compensates for some of the lack of data and modeling approximations introduced in the formulation of the problem.",10.1007/s00477-010-0384-1,2010,0,0.0,0.0,0.0,ngr,0.0,1,0
88,An application of judgment analysis to examination marking in psychology,"Statistical combinations of specific measures have been shown to be superior to expert judgment in several fields. In this study, judgment analysis was applied to examination marking to investigate factors that influenced marks awarded and contributed to differences between first and second markers. Seven markers in psychology rated 551 examination answers on seven 'aspects' for which specific assessment criteria had been developed to support good practice in assessment. The aspects were: addressing the question, covering the area, understanding, evaluation, development of argument, structure and organization, and clarity. Principal-components analysis indicated one major factor and no more than two minor factors underlying the seven aspects. Aspect ratings were used to predict overall marks, using multiple regression to 'capture' the marking policies of individual markers. These varied from marker to marker in terms of the numbers of aspect ratings that made independent contributions to the prediction of overall marks and the extent to which aspect ratings explained the variance in overall marks. The number of independently predictive aspect ratings, and the amount of variance in overall marks explained by aspect ratings, were consistently higher for first markers (question setters) than for second markers. Co-markers' overall marks were then used as an external criterion to test the extent to which a simple model consisting of the sum of the aspect ratings improved on overall marks in the prediction of co-markers marks. The model significantly increased the variance in co-markers' marks accounted for, but only for second markers, who had not taught the material and had not set the question. Further research is needed to develop the criteria and especially to establish the reliability and validity of specific aspects of assessment. The present results support the view that, for second markers at least, combined measures of specific aspects of examination answers may help to improve the reliability of marking.",10.1348/000712602760146233,2002,0,0.0,1.0,0.0,ngr,1.0,0,0
89,Life cycle assessment of fuel cell vehicles - A methodology example of input data treatment for future technologies,"Life cycle assessment (LCA) will always involve some subjectivity and uncertainty. This reality is especially true when the analysis concerns new technologies. Dealing with uncertainty can generate richer information and minimize some of the result mismatches currently encountered in the literature. As a way of analyzing future fuel cell vehicles and their potential new fuels, the Fuel Upstream Energy and Emission Model (FUEEM) developed at the University of California - Davis, pioneered two different ways to incorporate uncertainty into the analysis. First, the model works with probabilistic curves as inputs and with Monte Carlo simulation techniques to propagate the uncertainties. Second, the project involved the interested parties in the entire process, not only in the critical review phase. The objective of this paper is to present, as a case study, the tools and the methodologies developed to acquire most of the knowledge held by interested parties and to deal with their eventually conflicted - interests. The analysis calculation methodology, the scenarios, and all assumed probabilistic curves were derived from a consensus of an international expert network discussion, using existing data in the literature along with new information collected from companies. The main part of the expert discussion process uses a variant of the Delphi technique, focusing on the group learning process through the information feedback feature. A qualitative analysis indicates that a higher level of credibility and a higher quality of information can be achieved through a more participatory process. The FUEEM method works well within technical information and also in establishing a reasonable set of simple scenarios. However, for a complex combination of scenarios, it will require some improvement. The time spent in the process was the major drawback of the method and some alternatives to share this time cost are suggested.",10.1065/lca2002.02.074,2002,0,0.0,0.0,1.0,nw,1.0,0,0
90,Out-of-Sample Validation for Structured Expert Judgment of Asian Carp Establishment in Lake Erie,"Structured expert judgment (SEJ) is used to quantify the uncertainty of nonindigenous fish (bighead carp [Hypophthalmichthys nobilis] and silver carp [H. molitrix]) establishment in Lake Erie. The classical model for structured expert judgment model is applied. Forming a weighted combination (called a decision maker) of experts' distributions, with weights derived from performance on a set of calibration variables from the experts' field, exhibits greater statistical accuracy and greater informativeness than simple averaging with equal weights. New methods of cross validation are applied and suggest that performance characteristics relative to equal weighting could be predicted with a small number (1-2) of calibration variables. The performance-based decision maker is somewhat degraded on out-of-sample prediction, but remained superior to the equal weight decision maker in terms of statistical accuracy and informativeness. Integr Environ Assess Manag 2014;10:522-528. (c) 2014 The Authors. Integrated Environmental Assessment and Management published by Wiley Periodicals, Inc. on behalf of SETAC.",10.1002/ieam.1559,2014,1,0.0,1.0,0.0,ngr,1.0,1,1
91,"Morpho-dynamics of the Brahmaputra-Jamuna River, Bangladesh","The Jamuna River is the downstream continuation of the Brahmaputra in Bangladesh. It is one of the largest sand-bed braided rivers in the world and every year it erodes thousand hectares of mainland floodplain, rendering tens of thousands of people landless and/or homeless. Understanding the morpho-dynamics of this river and its responses to the various drivers of morphological change that act on it is essential to improving the livelihoods of millions of floodplain dwellers in Bangladesh, especially given the threats posed by climate change. Reliable data, information and knowledge of river process are sparse and so progress in linking the impacts of multiple drivers (including neo-tectonics, earthquakes, large-scale avulsions and engineering interventions) to complex morphological responses depends on making best use of historical maps, time-series satellite images, hydro-morphological data, expert judgment and local knowledge. This paper draws on all these sources to chronicle the morphological evolution of the Jamuna River since the avulsion that created it about 200 years ago, and to establish temporal trends and spatial patterns in the changes that have characterized process-response mechanisms in this fluvial system since then. The understanding gained from these investigations then supports deeper analyses to: explain how historical migration of the river westward has produced significant contrasts between left and right (west) bank material properties; elucidate the relationships between discharge, fluvial processes, anabranch instability and floodplain erosion rates, and; identify causal links between drivers and morphological responses at a variety of time and space scales. Finally, the new knowledge generated by the analyses developed herein are combined with existing, conceptual and empirical process-response models for the Jamuna to predict possible future morphological adjustments in ways helpful in identifying appropriate strategies for climate change adaptation in Bangladesh. The enhanced knowledge gained from these historical and contemporary investigations may also be useful in assessing the impacts of natural and anthropogenic drivers on other large, braided rivers. (C) 2013 Elsevier B.V. All rights reserved.",10.1016/j.geomorph.2013.07.025,2014,0,0.0,0.0,0.0,ngr,0.0,1,0
92,Brain Substrates of Recovery from Misleading Influence,"Humans are strongly influenced by their environment, a dependence that can lead to errors in judgment. Although a rich literature describes how people are influenced by others, little is known regarding the factors that predict subsequent rectification of misleading influence. Using a mediation model in combination with brain imaging, we propose a model for the correction of misinformation. Specifically, our data suggest that amygdala modulation of hippocampal mnemonic representations, during the time of misleading social influence, is associated with reduced subsequent anterior-lateral prefrontal cortex activity that reflects correction. These findings illuminate the process by which erroneous beliefs are, or fail to be, rectified and highlight how past influence constrains subsequent correction.",10.1523/JNEUROSCI.4720-13.2014,2014,0,0.0,0.0,0.0,ngr,0.0,1,0
93,The vision heuristic: Judging music ensembles by sight alone,"Team effectiveness and group performance are often defined by standards set by domain experts. Professional musicians consistently report that sound output is the most important standard for evaluating the quality of group performance in the domain of music. However, across six studies, visual information dominated rapid judgments of group performance. Participants (1062 experts and novices) were able to select the actual winners of live ensemble competitions and distinguish top-ranked orchestras from non-ranked orchestras based on 6-s silent video recordings yet were unable to do so from sound recordings or recordings with both video and sound. These findings suggest that judgments of group performance in the domain of music are driven at least in part by visual cues about group dynamics and leadership. (C) 2013 The Author. Published by Elsevier Inc. All rights reserved.",10.1016/j.obhdp.2013.10.003,2014,0,0.0,0.0,0.0,nw,0.0,1,0
94,Crowding follows the binding of relative position and orientation,"Crowding-the deleterious influence of clutter on object recognition-disrupts the identification of visual features as diverse as orientation, motion, and color. It is unclear whether this occurs via independent feature-specific crowding processes (preceding the feature binding process) or via a singular (late) mechanism tuned for combined features. To examine the relationship between feature binding and crowding, we measured interactions between the crowding of relative position and orientation. Stimuli were a target cross and two flanker crosses (each composed of two near-orthogonal lines), 15 degrees in the periphery. Observers judged either the orientation (clockwise/counterclockwise) of the near-horizontal target line, its position (up/down relative to the stimulus center), or both. For single-feature judgments, crowding affected position and orientation similarly: thresholds were elevated and responses biased in a manner suggesting that the target appeared more like the flankers. These effects were tuned for orientation, with near-orthogonal elements producing little crowding. This tuning allowed us to separate the predictions of independent (feature specific) and combined (singular) models: for an independent model, reduced crowding for one feature has no effect on crowding for other features, whereas a combined process affects either all features or none. When observers made conjoint judgments, a reduction of orientation crowding (by increasing target-flanker orientation differences) increased the rate of correct responses for both position and orientation, as predicted by our combined model. In contrast, our independent model incorrectly predicted a high rate of position errors, since the probability of positional crowding would be unaffected by changes in orientation. Thus, at least for these features, crowding is a singular process that affects bound position and orientation values in an all-or-none fashion. Keywords: crowding, orientation, position, feature binding, peripheral visual field",10.1167/12.3.18,2012,0,0.0,0.0,0.0,ngr,0.0,1,0
95,Methods for assessing uncertainty in fundamental assumptions and associated models for cancer risk assessment,"The distributional approach for uncertainty analysis in cancer risk assessment is reviewed and extended. The method considers a combination of bioassay study results, targeted experiments, and expert judgment regarding biological mechanisms to predict a probability distribution for uncertain cancer risks. Probabilities are assigned to alternative model components, including the determination of human carcinogenicity, mode of action, the dosimetry measure for exposure, the mathematical form of the dose-response relationship, the experimental data set(s) used to fit the relationship, and the formula used for interspecies extrapolation. Alternative software platforms for implementing the method are considered, including Bayesian belief networks (BBNs) that facilitate assignment of prior probabilities, specification of relationships among model components, and identification of all output nodes on the probability tree. The method is demonstrated using the application of Evans, Sielken, and co-workers for predicting cancer risk from formaldehyde inhalation exposure. Uncertainty distributions are derived for maximum likelihood estimate (MLE) and 95th percentile upper confidence limit (UCL) unit cancer risk estimates, and the effects of resolving selected model uncertainties on these distributions are demonstrated, considering both perfect and partial information for these model components. A method for synthesizing the results of multiple mechanistic studies is introduced, considering the assessed sensitivities and selectivities of the studies for their targeted effects. A highly simplified example is presented illustrating assessment of genotoxicity based on studies of DNA damage response caused by naphthalene and its metabolites. The approach can provide a formal mechanism for synthesizing multiple sources of information using a transparent and replicable weight-of-evidence procedure.",10.1111/j.1539-6924.2008.01134.x,2008,1,0.0,0.0,0.0,gcg,0.0,0,0
96,Predictive assessment of fish health and fish kills in the Neuse River Estuary using elicited expert judgment,"Declining fish health and the occurrence of large fish kills are some of the more publicly meaningful indicators of water quality in the impaired Neuse River Estuary, North Carolina. It is generally believed that such problems are caused by the widespread depletion of dissolved oxygen-an indirect result of anthropogenic nutrient pollution. However, the development of scientific simulation models to predict how improvements in oxygen conditions will improve the health of fish and reduce the frequency of fish kills has proven elusive. As a pragmatic solution to this problem, the expert opinion of estuarine fisheries scientists in possession of relevant data and experience was elicited. The relations between joint and conditional probabilities were exploited to translate quantities that are normally hard to assess into quantities that can be drawn more directly from the experiential knowledge of the experts. A combined model of expert opinion was constructed as an influence diagram, and Monte Carlo simulation was used to generate predictions of fish health and fish kills in the Neuse River Estuary under current and improved oxygen conditions. Full model results are expressed as probability distributions, capturing the effects of natural variability and knowledge uncertainty-both contributors to total ecological risk.",10.1080/10807030490438454,2004,1,0.0,0.0,1.0,nw,1.0,1,1
97,"Smart growth and transit-oriented development planning in site selection for a new metro transit station in Taipei, Taiwan","In recent years, the application of transit-oriented development (TOD) concept to urban development has been proposed based on the planning principles of smart growth and sustainable development The development of appropriate design techniques for the surrounded built environment of TOD has become increasingly important as TOD concepts apply to urban development The available evidence lends itself to the argument that a combined design strategies and TOD patterns planning approach that promotes the quality of urban built environment will help create active, healthier, and more livable communities. This is an essential element of this research. The TOD design strategies can be proposed by utilizing the supply side prediction methodologies of planning. There has also been an increasing interest in the urban built environment design in the past decade. This interest is motivated by the possibility that design policies associated with the built environment can be used to control, manage, and shape individual activity and behavior. This paper first studies and classifies smart growth principles based on literature review. Then the individual expert's judgments are obtained and utilized to evaluate the relative importance of smart growth principles. Next, the site selection for a new metro transit station in Taipei (Taiwan) is conducted to show the application of our proposed methodological approach. A combined Fuzzy Analytic Hierarchy Process (FAHP) and Data Envelopment Analysis (DEA) model with assurance region approach is applied to select the most suitable station from a given set of possible station sites. Both the selected station and the proposed methodological approach are provided to the public sector. (C) 2015 Elsevier Ltd. All rights reserved.",10.1016/j.habitatint.2015.01.020,2015,0,0.0,0.0,0.0,nw,0.0,1,0
98,On the identification of sales forecasting models in the presence of promotions,"Shorter product life cycles and aggressive marketing, among other factors, have increased the complexity of sales forecasting. Forecasts are often produced using a Forecasting Support System that integrates univariate statistical forecasting with managerial judgment. Forecasting sales under promotional activity is one of the main reasons to use expert judgment. Alternatively, one can replace expert adjustments by regression models whose exogenous inputs are promotion features (price, display, etc). However, these regression models may have large dimensionality as well as multicollinearity issues. We propose a novel promotional model that overcomes these limitations. It combines Principal Component Analysis to reduce the dimensionality of the problem and automatically identifies the demand dynamics. For items with limited history, the proposed model is capable of providing promotional forecasts by selectively pooling information across established products. The performance of the model is compared against forecasts provided by experts and statistical benchmarks, on weekly data; outperforming both substantially.",10.1057/jors.2013.174,2015,0,0.0,0.0,0.0,nw,0.0,1,0
99,Accuracy of Combined Forecasts for the 2012 Presidential Election: The PollyVote,"We review the performance of the PollyVote, which combined forecasts from polls, prediction markets, experts' judgment, political economy models, and index models to predict the two-party popular vote in the 2012 US presidential election. Throughout the election year the PollyVote provided highly accurate forecasts, outperforming each of its component methods, as well as the forecasts from FiveThirtyEight.com. Gains in accuracy were particularly large early in the campaign, when uncertainty about the election outcome is typically high. The results confirm prior research showing that combining is one of the most effective approaches to generating accurate forecasts.",10.1017/S1049096514000341,2014,1,1.0,0.0,0.0,gcg,1.0,1,1
100,"A Flexible, Corpus-Driven Model of Regular and Inverse Selectional Preferences","We present a vector space-based model for selectional preferences that predicts plausibility scores for argument headwords. It does not require any lexical resources (such as WordNet). It can be trained either on one corpus with syntactic annotation, or on a combination of a small semantically annotated primary corpus and a large, syntactically analyzed generalization corpus. Our model is able to predict inverse selectional preferences, that is, plausibility scores for predicates given argument heads. We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task (prediction of human plausibility judgments), gauging the influence of different parameters and comparing our model against other model classes. We obtain consistent benefits from using the disambiguation and semantic role information provided by a semantically tagged primary corpus. As for parameters, we identify settings that yield good performance across a range of experimental conditions. However, frequency remains a major influence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items.",10.1162/coli_a_00017,2010,0,0.0,0.0,0.0,gcg,0.0,1,0
101,Methyldibromoglutaronitrile: skin sensitization and quantitative risk assessment,"Preservatives can be a frequent cause of allergic contact dermatitis (ACD). A quantitative risk assessment (QRA) method for identifying safe exposure levels has been suggested as a more effective tool for this purpose. This work assesses the validity of QRA by its retrospective application to the sensitizing preservative methyldibromoglutaronitrile (MDGN), which has recently been associated with unacceptable exposure levels in consumer products. Using a recently published QRA analysis of 4 preservatives in 5 consumer product types, the accuracy of the predictions for MDGN was assessed in light of what is known clinically about the nature and incidence of ACD to this material. Based on a local lymph node assay (LLNA) EC3 value (concentration of test chemical required to provoke a 3-fold increase in lymph node cell proliferation) of 0.9% in a weight-of-evidence approach to the identification of thresholds for the induction of skin sensitization, it can be determined that the acceptable levels of exposure to MDGN in a range of products range from as little as 25 ppm to in excess of 10,000 ppm. Thus, proactive use of QRA, used conservatively and in combination with expert judgment, would have limited the problem of ACD to this new preservative that is known to have caused problems on the consumer market.</.",10.3109/15569520903351151,2010,0,0.0,0.0,0.0,gcg,0.0,1,0
102,Automatic Evaluation of Text Coherence: Models and Representations,"This paper investigates the automatic evaluation of text coherence for machine-generated texts. We introduce a fully-automatic, linguistically rich model of local coherence that correlates with human judgments. Our modeling approach relies on shallow text properties and is relatively inexpensive. We present experimental results that assess the predictive power of various discourse representations proposed in the linguistic literature. Our results demonstrate that certain models capture complementary aspects of coherence and thus can be combined to improve performance.",0,2005,0,0.0,0.0,0.0,ngr,0.0,1,0
103,Effective judgmental forecasting in the context of fashion products,"We study the conditions that influence judgmental forecasting effectiveness when predicting demand in the context of fashion products. Human judgment is of practical importance in this setting. Our goal is to investigate what type of decision support, in particular historical and/or contextual predictors, should be provided to human forecasters to improve their ability to detect and exploit linear and nonlinear cue-criterion relationships in the task environment. Using a field experiment on new product forecasts in the music industry, our analysis reveals that when forecasters are concerned with predictive accuracy and only managerial judgments are employed, providing both types of decision support data is beneficial. However, if judgmental forecasts are combined with a statistical forecast, restricting the decision support provided to human judges to contextual anchors is beneficial. We identify two novel interactions demonstrating that the exploitation of nonlinearities is easiest for human judgment if contextual data are present but historical data are absent. Thus, if the role of human judgment is to detect these nonlinearities (and the linearities are taken care of by some statistical model with which judgments are combined), then a restriction of the decision support provided makes sense. Implications for the theory and practice of building decision support models are discussed. (C) 2015 Elsevier B.V. All rights reserved.",10.1016/j.jom.2015.02.001,2015,0,0.0,0.0,0.0,ngr,0.0,1,0
104,Development of a surrogate model and sensitivity analysis for spatio-temporal numerical simulators,"To evaluate the consequences on human health of radionuclide releases in the environment, numerical simulators are used to model the radionuclide atmospheric dispersion. These codes can be time consuming and depend on many uncertain variables related to radionuclide, release or weather conditions. These variables are of different kind: scalar, functional and qualitative. Given the uncertain parameters, code provides spatial maps of radionuclide concentration for various moments. The objective is to assess how these uncertainties can affect the code predictions and to perform a global sensitivity analysis of code in order to identify the most influential uncertain parameters. This sensitivity analysis often calls for the estimation of variance-based importance measures, called Sobol' indices. To estimate these indices, we propose a global methodology combining several advanced statistical techniques which enable to deal with the various natures of the uncertain inputs and the high dimension of model outputs. First, a quantification of input uncertainties is made based on data analysis and expert judgment. Then, an initial realistic sampling design is generated and the corresponding code simulations are performed. Based on this sample, a proper orthogonal decomposition of the spatial output is used and the main decomposition coefficients are modeled with Gaussian process surrogate model. The obtained spatial metamodel is then used to compute spatial maps of Sobol' indices, yielding the identification of global and local influence of each input variable and the detection of areas with interactions. The impact of uncertainty quantification step on the results is also evaluated.",10.1007/s00477-014-0927-y,2015,0,0.0,0.0,1.0,nw,1.0,0,0
105,Identification of more risks can lead to increased over-optimism of and over-confidence in software development effort estimates,"Software professionals are, on average, over-optimistic about the required effort usage and over-confident about the accuracy of their effort estimates. A better understanding of the mechanisms leading to the over-optimism and over-confidence may enable better estimation processes and, as a consequence, better managed software development projects We hypothesize that there are situations where more work on risk identification leads to increased over-optimism and over-confidence in software development effort estimates, instead of the Intended improvement of realism. Four experiments with software professionals are conducted to test the hypothesis All four experiments provide results in support of the hypothesis. Possible explanations of the counter-intuitive finding relate to results from cognitive science on ""illusion-of-control"", ""cognitive accessibility"", ""the peak-end rule"" and ""risk as feeling"" Thorough work on risk identification is essential for many purposes and our results should not lead to less emphasis on this activity Our results do, however, suggest that it matters how risk identification and judgment-based effort estimation processes are combined A simple approach for better combination of risk identification work and effort estimation is suggested (C) 2009 Elsevier B V All rights reserved",10.1016/j.infsof.2009.12.002,2010,0,0.0,1.0,0.0,ngr,1.0,0,0
106,Probabilistic inversion for chicken processing lines,"We discuss an application of probabilistic inversion techniques to a model of campylobacter transmission in chicken processing lines. Such techniques are indicated when we wish to quantify a model which is new and perhaps unfamiliar to the expert community. In this case there are no measurements for estimating model parameters, and experts are typically unable to give a considered judgment. In such cases, experts are asked to quantify their uncertainty regarding variables which can be predicted by the model. The experts' distributions (after combination) are then pulled back onto the parameter space of the model, a process termed ""probabilistic inversion"". This study illustrates two such techniques, iterative proportional fitting (IPF) and PARmeter fitting for uncertain models (PARFUM). In addition, we illustrate how expert judgement on predicted observable quantities in combination with probabilistic inversion may be used for model validation and/or model criticism. (c) 2005 Elsevier Ltd. All rights reserved.",10.1016/j.ress.2005.11.054,2006,1,0.0,0.0,0.0,ngr,0.0,0,0
107,Spectral representation-analyzing single-unit activity in extracellularly recorded neuronal data without spike sorting,"One step in the conventional analysis of extracellularly recorded neuronal data is spike sorting, which separates electrical signal into action potentials from different neurons. Because spike sorting involves human judgment, it can be subjective and time intensive, particularly for large sets of neurons. Here we propose a simple, automated way to construct alternative representations of neuronal activity, called spectral representation (SR). In this approach, neuronal spikes are mapped to a discrete space of spike waveform features and time. Spectral representation enables us to find single-unit stimulus-related changes in neuronal activity without spike sorting. We tested the ability of this method to predict stimuli using both simulated data and experimental data from an auditory mapping study in anesthetized marmoset monkeys. We find that our approach produces more accurate classification of stimuli than spike-sorted data for both simulated and experimental conditions. Furthermore, this method lends itself to automated analysis of extracellularly recorded neuronal ensembles. Additionally, we suggest ways in which these representations can be readily extended to assist in spike sorting and the evaluation of single-neuron peri-stimulus time histograms. &COPY; 2004 Elsevier B.V. All rights reserved.",10.1016/j.jneumeth.2004.10.009,2005,0,0.0,0.0,0.0,ngr,0.0,1,0
108,Deep Learning the City: Quantifying Urban Perception at a Global Scale,"Computer vision methods that quantify the perception of urban environment are increasingly being used to study the relationship between a city's physical appearance and the behavior and health of its residents. Yet, the throughput of current methods is too limited to quantify the perception of cities across the world. To tackle this challenge, we introduce a new crowdsourced dataset containing 110,988 images from 56 cities, and 1,170,000 pairwise comparisons provided by 81,630 online volunteers along six perceptual attributes: safe, lively, boring, wealthy, depressing, and beautiful. Using this data, we train a Siamese-like convolutional neural architecture, which learns from a joint classification and ranking loss, to predict human judgments of pairwise image comparisons. Our results show that crowdsourcing combined with neural networks can produce urban perception data at the global scale.",10.1007/978-3-319-46448-0_12,2016,0,0.0,0.0,0.0,nw,0.0,1,0
109,Does quality and content matter for citedness? A comparison with para-textual factors and over time,"Using (binomial) regression analysis, we run models using citation windows of one to ten years with both annual citation and cumulative citations as dependent variables, and with both bibliometric and quality indicators (judgments of peers) as independent variables. The bibliometric variables are the Journal Impact Factor (JIF) of the publication medium, the numbers of authors and pages, and the statistical citedness of the references used within the paper. We find that the JIF has a larger influence on the citation impact of a publication than the quality (measured by judgments of peers). However, the number of pages and the quality of the references are less influential. The influence of JIF peaks after three years and then declines (in most regression analyses), but remains higher than the influence of quality judgments even after ten years. These results call into question a discrepancy between the algorithmically based indicators and the qualitative judgments by experts. The latter seems less predictive for future citation than a combination of algorithmic constructs. The results of this study can contribute to the empirical specification of the relevance of a normative versus a constructivist theory of citation. (C) 2015 Elsevier Ltd. All rights reserved.",10.1016/j.joi.2015.03.001,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
110,MAKING IMAGE QUALITY ASSESSMENT ROBUST,"We develop a robust framework for natural scene statistic (NSS) model based blind image quality assessment (IQA). The robustified IQA model utilizes a robust statistics approach based on L-moments. Such robust statistics based approaches are effective when natural or distorted images deviate from assumed statistical models, and achieves better prediction performance on distorted images relative to human subjective judgments. We also show how robustifying the model makes IQA approach resilient against deviation in model assumptions, small variations in the distortions and amount of data the model is trained on.",0,2012,0,0.0,0.0,0.0,gcg,0.0,1,0
111,"Hybridising Human Judgment, AHP, Grey Theory, and Fuzzy Expert Systems for Candidate Well Selection in Fractured Reservoirs","The selection of appropriate wells for hydraulic fracturing is one of the most important decisions faced by oilfield engineers. It has significant implications for the future development of an oilfield in terms of its productivity and economics. In this study, we developed a fuzzy model for well selection that combines the major objective criteria with the subjective judgments of decision makers. This was done by fusing the analytic hierarchy process (AHP) method, grey theory and an advanced version of fuzzy logic theory (FLT). The AHP component was used to identify the relevant criteria involved in selecting wells for hydraulic fracturing. Grey theory was used to determine the relative importance of those criteria. Then a fuzzy expert system was applied to fuzzily process the aggregated inputs using a Type-2 fuzzy logic system. This undertakes approximate reasoning and generates recommendations for candidate wells. These techniques and technologies were hybridized by using an intercommunication job-sharing method that integrates human judgment. The proposed method was tested on data from an oilfield in Western China and finally the most appropriate candidate wells for hydraulic fracturing were ranked in order of their projected output after fracturing.",10.3390/en10040447,2017,1,0.0,0.0,0.0,ngr,0.0,0,0
112,Self-compassion and depressive symptoms in a Norwegian student sample,"Excessive self-criticism is common to many mental health problems, including depression. Theoretically, positive self-compassion may work to prevent depression by protecting against the proliferation of self-condemning responses. A sample of Norwegian university students (N = 277, mean age = 22.9 years, SD = 3.5 years, 56% women) completed the Self-Compassion Scale (SCS) and the SCL-90 Depression subscale. Items of the three positive SCS-subscales (self-kindness, mindfulness, and common humanity) and items of the three negative SCS subscales (self-judgment, over-identification, and isolation) were combined to provide measures of Positive Self-Compassion and Self-Condemnation respectively. A moderation analysis indicated that the association between Self-Condemnation and Depressive Symptoms was weaker for individuals high in positive self-compassion, as expected. Bootstrap mediation analyses (conducted separately in groups scoring high and low in positive self-compassion) suggested that, in individuals high in positive self-compassion, self-compassion worked to reduce depressive symptoms by inversely affecting self-condemnation. When positive self-compassion was low, however, only Self-Condemnation predicted Depressive Symptoms. These results suggest that when positive self-compassion is above a certain level, it can keep self-condemning responses in check. If positive self-compassion is too weak, however, something else is needed, perhaps understanding input from another person.",10.1080/19012276.2015.1071203,2016,0,0.0,0.0,0.0,gcg,0.0,1,0
113,Impact of prior perception on bridge health diagnosis,"We use Bayesian logic in reproducing how a rational agent, called Ernest in the paper, analyses monitoring data and infers structural condition. The case study is Adige Bridge, a 260 m-long statically indeterminate structure with a deck supported by 12 stay cables. Bridge structural redundancy, possible relaxation losses and an asbuilt condition differing from design suggest that long-term load redistribution between cables can be expected. Therefore, the bridge owner installed a monitoring system, including fiberoptic sensors that allow measurement of deformation with an accuracy of a few microstrains. After 1 year of system operation, which included maintenance of the interrogation unit, the data analysis showed an apparent contraction of the cable lengths. This result is in contrast with the expected behavior. We analyze how a rational agent analyzes the observed response, and, in particular, we discuss to what extent he is prone to accept the sensor response as a result of the real mechanical behavior of the bridge versus a mere malfunction of the interrogation unit. In this analysis, we consider four psychological profiles, which vary based on their personal trust in the reliability of the instrumentation and on their knowledge of the structural behavior of the bridge. Using Bayesian logic as a tool to combine prior belief with sensor data, we explore how the extent of prior knowledge can alter the final engineering perception of the current state of the bridge and we demonstrate how the engineer's posterior judgment is predictable with a mathematical model. Formal reproduction of the human decision-making process can have strong impact in the field of structural health monitoring, as it may enable: (1) quantification of probabilities that engineers attribute to various events based on their subjective experience (which is currently an important challenge); (2) better understanding and improvement of the decisionmaking process itself; (3) embedding of decision making into structural health-monitoring methods for the full benefit of the latter.",10.1007/s13349-015-0120-0,2015,0,0.0,0.0,0.0,ngr,0.0,1,0
114,Knowing the crowd within: Metacognitive limits on combining multiple judgments,"We investigated how decision-makers use multiple opportunities to judge a quantity. Decision-makers undervalue the benefit of combining their own judgment with an advisor's, but theories disagree about whether this bias would apply to combining several of one's own judgments. Participants estimated percentage answers to general knowledge questions (e.g., What percent of the world's population uses the Internet?) on two occasions. In a final decision phase, they selected their first, second, or average estimate to report for each question. We manipulated the cues available for this final decision. Given cues to general theories (the labels first guess, second guess, average), participants mostly averaged, but no more frequently on trials where the average was most accurate. Given item-specific cues (numerical values of the options), metacognitive accuracy was at chance. Given both cues, participants mostly averaged and switched strategies based on whichever yielded the most accurate value on a given trial. These results indicate that underappreciation of averaging estimates does not stem only from social differences between the self and an advisor and that combining general and item-specific cues benefits metacognition. (C) 2013 Elsevier Inc. All rights reserved.",10.1016/j.jml.2013.10.002,2014,0,0.0,0.0,1.0,nw,1.0,0,0
115,Median Aggregation of Distribution Functions,"When multiple redundant probabilistic judgments are obtained from subject matter experts, it is common practice to aggregate their differing views into a single probability or distribution. Although many methods have been proposed for mathematical aggregation, no single procedure has gained universal acceptance. The most widely used procedure is simple arithmetic averaging, which has both desirable and undesirable properties. Here we propose an alternative for aggregating distribution functions that is based on the median cumulative probabilities at fixed values of the variable. It is shown that aggregating cumulative probabilities by medians is equivalent, under certain conditions, to aggregating quantiles. Moreover, the median aggregate has better calibration than mean aggregation of probabilities when the experts are independent and well calibrated and produces sharper aggregate distributions for well-calibrated and independent experts when they report a common location-scale distribution. We also compare median aggregation to mean aggregation of quantiles.",10.1287/deca.2013.0282,2013,1,0.0,0.0,0.0,gcg,0.0,0,0
116,Robust future-oriented technology portfolios: Black-Litterman approach,"We propose a new way of constructing more robust technology portfolios to overcome the weaknesses of previous technology portfolios based either on the judgments of experts or on quantitative data such as patents. Instead of using historical data, the method of nonlinear forecasting enables us to forecast the future number of patent citations and accordingly, to use the forecast as a quantitative proxy for future returns and risks of technologies. Using the Black-Litterman portfolio model, we improve the accuracy of inputs by combining the future views of experts with the future returns and risks of technologies. As a consequence of this, the portfolio becomes strongly future-oriented. With our approach, corporate managers use both experts and data more effectively to build robust technology portfolios. In particular, our method is of great help for companies launching new businesses because the method avoids heavy dependency on internal experts with little knowledge about emerging technologies. A company entering the molecular amplification instrument market is exemplified herein.",10.1111/radm.12022,2013,1,0.0,0.0,1.0,nw,1.0,1,1
117,Local Visual Energy Mechanisms Revealed by Detection of Global Patterns,"A central goal of visual neuroscience is to relate the selectivity of individual neurons to perceptual judgments, such as detection of a visual pattern at low contrast or in noise. Since neurons in early areas of visual cortex carry information only about a local patch of the image, detection of global patterns must entail spatial pooling over many such neurons. Physiological methods provide access to local detection mechanisms at the single-neuron level but do not reveal how neural responses are combined to determine the perceptual decision. Behavioral methods provide access to perceptual judgments of a global stimulus but typically do not reveal the selectivity of the individual neurons underlying detection. Here we show how the existence of a nonlinearity in spatial pooling does allow properties of these early mechanisms to be estimated from behavioral responses to global stimuli. As an example, we consider detection of large-field sinusoidal gratings in noise. Based on human behavioral data, we estimate the length and width tuning of the local detection mechanisms and show that it is roughly consistent with the tuning of individual neurons in primary visual cortex of primate. We also show that a local energy model of pooling based on these estimated receptive fields is much more predictive of human judgments than competing models, such as probability summation. In addition to revealing underlying properties of early detection and spatial integration mechanisms in human cortex, our findings open a window on new methods for relating system-level perceptual judgments to neuron-level processing.",10.1523/JNEUROSCI.3881-11.2012,2012,0,0.0,0.0,0.0,gcg,0.0,1,0
118,The ethics of using or not using statistical prediction rules in psychological practice and related consulting activities,"Professionals often believe that they must ""exercise judgment"" in making decisions critical to other people's lives. The relative superiority (established in roughly 150 studies) of statistical prediction rules (SPR's) to intuitive judgment for combining incomparable sources of information to predict important human outcomes leads us to question this personal input belief. Some professionals hence use SPR's to ""educate"" intuitive judgment, rather than replace it. In psychology in particular, such amalgamation is not justified. If a well-validated SPR that is superior to professional judgment exists in a relevant decision making context, professionals should use it, totally absenting themselves from the prediction.",10.1086/341844,2002,0,0.0,0.0,0.0,gcg,0.0,1,0
119,Bayesian model for fate and transport of polychlorinated biphenyl in upper Hudson River,"Modelers of contaminant fate and transport in surface waters typically rely on literature values when selecting parameter values for mechanistic models. While the expert judgment with which these selections are made is valuable, the information contained in contaminant concentration measurements should not be ignored. In this full-scale Bayesian analysis of polychlorinated biphenyl (PCB) contamination in the upper Hudson River, these two sources of information are combined using Bayes' theorem. A simulation model for the fate and transport of the PCBs in the upper Hudson River forms the basis of the likelihood function while the prior density is developed from literature values. The method provides estimates for the anaerobic biodegradation half-life, aerobic biodegradation plus volatilization half-life, contaminated sediment depth, and resuspension velocity of 4,400 d, 3.2 d, 0.32 m, and 0.02 m/yr, respectively. These are significantly different than values obtained with more traditional methods, and are shown to produce better predictions than those methods when used in a cross-validation study.",10.1061/(ASCE)0733-9372(1996)122:5(341),1996,0,0.0,0.0,0.0,gcg,0.0,1,0
120,On a Simple and Efficient Approach to Probability Distribution Function Aggregation,"In group decision making, it is inevitable that the individual decision maker's subjectivity is involved, which causes difficulty in reaching a group decision. One of the difficulties is to aggregate a small set of expert opinions with the individual subjectivity or uncertainty modeled with probability theory. This difficult problem is called probability distribution function aggregation (DFA). This paper presents a simple and efficient approach to the DFA problem. The main idea of the proposed approach is that the DFA problem is modeled as a nonlinear function of a set of probability distribution functions, and then a linear feedback iteration scheme is proposed to solve the nonlinear function, leading to a group judgment or decision. Illustration of this new approach is given by a well-known DFA example which was solved with the Delphi method. The DFA problem is a part of the group decision problem. Therefore, the proposed algorithm is also useful to the decision making problem in general. Another contribution of the this paper is the proposed notation of systematically representing the imprecise group decision problem with the classification of imprecise information into three classes, namely incomplete information, vague information, and uncertain information. The particular DFA problem dealt with in this paper is then characterized with this general notation.",10.1109/TSMC.2016.2531647,2017,1,1.0,0.0,0.0,gcg,1.0,1,1
121,GASFLOW-MPI: A new 3-D parallel all-speed CFD code for turbulent dispersion and combustion simulations,"The objective of the presented work is to develop an efficient and validated approach based on a multi-dimensional computational fluid dynamics (CFD) code for predicting turbulent gaseous dispersion, conjugated heat and mass transfer, multi-phase flow, and combustion of hydrogen mixtures. Applications of interest are accident scenarios relevant to nuclear power plant safety, renewable energy systems involved in hydrogen transport, hydrogen storage, facilities operating with hydrogen, as well as conventional large scale energy systems involving combustible gases. All model development is conducted within the framework of the high-performance scientific computing software GASFLOW-MultiPhysics-Integration (MPI). GASFLOW-MPI is the advanced parallel version of the GAS FLOW sequential code with many newly developed and validated models and features. The code provides reliability, robustness and excellent parallel scalability in predicting all speed flow-fields associated with hydrogen safety, including distribution, turbulent combustion and detonation. In the meanwhile, it has been well verified and validated by many international blind and open benchmarks. The recently developed combustion models in GASFLOW-MPI code are based on the transport equation of a reaction progress variable. The sources consist of turbulence dominated and chemistry kinetics dominated terms. Models have been implemented to compute the turbulent burning velocity for the turbulence controlled combustion rate. One-step and two-step models are included to obtain the chemical kinetics controlled reaction rate. These models, combined with the efficient and verified all-speed solver of the GASFLOW-MPI code, can be used for simulations of deflagration, detonation and the important transition processes like flame acceleration (FA) and deflagration-to-detonation transition (DDT), without additional need for expert judgment and intervention. It should be noted that the major goal is to develop a reliable and efficient numerical tool for largescale engineering analysis, instead of resolving the extremely complex physical phenomena and detailed chemistry kinetics on microscopic scales. During the course of this development, new verification and validation studies were completed for phenomena relevant to hydrogen-fueled combustion, such as shock wave capturing, premixed and non-premixed turbulent combustion with convective, conductive and radiation heat losses, detonation of unconfined hydrogen-air mixtures, and confined detonation waves in tubes. Excellent agreements between test data and model predictions support the predictive capabilities of the combustion models in GASFLOW-MPI code. In Part II of the paper, the newly developed CFD methodology has been successfully applied to a first analysis of hydrogen distribution and explosion in the Fukushi Daicchi Unit 1 accident. The major advantage of GASFLOW-MPI code is the all-speed capability of simulating laminar and turbulent distribution processes, slow deflagration, transition to fast hydrogen combustion modes including detonation, within a single scientific software framework without the need of transforming data between different solvers or codes. Since the code can model the detailed heat transfer mechanisms, including convective heat transfer, thermal radiation, steam condensation and heat conduction, the effects of heat losses on hydrogen deflagration or detonations can also be taken into account. Consequently, the code provides more accurate and reliable mechanical and thermal loads to the confining structures, compared to the overly conservative results from numerical simulations with the adiabatic assumptions. Predictions of flame acceleration mechanisms associated with turbulent flames and flow obstacles, as well as DDT modeling and their comparisons to available data will be presented in future papers. A structural analysis module will be further developed. The ultimate goal is to expand the GASFLOW-MPI code into an integral high-performance multi-physics simulation tool to cover the entire spectrum of phenomena involved in the mechanistic hydrogen safety analysis of large scale industrial facilities. (C) 2017 Hydrogen Energy Publications LLC. Published by Elsevier Ltd. All rights reserved.",10.1016/j.ijhydene.2017.01.215,2017,0,0.0,0.0,0.0,gcg,0.0,1,0
122,A Bayesian approach for predicting risk of autonomous underwater vehicle loss during their missions,"Autonomous Underwater Vehicles (AUVs) are effective platforms for science research and monitoring, and for military and commercial data-gathering purposes. However, there is an inevitable risk of loss during any mission. Quantifying the risk of loss is complex, due to the combination of vehicle reliability and environmental factors, and cannot be determined through analytical means alone. An alternative approach - formal expert judgment - is a time-consuming process; consequently a method is needed to broaden the applicability of judgments beyond the narrow confines of an elicitation for a defined environment. We propose and explore a solution founded on a Bayesian Belief Network (BBN), where the results of the expert judgment elicitation are taken as the initial prior probability of loss due to failure. The network topology captures the causal effects of the environment separately on the vehicle and on the support platform, and combines these to produce an updated probability of loss due to failure. An extended version of the Kaplan-Meier estimator is then used to update the mission risk profile with travelled distance. Sensitivity analysis of the BBN is presented and a case study of Autosub3 AUV deployment in the Amundsen Sea is discussed in detail. (C) 2015 Elsevier Ltd. All rights reserved.",10.1016/j.ress.2015.10.004,2016,1,0.0,1.0,0.0,ngr,1.0,1,1
123,Automated Box-Jenkins forecasting tool with an application for passenger demand in urban rail systems,"Efficient management of public transportation systems is one of the most important requirements in rapidly urbanizing world. Forecasting the demand for transportation is critical in planning and scheduling efficient operations by transportation systems managers. In this paper, a time series forecasting framework based on Box-Jenkins method is developed for public transportation systems. We present a framework that is comprehensive, automated, accurate, and fast. Moreover, it is applicable to any time series forecasting problem regardless of the application sector. It substitutes the human judgment with a combination of statistical tests, simplifies the time-consuming model selection part with enumeration, and it applies a number of comprehensive tests to select an accurate model. We implemented all steps of the proposed framework in MATLAB as a comprehensive forecasting tool. We tested our model on real passenger traffic data from Istanbul Metro. The numerical tests show the proposed framework is very effective and gives higher accuracy than the other models that have been used in many studies in the literature. Copyright (C) 2015 John Wiley & Sons, Ltd.",10.1002/atr.1332,2016,0,0.0,0.0,0.0,nw,0.0,1,0
124,Fuzzy Inference-Enhanced VC-DRSA Model for Technical Analysis: Investment Decision Aid,"To support investment decision based on technical analysis (TA), this study aims to retrieve the knowledge or rules of various indicators by a hybrid soft computing model. Although the validity of TA has been examined extensively by various statistical methods in literature, previous studies mainly explored the effectiveness of each technical indicator separately; therefore, a practical approach that may consider the inconsistency of various technical indicators simultaneously and the down-side risk of an investment decision is still underexplored. Thus, a hybrid model-by constructing a variable consistency dominance-based rough set approach (VC-DRSA) information system with the fuzzy inference-enhanced discretization of signals-is proposed, to retrieve the imprecise patterns from commonly adopted technical indicators. At the first stage, the trading signals (i.e., buy, neutral, or sell) are preprocessed in two groups: straight-forward signals and complicated signals. The straight-forward technical indicators (i.e., for signals that are decided by precise rules) are suggested by domain experts, and the buy-in signals are simulated by several trading strategies to examine the outcomes of each indicator. As for those complicated signals (i.e., for signals that require imprecise judgments with perceived feeling of domain experts to identify patterns), a fuzzy inference technique is incorporated to enhance the discretization of signals; those signals are also simulated by the aforementioned trading strategies to obtain the corresponding results. At the second stage, the trading signals generated by each technical indicator and their pertinent results from the previous stage are combined for VC-DRSA modeling to gain decision rules. To illustrate the proposed model, the weighted average index of the Taiwan stock market was examined from mid/2002 to mid/2014, and a set of decision rules with nearly 80 % classification accuracy (both in the training and the testing sets) were obtained in this empirical case. The findings suggest that several technical indicators should be considered simultaneously, and the retrieved rules (knowledge) have practical implications for investors.",10.1007/s40815-015-0058-8,2015,0,0.0,0.0,1.0,nw,1.0,0,0
125,COMPARISON OF DECISION-ASSIST AND CLINICAL JUDGMENT OF EXPERTS FOR PREDICTION OF LIFESAVING INTERVENTIONS,"Early recognition of hemorrhage during the initial resuscitation of injured patients is associated with improved survival in both civilian and military casualties. We tested a transfusion and lifesaving intervention (LSI) prediction algorithm in comparison with clinical judgment of expert trauma care providers. We collected 15 min of pulse oximeter photopletysmograph waveforms and extracted features to predict LSIs. We compared this with clinical judgment of LSIs by individual categories of prehospital providers, nurses, and physicians and a combined judgment of all three providers using the Area Under Receiver Operating Curve (AUROC). We obtained clinical judgment of need for LSI from 405 expert clinicians in135 trauma patients. The pulse oximeter algorithm predicted transfusion within 6 h (AUROC, 0.92; P < 0.003) more accurately than either physicians or prehospital providers and as accurately as nurses (AUROC, 0.76; P = 0.07). For prediction of surgical procedures, the algorithm was as accurate as the three categories of clinicians. For prediction of fluid bolus, the diagnostic algorithm (AUROC, 0.9) was significantly more accurate than prehospital providers (AUROC, 0.62; P = 0.02) and nurses (AUROC, 0.57; P = 0.04) and as accurate as physicians (AUROC, 0.71; P = 0.06). Prediction of intubation by the algorithm (AUROC, 0.92) was as accurate as each of the three categories of clinicians. The algorithm was more accurate (P < 0.03) for blood and fluid prediction than the combined clinical judgment of all three providers but no different from the clinicians in the prediction of surgery (P = 0.7) or intubation (P = 0.8). Automated analysis of 15 min of pulse oximeter waveforms predicts the need for LSIs during initial trauma resuscitation as accurately as judgment of expert trauma clinicians. For prediction of emergency transfusion and fluid bolus, pulse oximetry features were more accurate than these experts. Such automated decision support could assist resuscitation decisions, trauma team, and operating room and blood bank preparations.",10.1097/SHK.0000000000000288,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
126,On the relative importance of linear model and human judge(s) in combined forecasting,"When and to what extent should forecasts rely on linear model or human judgment? The judgmental forecasting literature suggests that aggregating model and judge using a simple 50:50 split tends to outperform the two inputs alone. However, current research disregards the important role that the structure of the task, judges' level of expertise, and the number of individuals providing a forecasting judgment may play. Ninety-two music industry professionals and 88 postgraduate students were recruited in a field experiment to predict chart entry positions of pop music singles in the UK and Germany. The results of a lens model analysis show how task structure and domain-specific expertise moderate the relative importance of model and judge. The study also delineates an upper boundary to which aggregating multiple judgments in model-expert combinations adds predictive accuracy. It is suggested that ignoring the characteristics of task and/or judge may lead to suboptimal forecasting performance. (C)12 Elsevier Inc. All rights reserved.",10.1016/j.obhdp.2012.08.003,2013,1,0.0,0.0,1.0,nw,1.0,1,1
127,Preliminary Strategic Environmental Assessment of the Great Western Development Strategy: Safeguarding Ecological Security for a New Western China,"The Great Western Development Strategy (GWDS) is a long term national campaign aimed at boosting development of the western area of China and narrowing the economic gap between the western and the eastern parts of China. The Strategic Environmental Assessment (SEA) procedure was employed to assess the environmental challenges brought about by the western development plans. These plans include five key developmental domains (KDDs): water resource exploitation and use, land utilization, energy generation, tourism development, and ecological restoration and conservation. A combination of methods involving matrix assessment, incorporation of expert judgment and trend analysis was employed to analyze and predict the environmental impacts upon eight selected environmental indicators: water resource availability, soil erosion, soil salinization, forest destruction, land desertification, biological diversity, water quality and air quality. Based on the overall results of the assessment, countermeasures for environmental challenges that emerged were raised as key recommendations to ensure ecological security during the implementation of the GWDS. This paper is intended to introduce a consensus-based process for evaluating the complex, long term pressures on the ecological security of large areas, such as western China, that focuses on the use of combined methods applied at the strategic level.",10.1007/s00267-011-9794-1,2012,0,0.0,0.0,1.0,nw,1.0,0,0
128,Support planning and controlling of early quality assurance by combining expert judgment and defect data-a case study,"Planning quality assurance (QA) activities in a systematic way and controlling their execution are challenging tasks for companies that develop software or software-intensive systems. Both require estimation capabilities regarding the effectiveness of the applied QA techniques and the defect content of the checked artifacts. Existing approaches for these purposes need extensive measurement data from historical projects. Due to the fact that many companies do not collect enough data for applying these approaches (especially for the early project lifecycle), they typically base their QA planning and controlling solely on expert opinion. This article presents a hybrid method combining commonly available measurement data and context-specific expert knowledge. To evaluate the method's applicability and usefulness, we conducted a case study in the context of independent verification and validation activities for critical software in the space domain. A hybrid defect content and effectiveness model was developed for the software requirements analysis phase and evaluated with available legacy data. One major result is that the hybrid model provides improved estimation accuracy when compared to applicable models based solely on data. The mean magnitude of relative error (MMRE) determined by cross-validation is 29.6% compared to 76.5% obtained by the most accurate data-based model.",10.1007/s10664-009-9112-1,2010,1,0.0,0.0,1.0,nw,1.0,1,1
129,Probabilistic Inversion in Priority Setting of Emerging Zoonoses,"This article presents methodology of applying probabilistic inversion in combination with expert judgment in priority setting problem. Experts rank scenarios according to severity. A linear multi-criteria analysis model underlying the expert preferences is posited. Using probabilistic inversion, a distribution over attribute weights is found that optimally reproduces the expert rankings. This model is validated in three ways. First, consistency of expert rankings is checked, second, a complete model fitted using all expert data is found to adequately reproduce observed expert rankings, and third, the model is fitted to subsets of the expert data and used to predict rankings in out-of-sample expert data.",10.1111/j.1539-6924.2010.01378.x,2010,1,0.0,0.0,0.0,gcg,0.0,0,0
130,Assessing source material difficulty for consecutive interpreting Quantifiable measures and holistic judgment,"Motivated by the need for better control of standards of a certification examination for interpreters in Taiwan, this exploratory study aimed at identifying indicators that may be used to predict source material difficulty for consecutive interpreting. A combination of quantifiable measures - readability level, information density and new concept density - was used to examine different aspects of three English source materials. Expert judgment was also used as a more holistic method of judging source material difficulty. The results of these analyses were compared with two groups of student interpreters' performance on consecutive interpreting of the source materials into Mandarin Chinese. The participants' assessment of speech difficulty after the interpreting task was also compared with the other measures and the expert judgment. The quantifiable measures all failed statistically in predicting source material difficulty, possibly due to the very small sample size of the materials or to the fact that the materials were very similar in the aspects assessed by these measures. A trend emerged to suggest that information density and sentence length may be potentially useful indicators for predicting source material difficulty. It was also shown that source material difficulty affected the performance of lower-skilled interpreters more than that of higher-skilled interpreters.",10.1075/intp.11.2.07liu,2009,0,0.0,0.0,0.0,nw,0.0,1,0
131,Computer simulations of the Rescorla-Wagner and Pearce-Hall models in conditioning and contingency judgment,"A Visual BASIC program, running under Windows 3.1, simulated the predictions of the Rescorla-Wagner (Rescorla & Wagner, 1972) and the Pearce-Hall (Pearce & Hall, 1980) models and compared them to the normative contingency coefficient Delta P (Jenkins & Ward, 1965). The simulations can be applied to a variety of phenomena in human contingency judgment as well as leaning and conditioning. Possible simulations include acquisition and extinction of excitatory and inhibitory conditioning, latent inhibition, blocking and overshadowing, or any other associative learning involving two single predictors, their compound, a contextual stimulus, and an outcome. The Pearce-Hall model has never been computerized before. In addition, unique features of this software include extensive use of the graphic user interface, context-sensitive help, verification of trial combinations, toggling of the contextual stimulus from ever-present to mutually exclusive with the discrete predictors, data entry via contingency tables or specifications trial by trial, single or batch randomizations of trial order, and specification of initial values. The associative simulator is both a powerful scientific instrument and a user-friendly teaching aid.",10.3758/BF03203636,1996,0,0.0,0.0,0.0,ngr,0.0,1,0
132,ESTIMATING THE STRENGTH OF EXPERT JUDGMENT - THE CASE OF UNITED-STATES MORTALITY FORECASTS,"The use of expert judgement is an important part of demographic forecasting. However, because judgement enters into the forecasting process in an informal way, it has been very difficult to assess its role relative to the analysis of past data. The use of targets in demographic forecasts permits us to embed the subjective forecasting process into a simple time-series regression model, in which expert judgement is incorporated via mixed estimation. The strength of expert judgement is defined, and estimated using the official forecasts of cause-specific mortality in the United States. We show that the weight given to judgement varies in an improbable manner by age. Overall, the weight given to judgement appears too high. An alternative approach to combining expert judgement and past data is suggested.",10.1002/for.3980110206,1992,1,0.0,1.0,0.0,ngr,1.0,1,1
133,A human judgment approach to epidemiological forecasting,"Infectious diseases impose considerable burden on society, despite significant advances in technology and medicine over the past century. Advanced warning can be helpful in mitigating and preparing for an impending or ongoing epidemic. Historically, such a capability has lagged for many reasons, including in particular the uncertainty in the current state of the system and in the understanding of the processes that drive epidemic trajectories. Presently we have access to data, models, and computational resources that enable the development of epidemiological forecasting systems. Indeed, several recent challenges hosted by the U. S. government have fostered an open and collaborative environment for the development of these technologies. The primary focus of these challenges has been to develop statistical and computational methods for epidemiological forecasting, but here we consider a serious alternative based on collective human judgment. We created the web-based | Epicast forecasting system which collects and aggregates epidemic predictions made in real-time by human participants, and with these forecasts we ask two questions: how accurate is human judgment, and how do these forecasts compare to their more computational, data-driven alternatives? To address the former, we assess by a variety of metrics how accurately humans are able to predict influenza and chikungunya trajectories. As for the latter, we show that real-time, combined human predictions of the 2014-2015 and 2015-2016 U.S. flu seasons are often more accurate than the same predictions made by several statistical systems, especially for short-term targets. We conclude that there is valuable predictive power in collective human judgment, and we discuss the benefits and drawbacks of this approach.",10.1371/journal.pcbi.1005248,2017,0,0.0,0.0,1.0,nw,1.0,0,0
134,Suburban watershed nitrogen retention: Estimating the effectiveness of stormwater management structures,"Excess nitrogen (N) is a primary driver of freshwater and coastal eutrophication globally, and urban stormwater is a rapidly growing source of N pollution. Stormwater best management practices (BMPs) are used widely to remove excess N from runoff in urban and suburban areas, and are expected to perform under a wide variety of environmental conditions. Yet the capacity of BMPs to retain excess N varies; and both the variation and the drivers thereof are largely unknown, hindering the ability of water resource managers to meet water quality targets in a cost-effective way. Here, we use structured expert judgment (SEJ), a performance-weighted method of expert elicitation, to quantify the uncertainty in BMP performance under a range of site-specific environmental conditions and to estimate the extent to which key environmental factors influence variation in BMP performance. We hypothesized that rain event frequency and magnitude, BMP type and size, and physiographic province would significantly influence the experts' estimates of N retention by BMPs common to suburban Piedmont and Coastal Plain watersheds of the Chesapeake Bay region. Expert knowledge indicated wide uncertainty in BMP performance, with N removal efficiencies ranging from < 0% (BMP acting as a source of N during a rain event) to > 40%. Experts believed that the amount of rain was the primary identifiable source of variability in BMP efficiency, which is relevant given climate projections of more frequent heavy rain events in the mid-Atlantic. To assess the extent to which those projected changes might alter N export from suburban BMPs and watersheds, we combined downscaled estimates of rainfall with distributions of N loads for different-sized rain events derived from our elicitation. The model predicted higher and more variable N loads under a projected future climate regime, suggesting that current BMP regulations for reducing nutrients may be inadequate in the future.",10.12952/journal.elementa.000063,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
135,Ensemble Predictions of Recovery Rates,"In many domains, the combined opinion of a committee of experts provides better decisions than the judgment of a single expert. This paper shows how to implement a successful ensemble strategy for predicting recovery rates on defaulted debts. Using data from Moody's Ultimate Recovery Database, it is shown that committees of models derived from the same regression method present better forecasts of recovery rates than a single model. More accurate predictions are observed whether we forecast bond or loan recoveries, and across the entire range of actual recovery values.",10.1007/s10693-013-0165-3,2014,1,0.0,1.0,0.0,ngr,1.0,1,1
136,Who's the Best? A Relativistic View of Expertise,"The dictionary and the expert performance approach view an expert as one who, after sufficient training and experience in a domain, can perform the requisite tasks above a threshold level. In contrast, we argue for a performance-based approach that implies expertise is a continuum; the experts are the best performers. Most tasks in which expertise can be demonstrated have an underlying core of judgment, including domains in which the tasks call for judgment to be overlain with performance, prediction, or instruction. To evaluate judgment, we employ the metaphor of the judge as a measuring instrument. Like an instrument, expert judgment according to the performance-based approach has three key properties: discrimination, consistency, and validity. Validity requires ground truth and is usually difficult to establish; but the other two properties are readily observable, and they are combined in the Cochran-Weiss-Shanteau index. Copyright (C) 2014 John Wiley & Sons, Ltd.",10.1002/acp.3015,2014,0,0.0,0.0,0.0,gcg,0.0,1,0
137,Model selection through robustness and fidelity criteria: Modeling the dynamics of the CX-100 wind turbine blade,"Several plausible modeling strategies are available to develop numerical models for simulating the dynamics of wind turbine blades. While the modeling strategy is typically selected according to expert judgment, the ""best"" modeling approach is unknown to the model developer. Thus, comparing plausible modeling strategies through a systematic and rigorous approach becomes necessary. This manuscript departs from the conventional approach that selects the model with the highest fidelity-to-data; and instead explores the trade-off between fidelity of model predictions to experiments and robustness of model predictions to model imprecision and inexactness. Exploring robustness in addition to fidelity lends credibility to the model, ensuring model predictions can be trusted even when lack-of-knowledge in the modeling assumptions and/or input parameters result in unforeseen. errors and uncertainties. This concept is demonstrated on the CX-100 wind turbine blade in an experimental configuration with large masses added to load the blade in bending during vibration testing. The finite element model of the blade is built with shell elements and validated against experimental evidence, while the large masses are modeled according to two different, but plausible strategies using (i) a combination of point-mass and spring elements, and (ii) solid elements. These two modeling strategies are evaluated considering both the fidelity of the natural frequency predictions against experiments, and the robustness of the predicted natural frequencies to uncertainties in the input parameters. By considering robustness during model selection, the authors determine the extent to which prediction accuracy deteriorates as the lack-of-knowledge increases. The findings suggest the model with solid elements offers a higher degree of fidelity-to-data and robustness to uncertainties, thus providing a superior modeling strategy than the model with point masses and stiffening springs. (C) 2013 Elsevier Ltd. All rights reserved.",10.1016/j.ymssp.2013.10.010,2014,0,0.0,0.0,0.0,nw,0.0,1,0
138,First tier modeling of consumer dermal exposure to substances in consumer articles under REACH: A quantitative evaluation of the ECETOC TRA for consumers tool,"The demonstration of safe use of chemicals in consumer products, as required under REACH, is proposed to follow a tiered process. In the first tier, simple conservative methods and assumptions should be made to quickly verify whether risks for a particular use are expected. The ECETOC TRA Consumer Exposure Tool was developed to assist in first tier risk assessments for substances in consumer products. The ECETOC TRA is not a prioritization tool, but is meant as a first screening. Therefore, the exposure assessment needs to cover all products/articles in a specific category. For the assessment of the dermal exposure for substances in articles, ECETOC TRA uses the concept of a 'contact layer', a hypothetical layer that limits the exposure to a substance contained in the product. For each product/article category, ECETOC TRA proposes default values for the thickness of this contact layer. As relevant experimental exposure data is currently lacking, default values are based on expert judgment alone. In this paper it is verified whether this concept meets the requirement of being a conservative exposure evaluation method. This is done by confronting the ECETOC TRA expert judgment based predictions with a mechanistic emission model, based on the well established theory of diffusion of substances in materials. Diffusion models have been applied and tested in many applications of emission modeling. Experimentally determined input data for a number of material and substance combinations are available. The estimated emissions provide information on the range of emissions that could occur in reality. First tier tools such as ECETOC TRA tool are required to cover all products/articles in a category and to provide estimates that are at least as high as is expected on the basis of current scientific knowledge. Since this was not the case, it is concluded that the ECETOC TRA does not provide a proper conservative estimation method for the dermal exposure to articles. An alternative method was proposed. (C) 2012 Elsevier Inc. All rights reserved.",10.1016/j.yrtph.2012.10.015,2013,0,0.0,0.0,0.0,gcg,0.0,1,0
139,Combining expert judgment: On the performance of trimmed mean vote aggregation procedures in the presence of strategic voting,"Analytic group decision techniques for selecting a subset of alternatives range between multicriteria decision analysis techniques such as multiattribute utility theory and the analytic hierarchy process to voting techniques where each member of the decision group submits a ranking of the alternatives, and these individual rankings are then aggregated into an overall ranking. The obvious advantage of voting is that it bypasses the rather intensive data generation requirements of multicriteria techniques. In this paper we compare the performance of trimmed mean rank-order aggregation procedures in the case where a subset of the individuals in the group charged with the decision vote strategically. We employ a Monte Carlo simulation experiment on a specific decision instance and find that trimmed mean aggregation compares favorably with other procedures. (C) 2002 Elsevier Science B.V. All rights reserved.",10.1016/S0377-2217(01)00226-0,2002,1,0.0,1.0,0.0,ngr,1.0,1,1
140,Predicting holistic ratings of written performance assessments from analytic scoring,"The use of experts to judge performance assessments is desirable because ratings of performances, carried out by experts in the content domain of the examination, are often considered to be the ""gold standard."" However, one drawback of using experts to rate performances is the high cost involved, A more economic alternative for scoring performance assessments entails using analytic scoring, which typically involves assigning points to individual traits present in the performance, and summing to arrive at a single score. This strategy is less costly, but may lack the richness of holistic scoring. This study investigates the use of regression-based techniques to predict expert judgments on a written performance task from a combination of analytic scores. Potentially, this will result in scores that approximate the richness of holistic ratings while maintaining the cost-effectiveness of analytic scoring. Results show that a substantial proportion of variance in expert judgments can be explained by the analytic scores, but that decisions based on actual expert judgments and the predicted expert judgments were not sufficiently consistent to warrant the substitution of one score for the other.",10.1023/A:1011478224834,2001,0,0.0,0.0,0.0,gcg,0.0,1,0
141,Aggregating and updating experts' knowledge: An experimental evaluation of five classification techniques,"Knowledge acquisition consists of eliciting expertise from one or more experts in order to construct a knowledge base. When knowledge is elicited from multiple experts, it is necessary to combine the multiple sources of expertise in order to arrive at a single knowledge base. In this paper; we present and compare five techniques for aggregating expertise. An experiment was conducted to extract expert judgments on new product entry timing. The elicited knowledge was aggregated using classical statistical methods (logit regression and discriminant analysis), the ID3 pattern classification method, the k-NN (Nearest Neighbor) technique, and neural networks. The neural net method was shown to outperform the other methods in robustness and predictive accuracy. In addition, the explanation capability of the neural net was investigated. The contributions of the input variables to the change in the output variable were interpreted by analyzing the connection strengths of the neural net when the net stabilized. We conclude by discussing the use of neural nets in knowledge aggregation and decision support.",10.1016/0957-4174(95)00049-6,1996,1,1.0,0.0,0.0,gcg,1.0,1,1
142,Simulating Moral Actions: An Investigation of Personal Force in Virtual Moral Dilemmas,"Advances in Virtual Reality (VR) technologies allow the investigation of simulated moral actions in visually immersive environments. Using a robotic manipulandum and an interactive sculpture, we now also incorporate realistic haptic feedback into virtual moral simulations. In two experiments, we found that participants responded with greater utilitarian actions in virtual and haptic environments when compared to traditional questionnaire assessments of moral judgments. In experiment one, when incorporating a robotic manipulandum, we found that the physical power of simulated utilitarian responses (calculated as the product of force and speed) was predicted by individual levels of psychopathy. In experiment two, which integrated an interactive and life-like sculpture of a human into a VR simulation, greater utilitarian actions continued to be observed. Together, these results support a disparity between simulated moral action and moral judgment. Overall this research combines state-of-the-art virtual reality, robotic movement simulations, and realistic human sculptures, to enhance moral paradigms that are often contextually impoverished. As such, this combination provides a better assessment of simulated moral action, and illustrates the embodied nature of morally-relevant actions.",10.1038/s41598-017-13909-9,2017,0,0.0,0.0,0.0,nw,0.0,1,0
143,Scoring and Testing Procedures Devoted to Probabilistic Seismic Hazard Assessment,"This review addresses long-term (tens of years) seismic ground-motion forecasting (seismic hazard assessment) in the presence of alternative computational models (the so-called epistemic uncertainty affecting hazard estimates). We review the different approaches that have been proposed to manage epistemic uncertainty in the context of probabilistic seismic hazard assessment (PSHA). Ex-ante procedures (based on the combination of expert judgments about inherent characteristics of the PSHA model) and ex-post approaches (based on empirical comparison of model outcomes and observations) should not be considered as mutually exclusive alternatives but can be combined in a coherent Bayesian view. Therefore, we propose a procedure that allows a better exploitation of available PSHA models to obtain comprehensive estimates, which account for both epistemic and aleatory uncertainty. We also discuss the respective roles of empirical ex-post scoring and testing of alternative models concurring in the development of comprehensive hazard maps. In order to show how the proposed procedure may work, we also present a tentative application to the Italian area. In particular, four PSHA models are evaluated ex-post against macroseismic effects actually observed in a large set of Italian municipalities during the time span 1957-2006. This analysis shows that, when the whole Italian area is considered, all the models provide estimates that do not agree with the observations. However, two of them provide results that are compatible with observations, when a subregion of Italy (Apulia Region) is considered. By focusing on this area, we computed a comprehensive hazard curve for a single locality in order to show the feasibility of the proposed procedure.",10.1007/s10712-015-9316-4,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
144,PROBABILITY AGGREGATION IN TIME-SERIES: DYNAMIC HIERARCHICAL MODELING OF SPARSE EXPERT BELIEFS,"Most subjective probability aggregation procedures use a single probability judgment from each expert, even though it is common for experts studying real problems to update their probability estimates over time. This paper advances into unexplored areas of probability aggregation by considering a dynamic context in which experts can update their beliefs at random intervals. The updates occur very infrequently, resulting in a sparse data set that cannot be modeled by standard time-series procedures. In response to the lack of appropriate methodology, this paper presents a hierarchical model that takes into account the expert's level of self-reported expertise and produces aggregate probabilities that are sharp and well calibrated both in-and out-of-sample. The model is demonstrated on a real-world data set that includes over 2300 experts making multiple probability forecasts over two years on different subsets of 166 international political events.",10.1214/14-AOAS739,2014,1,0.0,0.0,1.0,nw,1.0,1,1
145,A Scorecard-Markov model for new product screening decisions,"Purpose - The paper aims to propose a novel strategic approach, named a Scorecard-Markov model, combining an evaluation scorecard and a hidden Markov model (HMM) for new product idea screening (NPIS) decisions. Design/methodology/approach - A scorecard is constructed to evaluate new product ideas on several criteria, including customer needs, marketing strength, competency, manufacturing compatibility, and distribution channels, involving a consideration of risk buy. A HMM is then developed accordingly to predict the overall performance of new ideas in terms of success probability. To implement the model, it is trained and tested by the historical dataset of a world-class, leading company in the power tools industry through a case study. Findings - The approach is proven to be encouraging and meaningful. The scorecard can serve as a guide for new product idea evaluation to convert experts' linguistic judgments to quantifiable and comparable data, whereas the HMM can determine the success probability of new product ideas to support NPIS decision making based on their computed evaluation performance. The optimal cut-off value for making either a go or kill decision on each idea can thus be determined. Concerning the case company, a go decision should be made when the probability lies in the interval [0.53, 1]. Practical implications - The model can prevent companies from undertaking risky and failed new product development projects. Further, it is believed that this study can assist decision makers in choosing winning new product ideas towards commercialization in an effective and certain manner, thus enhancing the new product success rate in the innovation industry. Originality/value - The approach incorporating the scorecard method and HMM is novel. Illustrated by the case study, the application of this approach to NPIS decisions is confirmed to be effective.",10.1108/02635571011069068,2010,0,0.0,0.0,0.0,ngr,0.0,1,0
146,Integrating relevance feedback in boosting for content-based image retrieval,"Many content-based image retrieval applications suffer from small sample set and high dimensionality problems. Relevance feedback is often used to alleviate those problems. In this paper, we propose a novel interactive boosting framework to integrate user feedback into boosting scheme and bridge the gap between high-level semantic concept and low-level image features. Our method achieves more performance improvement from the relevance feedback than AdaBoost does because human judgment is accumulated iteratively to facilitate learning process. It also has obvious advantage over the classic relevance feedback method in that the classifiers are trained to pay more attention to wrongfully predicted samples in user feedback through a reinforcement training process. An interactive boosting scheme called i.Boost is implemented and tested using Adaptive Discriminant Projection (ADP) as base classifiers, which not only combines but also enhances a set of ADP classifiers into a more powerful one. To evaluate its performance, several applications are designed on UCI benchmark data sets, Harvard, UMIST, ATT facial image data sets and COREL color image data sets. The proposed method is compared to normal AdaBoost, classic relevance feedback and the state-of-the-art projection-based classifiers. The experiment results show the superior performance of i.Boost and the interactive boosting framework.",0,2007,0,0.0,0.0,0.0,gcg,0.0,1,0
147,In defense of clinical judgment... and mechanical prediction,"Despite over 50 years of one-sided research favoring formal prediction rules over human judgment, the ""clinical-statistical controversy,"" as it has come to be known, remains something of a hot-button issue. Surveying the objections to the formal approach, it seems the strongest point of disagreement is that clinical expertise can be replaced by statistics. We review and expand upon an unfortunately obscured part of Meehl's book to try to reconcile the issue. Building on Meehl, we argue that the clinician provides information that cannot be captured in, or outperformed by, mere frequency tables. However, that information is still best harnessed by a mechanical prediction rule that makes the ultimate decision. Two original studies support our arguments. The first study shows that multivariate prediction models using no data other than clinical speculations can perform well against statistical regression models. Study 2, however, showed that holistic predictions were less accurate than predictions made by mechanically combining smaller judgments without input from the judge at the combination stage. While we agree that clinical expertise cannot be replaced or neglected, we see no ethical reason to resist using explicit, mechanical rules for socially important decisions. Copyright (c) 2006 John Wiley & Sons, Ltd.",10.1002/bdm.537,2006,0,0.0,0.0,0.0,ngr,0.0,1,0
148,Bayesian approaches in ecological analysis and modeling,"Bayesian analysis provides a normative framework for use of uncertain information in decision making and inference. From a practical perspective, Bayes Theorem has a logical appeal in that it characterizes a process of knowledge updating that is based on pooling precision-weighted information. For years however, Bayesian inference was largely ignored or even discredited in favor of frequentist inference; among the reasons were computational difficulties and the formal use of subjective probabilities in applications of Bayes Theorem. In recent years, new computational approaches (e.g., Markov chain Monte Carlo) have greatly reduced the first problem, while the general recognition of the role of expert judgment in science has at least lessened resistance with respect to the second problem. Beyond that, Bayesian approaches facilitate certain analyses and interpretations that are often important to scientists. For example, the growing recognition of the value of combining information or ""borrowing strength"" in ecological studies, as new information is acquired to augment existing knowledge, is one of several reasons why interest in Bayesian inference continues to increase. Many currently used analytic techniques, such as random coefficients regression, multilevel models, data assimilation, and the Kalman filter are focused on this theme; all of these techniques reflect the basic framework of Bayes Theorem for pooling information. Most ecologists initially are taught that probabilities represent long-run frequencies: A consequence of this perspective is that probabilities have no meaning in a single unique or nonreplicated analysis. Scientists often ignore this constraint and interpret probabilities to suit the particular analysis. Related confusion sometimes arises in classical hypothesis testing and in the interpretation of p values. Bayesian inference provides appealing options in these situations. Collectively, these developments and perspectives have resulted in an increase in the application of Bayesian approaches in ecological studies, a number of which are noted here. Specific examples dealing with combining information, hypothesis testing, and Bayesian networks are discussed in more detail. In sum, it seems reasonable to make the judgmental forecast that Bayesian approaches will continue to increase in use in ecology.",0,2003,0,0.0,0.0,0.0,gcg,0.0,1,0
149,More than a metaphor: How the understanding of power is grounded in experience,"Judgment and thinking about power, a universal form of human sociality, is intimately tied to spatial cues: Nonverbal communication, cultural production of power symbols, and metaphors of power all make use of the vertical spatial dimension. We argue that this overlap is due to a grounding of the concept of power in spatial thought. Evidence confirming this proposition can be found in experiments showing the impact of highly schematized spatial cues on judgments of power. We will discuss how semantic network theories, embodied theories of cognition, and conceptual metaphor theory fare in explaining and predicting the combined evidence on nonverbal behavior, cultural production, and metaphors. In particular, we will ask what role language in the form of metaphors plays for our understanding of power as size and elevation: Whether it is causal, or mainly an outcome of other processes that are not based on language.",0,2011,0,0.0,0.0,0.0,gcg,0.0,1,0
150,Increasing diversity: Natural language measures for software fault prediction,"While challenging, the ability to predict faulty modules of a program is valuable to a software project because it can reduce the cost of software development, as well as software maintenance and evolution. Three language-processing based measures are introduced and applied to the problem of fault prediction. The first measure is based on the usage of natural language in a program's identifiers. The second measure concerns the conciseness and consistency of identifiers. The third measure, referred to as the QALP score, makes use of techniques from information retrieval to judge software quality. The QALP score has been shown to correlate with human judgments of software quality. Two case studies consider the language processing measures applicability to fault prediction using two programs (one open source, one proprietary). Linear mixed-effects regression models are used to identify relationships between defects and the measures. Results, while complex, show that language processing measures improve fault prediction, especially when used in combination. Overall, the models explain one-third and two-thirds of the faults in the two case studies. Consistent with other uses of language processing, the value of the three measures increases with the size of the program module considered. (C) 2009 Elsevier Inc. All rights reserved.",10.1016/j.jss.2009.06.036,2009,0,0.0,0.0,0.0,ngr,0.0,1,0
151,Bayesian reanalysis of the challenger O-ring data,"A Bayesian forecasting model is developed to quantify uncertainty about the postflight state of a field-joint primary O-ring (not damaged or damaged), given the O-ring temperature at the time of launch of the space shuttle Challenger in 1986. The crux of this problem is the enormous extrapolation that must be performed: 23 previous shuttle flights were launched at temperatures between 53 degrees F and 81 degrees F, but the next launch is planned at 31 degrees F. The fundamental advantage of the Bayesian model is its theoretic structure, which remains correct over the entire sample space of the predictor and that affords flexibility of implementation. A novel approach to extrapolating the input elements based on expert judgment is presented; it recognizes that extrapolation is equivalent to changing the conditioning of the model elements. The prior probability of O-ring damage can be assessed subjectively by experts following a nominal-interacting process in a group setting. The Bayesian model can output several posterior probabilities of O-ring damage, each conditional on the given temperature and on a different strength of the temperature effect hypothesis. A lower bound on, or a value of, the posterior probability can be selected for decision making consistently with expert judgment, which encapsulates engineering information, knowledge, and experience. The Bayesian forecasting model is posed as a replacement for the logistic regression and the nonparametric approach advocated in earlier analyses of the Challenger O-ring data. A comparison demonstrates the inherent deficiency of the generalized linear models for risk analyses that require (1) forecasting an event conditional on a predictor value outside the sampling interval, and (2) combining empirical evidence with expert judgment.",10.1111/j.1539-6924.2008.01081.x,2008,0,0.0,1.0,0.0,ngr,1.0,0,0
152,When political expertise moderates the impact of scandals on young adults' judgments of politicians,"This short note investigated how expertise in a political scandal moderates whether the activation of this scandal produces assimilation in the evaluation of politicians in general and contrast in the evaluation of specific politicians. It was hypothesized that participants with a rich knowledge about the scandal would display the assimilation and contrast effects whereas those with a poorer knowledge would not. Results tended to support this prediction, suggesting that the impact on judgment of a specific context depends on the amount of knowledge participants possess about this context. Copyright (c) 2005 John Wiley T Sons, Ltd.",10.1002/ejsp.245,2005,0,0.0,0.0,0.0,nw,0.0,1,0
153,Combining multiple time series predictors: a useful inferential procedure,"We present a general result that allows us to combine data from two different sources of information in order to improve the efficiency of predictors within the context of multiple time series analysis. Such a result is derived from generalized least squares and is given as a combining rule that takes into account the possibility of correlation between forecasts and bias in one of them. We then specialize that result to situations in which the predictors are unbiased and uncorrelated. Afterwards we propose measuring precision shares and testing for compatibility in order for the combination to make sense. Several applications of the combining rule are presented according to the nature of the linear constraints imposed by one of the data sources. When the constraints are binding we consider the case of restricted forecasts with exact linear restrictions, deterministic changes in the model structure and partial information on some variables. When the constraints are stochastic we study forecast combinations that include expert judgments and benchmarking. Thus, the connections among different standard techniques are emphasized by the combining rule and its companion compatibility test. An empirical example illustrates the usefulness of this inferential procedure in practice. (C) 2002 Elsevier Science B.V. All rights reserved.",10.1016/S0378-3758(02)00186-6,2003,0,0.0,0.0,0.0,gcg,0.0,1,0
154,Identifying Species Conservation Strategies to Reduce Disease-Associated Declines,"Emerging infectious diseases (EIDs) are a salient threat to many animal taxa, causing local and global extinctions, altering communities and ecosystem function. The EID chytridiomycosis is a prominent driver of amphibian declines, which is caused by the fungal pathogen Batrachochytrium dendrobatidis (Bd). To guide conservation policy, we developed a predictive decision-analytic model that combines empirical knowledge of host-pathogen metapopulation dynamics with expert judgment regarding effects of management actions, to select from potential conservation strategies. We apply our approach to a boreal toad (Anaxyrus boreas boreas) and Bd system, identifying optimal strategies that balance tradeoffs in maximizing toad population persistence and landscape-level distribution, while considering costs. The most robust strategy is expected to reduce the decline of toad breeding sites from 53% to 21% over 50 years. Our findings are incorporated into management policy to guide conservation planning. Our online modeling application provides a template for managers of other systems challenged by EIDs.",10.1111/conl.12393,2018,0,0.0,0.0,0.0,gcg,0.0,1,0
155,Can cancer researchers accurately judge whether preclinical reports will reproduce?,"There is vigorous debate about the reproducibility of research findings in cancer biology. Whether scientists can accurately assess which experiments will reproduce original findings is important to determining the pace at which science self-corrects. We collected forecasts from basic and preclinical cancer researchers on the first 6 replication studies conducted by the Reproducibility Project: Cancer Biology (RP: CB) to assess the accuracy of expert judgments on specific replication outcomes. On average, researchers forecasted a 75% probability of replicating the statistical significance and a 50% probability of replicating the effect size, yet none of these studies successfully replicated on either criterion (for the 5 studies with results reported). Accuracy was related to expertise: experts with higher h-indices were more accurate, whereas experts with more topic-specific expertise were less accurate. Our findings suggest that experts, especially those with specialized knowledge, were overconfident about the RP: CB replicating individual experiments within published reports; researcher optimism likely reflects a combination of overestimating the validity of original studies and underestimating the difficulties of repeating their methodologies.",10.1371/journal.pbio.2002212,2017,0,1.0,0.0,0.0,gcg,1.0,0,0
156,Finding the future: Crowdsourcing versus the Delphi technique,"When managers are unable to use quantifiable time series data to make forecasts or decide on uncertainties, they can either rely on their own intuition and judgment or resort to the insights of others. The Delphi technique is a well-known forecasting technique that relies on the pooled perspectives of experts to predict uncertain quantities or the outcomes of events. This relies on polling the opinions of experts, aggregating these opinions, feeding them back to the responding experts along with their own estimates, and having them repeat their judgment calls until some level of consensus is reached. More recently, however, the opinions of many others who are not experts have been sought on a range of topics in a loose assembly of similar techniques bundled under the title of crowdsourcing. This article compares Delphi and crowdsourcing as prediction and estimation tools for managers. It notes their differences and similarities, and provides a simple tool for executives to use in deciding whether or not to use these tools, and if so, which tool or combination of them will work best in a given situation. (C) 2016 Kelley School of Business, Indiana University. Published by Elsevier Inc. All rights reserved.",10.1016/j.bushor.2016.11.007,2017,1,0.0,0.0,1.0,nw,1.0,1,1
157,A Task Analysis toward Characterizing Cyber-Cognitive Situation Awareness (CCSA) in Cyber Defense Analysts,"Cyberspace is an increasingly crucial part of everyday living. We have long recognized that defending this space is complex, requiring information integration, and decisions of man and machine to coalesce in a dynamic environment full of shifting priorities. These properties suggest that, as in other domains with similar characteristics, situation awareness (SA) of a human cyber defender is paramount to the quality of decision outcomes in cyber defense. The majority of existing research in cyber situation awareness, centers on information systems and computers, which piece together disparate data. Fused data from multiple sources, for example, is necessary for cyberspace visualization efforts. The judgment for successful cyber SA from this perspective is different from one that is human-centered. In comparison, we rarely assess human cognitive awareness in cyberspace. In part, this reflects a need, based on prior theory, to first define critical elements of information that the human must perceive, work to elucidate how humans combine these elements to comprehend the state of the network, and how together, this awareness helps analysts predict the future state of the network. In other words, although data fusion can provide value by reducing the cognitive load created to piece together disparate sources of information, human awareness of the network (cyber-cognitive situation awareness CCSA) is perhaps the ultimate intermediary for defense performance. Toward such an understanding, we discuss the results of a cognitive task analysis (CTA) which sought to determine the goals and abstracted elements of awareness that cyber analysts seek in network defense. We present the foundation for a series of planned experiments that establishes CCSA measurement, and baselines the efforts of cyber defenders. Once assessed, we can then begin to consider the help offered by fusion systems, automation of defensive capabilities, and cyber visualizations in a methodologically rigorous manner that has been lacking.",0,2016,0,0.0,0.0,0.0,gcg,0.0,1,0
158,Predictive validity of evidence-based persuasion principles An application of the index method,"Purpose - This paper aims to test whether a structured application of persuasion principles might help improve advertising decisions. Evidence-based principles are currently used to improve decisions in other complex situations, such as those faced in engineering and medicine. Design/methodology/approach - Scores were calculated from the ratings of 17 self-trained novices who rated 96 matched pairs of print advertisements for adherence to evidence-based persuasion principles. Predictions from traditional methods - 10,809 unaided judgments from novices and 2,764 judgments from people with some expertise in advertising and 288 copy-testing predictions - provided benchmarks. Findings - A higher adherence-to-principles-score correctly predicted the more effective advertisement for 75 per cent of the pairs. Copy testing was correct for 59 per cent, and expert judgment was correct for 55 per cent. Guessing would provide 50 per cent accurate predictions. Combining judgmental predictions led to substantial improvements in accuracy. Research limitations/implications - Advertisements for high-involvement utilitarian products were tested on the assumption that persuasion principles would be more effective for such products. The measure of effectiveness that was available - day-after-recall - is a proxy for persuasion or behavioral measures. Practical/implications - Pretesting advertisements by assessing adherence to evidence-based persuasion principles in a structured way helps in deciding which advertisements would be best to run. That procedure also identifies how to make an advertisement more effective. Originality/value - This is the first study in marketing, and in advertising specifically, to test the predictive validity of evidence-based principles. In addition, the study provides the first test of the predictive validity of the index method for a marketing problem.",10.1108/EJM-10-2015-0728,2016,0,0.0,0.0,0.0,gcg,0.0,1,0
159,Development and Initial Assessment of the Medication User Self-Evaluation (MUSE) Tool,"Background: Using patient-reported data to supplement claims-based indicators may be helpful in identifying Medicare beneficiaries likely to benefit from medication therapy management (MTM) services. Objective: Our objective was to develop and initially assess a patient medication user self-evaluation (MUSE) tool to identify Medicare Part D beneficiaries who would benefit from a comprehensive medication review. Methods: A random sample of 225 patient medication profiles was created from a survey of Medicare beneficiaries; the survey also included demographic characteristics, responses to adherence questions, and reported symptoms. Three clinical pharmacists used the patient profiles to make judgments regarding the likelihood (low, moderate, or high) that each patient would benefit from an MTM visit in the next 3 months. A total of 150 cases were used for model calibration, and 75 were used for validation. Ordinal logistic regression models were fit to predict the likelihood of benefit from an MTM visit by using different combinations of potential MUSE items. Final model selection was based on the Akaike information criterion and the percent agreement between model prediction and expert judgments in the validation data. Measures considered for inclusion in the MUSE tool were related to medication use, medical conditions, and health care utilization. Results: The final MUSE items incorporated number of medications, number of physicians, number of pharmacies, number of hospitalizations in the past 6 months, having forgotten to take medications, cost-related problems, and number of medical conditions. Conclusion: The 7-item MUSE tool could be used in targeting MTM services, such as comprehensive medication reviews, among Medicare beneficiaries. (Clin Ther. 2013;35:344-350) (C) 2013 Elsevier HS Journals, Inc. All rights reserved.",10.1016/j.clinthera.2013.02.010,2013,0,0.0,0.0,0.0,ngr,0.0,1,0
160,Averaging Model Forecasts and Expert Forecasts: Why Does It Work?,"This paper addresses a situation in which a manager has forecasts of country-specific stock-keeping unit (SKU)-level sales data from both a statistical model and from a range of experts. Empirical evidence suggests that averaging model forecasts and expert forecasts could give more accuracy than the individual forecasts do. At the same time, empirical evidence demonstrates that expert forecasts tend to be biased. So, why is it that this average could work? This paper assumes that expert forecasts can be decomposed into a judgmental bootstrapping equation, to be created by the manager, and a part based on unobserved intuition. When the bootstrapping equation dominates the intuition, it is this underlying model for the expert that can make the combined average forecast more accurate. An illustration for no less than 1,221 cases supports this conjecture.",10.1287/inte.1100.0554,2011,1,0.0,1.0,0.0,ngr,1.0,1,1
161,Forecasting China's foreign trade volume with a kernel-based hybrid econometric-AI ensemble learning approach,"Due to the complexity of economic system and the interactive effects between all kinds of economic variables and foreign trade, it is not easy to predict foreign trade volume. However, the difficulty in predicting foreign trade volume is usually attributed to the limitation of many conventional forecasting models. To improve the prediction performance, the study proposes a novel kernel-based ensemble learning approach hybridizing econometric models and artificial intelligence (AI) models to predict China's foreign trade volume. In the proposed approach, an important econometric model, the co-integration-based error correction vector auto-regression (EC-VAR) model is first used to capture the impacts of all kinds of economic variables on Chinese foreign trade from a multivariate linear analysis perspective. Then an artificial neural network (ANN) based EC-VAR model is used to capture the nonlinear effects of economic variables on foreign trade from the nonlinear viewpoint. Subsequently, for incorporating the effects of irregular events on foreign trade, the text mining and expert's judgmental adjustments are also integrated into the nonlinear ANN-based EC-VAR model. Finally, all kinds of economic variables, the outputs of linear and nonlinear EC-VAR models and judgmental adjustment model are used as input variables of a typical kernel-based support vector regression (SVR) for ensemble prediction purpose. For illustration, the proposed kernel-based ensemble learning methodology hybridizing econometric techniques and AI methods is applied to China's foreign trade volume prediction problem. Experimental results reveal that the hybrid econometric-AI ensemble learning approach can significantly improve the prediction performance over other linear and nonlinear models listed in this study.",10.1007/s11424-008-9062-5,2008,0,0.0,0.0,1.0,nw,1.0,0,0
162,Multitasking as a choice: a perspective,"Performance decrements in multitasking have been explained by limitations in cognitive capacity, either modelled as static structural bottlenecks or as the scarcity of overall cognitive resources that prevent humans, or at least restrict them, from processing two tasks at the same time. However, recent research has shown that individual differences, flexible resource allocation, and prioritization of tasks cannot be fully explained by these accounts. We argue that understanding human multitasking as a choice and examining multitasking performance from the perspective of judgment and decision-making (JDM), may complement current dual-task theories. We outline two prominent theories from the area of JDM, namely Simple Heuristics and the Decision Field Theory, and adapt these theories to multitasking research. Here, we explain how computational modelling techniques and decision-making parameters used in JDM may provide a benefit to understanding multitasking costs and argue that these techniques and parameters have the potential to predict multitasking behavior in general, and also individual differences in behavior. Finally, we present the one-reason choice metaphor to explain a flexible use of limited capacity as well as changes in serial and parallel task processing. Based on this newly combined approach, we outline a concrete interdisciplinary future research program that we think will help to further develop multitasking research.",10.1007/s00426-017-0938-7,2018,0,0.0,0.0,0.0,gcg,0.0,1,0
163,Crowd Wisdom Relies on Agents' Ability in Small Groups with a Voting Aggregation Rule,"In the last decade, interest in the ""wisdom of crowds"" effect has gained momentum in both organizational research and corporate practice. Crowd wisdom relies on the aggregation of independent judgments. The accuracy of a group's aggregate prediction rises with the number, ability, and diversity of its members. We investigate these variables' relative importance for collective prediction using agent-based simulation. We replicate the ""diversity trumps ability"" proposition for large groups, showing that samples of heterogeneous agents outperform same-sized homogeneous teams of high ability. In groups smaller than approximately 16 members, however, the effects of group composition depend on the social decision function employed: diversity is key only in continuous estimation tasks (averaging) and much less important in discrete choice tasks (voting), in which agents' individual abilities remain crucial. Thus, strategies to improve collective decision making must adapt to the predictive situation at hand.",10.1287/mnsc.2015.2364,2017,0,1.0,0.0,0.0,gcg,1.0,0,0
164,"Expertise, credibility of system forecasts and integration methods in judgmental demand forecasting","Expert knowledge elicitation lies at the core of judgmental forecasting a domain that relies fully on the power of such knowledge and its integration into forecasting. Using experts in a demand forecasting framework, this work aims to compare the accuracy improvements and forecasting performances of three judgmental integration methods. To do this, a field study was conducted with 31 experts from four companies. The methods compared were the judgmental adjustment, the 50-50 combination, and the divide-and-conquer. Forecaster expertise, the credibility of system forecasts and the need to rectify system forecasts were also assessed, and mechanisms for performing this assessment were considered. When (a) a forecaster's relative expertise was high, (b) the relative credibility of the system forecasts was low, and (c) the system forecasts had a strong need of correction, judgmental adjustment improved the accuracy relative to both the other integration methods and the system forecasts. Experts with higher levels of expertise showed higher adjustment frequencies. Our results suggest that judgmental adjustment promises to be valuable in the long term if adequate conditions of forecaster expertise and the credibility of system forecasts are met. (C) 2016 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.",10.1016/j.ijforecast.2015.12.010,2017,1,1.0,0.0,0.0,gcg,1.0,1,1
165,Quantifying ecosystem quality by modeling multi-attribute expert opinion,"The evaluation of ecosystem quality is inherently subjective, requiring decisions about which variables to notice or measure, and how these variables are integrated into a coherent evaluation. Despite the central role of human judgment, few evaluation methods address the subjectivity that is inherent in their design. There are, however, advantages to directly using opinion to create an expert system where the metric is constructed around opinion data. These advantages include stakeholder inclusion and the encouragement of a dialogue of data-driven criticism rather than subjective counter-opinion. We create an expert system to express the quality of a grassland ecosystem in Australia. We use an ensemble of bagged regression trees trained on calibrated expert preference data, to model the perceived quality of this grassland using a set of eight site variables as inputs. The model provides useful predictions of grassland quality, producing predictions similar to real expert evaluations of independent synthetic test sites not used to train the model. We apply the model to real grassland sites ranging from pristine to highly degraded, and confirm that our model orders the sites according to their degree of modification. We demonstrate that the use of too few experts produces relatively poor results, and show that for our problem the use of data from over twenty experts is appropriate. The scaling approach we used to calibrate between-expert data is shown to be an appropriate mechanism for aggregating the opinions of multiple experts. The resultant model will be useful in many contexts, and can be used by managers as a tool to evaluate real sites. It can also be integrated into ecological models of change as a means of evaluating predicted changes, for example, as a measure of utility when combined with cost estimates. The basic approach demonstrated here is applicable to any ecosystem, and we discuss the opportunities and limitations of its wider use.",10.1890/14-1485.1,2015,0,0.0,0.0,1.0,nw,1.0,0,0
166,Structural features of endocrine active chemicals - A comparison of in vivo and in vitro data,"Studies on reproductive toxicity need high numbers of test animals. Therefore, we investigated whether chemical structural features (SF) in combination with in vitro data on specific adverse outcome pathways (AOPs) may be used for predicting reproductive toxicity of untested chemicals. Using the OECD Toolbox and expert judgment, we identified 89 structure groups for 275 chemicals for which the results of prenatal developmental toxicity or multigeneration studies were present in the Fraunhofer database on Fertility and Developmental Toxicity in experimental animals (FeDTex) database. Likewise, we evaluated 220 chemicals which had been tested in reporter gene assays on endocrine ((anti)estrogenic and (anti)androgenic) properties in the CALUX(R) test battery. There was a large spread of effect levels for substances within the chemical structure groups for both, in vivo and in vitro results. The groups of highest concern (diphenyl derivatives, planar conjugated systems with fused rings, phenols and organophosphates) correlated quite well, however, between the in vivo and in vitro data on estrogenic activity. For the 56 chemicals represented in both databases, lowest effect doses in vivo correlated well with the estrogenic activity in vitro. These results suggest that a panel of assays covering relevant AOPs and data on metabolism and toxicokinetics may allow prediction of relative reproductive or development toxicity potency within the identified chemical structure groups. (C) 2014 Elsevier Inc. All rights reserved.",10.1016/j.reprotox.2014.10.009,2015,0,0.0,0.0,0.0,nw,0.0,1,0
167,Classification and clinical diagnosis of cutaneous vasculitides,"The definition, diagnostic criteria and classification of systemic vasculitides, of which cutaneous vasculitides (CV) are a part, have long been discussed by the medical scientific world. The most significant contribution is due to the consensus-based criteria specifically derived by the combination of judgments from groups of experts, after accurate literature reviews and developed using consensus techniques. First of them came from the American College of Rheumatology (ACR) in 1990. In 1994 the Chapel Hill International Consensus Conference (CHCC) produced the Consensus-based Criteria essentially providing proper nomenclature for systemic vasculitis, which has been modified in 2012 by the CHCC2012. Moreover, in 2006 European League against Rheumatism and Pediatric Rheumatology European Society produced consensus criteria for the classification of childhood vasculitis. In CHCC2012 CV, affecting small vessels with a predominant skin involvement, have been included in both small vessel vasculitis and single organ vasculitis. The general characteristics of so-called CV have been described (epidemiology, clinical features, histopathology and etiopathogenesis) and, finally, the major characteristics of each clinical type of CV as well as their diagnostic criteria currently available in the literature have been reported.",0,2015,0,0.0,1.0,0.0,ngr,1.0,0,0
168,Accuracy gains of adding vote expectation surveys to a combined forecast of US presidential election outcomes,"In averaging forecasts within and across four-component methods (i.e. polls, prediction markets, expert judgment and quantitative models), the combined PollyVote provided highly accurate predictions for the US presidential elections from 1992 to 2012. This research note shows that the PollyVote would have also outperformed vote expectation surveys, which prior research identified as the most accurate individual forecasting method during that time period. Adding vote expectations to the PollyVote would have further increased the accuracy of the combined forecast. Across the last 90 days prior to the six elections, a five-component PollyVote (i.e. including vote expectations) would have yielded a mean absolute error of 1.08 percentage points, which is 7% lower than the corresponding error of the original four-component PollyVote. This study thus provides empirical evidence in support of two major findings from forecasting research. First, combining forecasts provides highly accurate predictions, which are difficult to beat for even the most accurate individual forecasting method available. Second, the accuracy of a combined forecast can be improved by adding component forecasts that rely on different data and different methods than the forecasts already included in the combination.",10.1177/2053168015570416,2015,1,0.0,0.0,0.0,ngr,0.0,0,0
169,Aiding Intrusion Analysis Using Machine Learning,"Intrusion analysis, i.e., the process of combing through IDS alerts and audit logs to identify real successful and attempted attacks, remains a difficult problem in practical network security defense. The major contributing cause to this problem is the high false-positive rate in the sensors used by IDS systems to detect malicious activities. The goal of our work is to examine whether a machine-learned classifier can help a human analyst filter out non-interesting scenarios reported by an IDS alert correlator, so that analysts' time can be saved. This research is conducted in the open-source SnIPS intrusion analysis framework. Throughout observing the output of SnIPS running on our departmental network, we found that an analyst would need to perform repetitive tasks in pruning out the false positives in the correlation graphs produced by it. We hypothesized that such repetitive tasks can yield (limited) labeled data that can enable the use of a machine learning-based approach to prune SnIPS' output based on the human analysts' feedback, much similar to spam filters that can learn from users' past judgment to prune emails. Our goal is to classify the correlation graphs produced from SnIPS into ""interesting"" and ""non-interesting"", where ""interesting"" means that a human analyst would want to conduct further analysis on the events. We spent significant amount of time manually labeling SnIPS' output correlations based on this criterion, and built prediction models using both supervised and semi-supervised learning approaches. Our experiments revealed a number of interesting observations that give insights into the pitfalls and challenges of applying machine learning in intrusion analysis. The experimentation results also indicate that semi-supervised learning is a promising approach towards practical machine learning-based tools that can aid human analysts, when a limited amount of labeled data is available.",10.1109/ICMLA.2013.103,2013,0,0.0,0.0,0.0,ngr,0.0,1,0
170,Exposure Modeling of Benzene Exploiting Passive-Active Sampling Data,"The objective of the present study is the exploitation of active sampling personal exposure data in assessing the factors that affect exposure to benzene in combination with the widely accepted scheme of passive sampling-time microenvironment-activity diaries (TMAD). The campaign included personal exposure measurements with both passive and active sampling in several microenvironments, evaluation of TMAD kept by the volunteers, and a variety of environmental data (ambient air benzene determination, traffic and meteorological observations). Due to the relatively elevated benzene traffic emissions, average personal exposure was determined to be equal to 8.9 mu g/m(3), ranging between 5 and 20 mu g/m(3), which is a value highly related to the average urban concentration (9.2 mu g/m(3)). The information gained from TMAD was embedded (in terms of spatial and temporal distribution) into three zones respectively, in order to draw statistically significant conclusions about the exposure levels and the activity patterns. The contribution of the activities to the overall amount of exposure was further quantified and refined by active sampling measurements. These data revealed that driving in a traffic-congested road was the main activity leading to elevated exposure levels (up to 70 mu g/m(3)), followed by walking on the roadside of a congested road (up to 35 mu g/m(3)). Indoor exposure to benzene was in general lower than outdoor (indicating that traffic is the dominant source of benzene emissions in the wider area), and it was significantly affected by the presence of environmental tobacco smoke. The higher significance of the regression coefficients obtained by statistical analysis of the active sampling data was fundamental for the development of a regression-based prediction exposure model. The model was evaluated through comparison with the passive sampling data, which were considered as an unknown but realistic data exposure pattern. The model performed very well in terms of expressing the variance of the exposure data with an average score of R (2) equal to 0.935. All of the above indicate that active sampling is a necessary albeit more laborious tool that needs to be used as a complement to passive sampling for precise quantification of the factors determining personal exposure patterns.",10.1007/s10666-009-9206-6,2010,0,0.0,0.0,0.0,ngr,0.0,1,0
171,Comparing Land Use Forecasting Methods: Expert Panel Versus Spatial Interaction Model,"Problem: Legal requirements and good planning practice dictate that land development induced by major highway investments be forecasted. Two forecasting methods, the first qualitative and based on expert judgment and the second quantitative and based on formal spatial interaction models, are often presented as equivalent. Purpose: We aim to extract lessons about the strengths and weaknesses of the two methods from a case study of a controversial highway, the Intercounty Connector (ICC), in the suburbs north of Washington, DC. Methods: We compare forecasts of induced development obtained using both methods and judge their reasonableness against the empirical literature. Results and conclusions: The two methods gave dramatically different results. The subjective judgment of experts predicted small impacts, on average, compared to a simple spatial interaction model. Also, subjectively forecasted impacts were limited to lands near the new facility, while modeled impacts rippled out across a much larger area. The subjective method seemed to give too little weight to accessibility effects and too much to zoning constraints, while a simple spatial interaction model seemed to do the opposite. Takeaway for practice: Where time, budget, or data limitations preclude the development of state-of-the-art integrated land use and transportation models, we conclude based on this case study that the best approach is to combine simple models and expert judgment. Expert panels can be used to check model inputs against local knowledge and to adjust outputs in light of factors otherwise unaccounted for. Conversely, model outputs can be used to check expert opinion for inconsistency with known land use-transportation relationships. Research support: None.",10.1080/01944360902956296,2009,0,0.0,0.0,0.0,gcg,0.0,1,0
172,THE NATURE OF HUMAN-NATURE - AN EMPIRICAL CASE FOR WITHHOLDING JUDGMENT - PERHAPS INDEFINITELY,"Claims to understanding the nature of human nature must be evaluated empirically, Specifically, understanding implies an ability to predict (although not vice versa), an ability greater than the benchmark ability based on characteristics and categorizations naively used in social prediction-such as age, gender, social and educational status, medical and legal history and so on, Moreover, art understanding of human nature implies a predictive ability that is greater than that obtained from a consideration of single characteristics and variables additively combined, A range of results indicates that such additive, ceteris paribus, prediction is all that results from social science research-and that many of the variables used in such prediction are of the benchmark variety Given the lack of validity of predictions based on nonlinear, hierarchical, or interactive assumptions about human nature, we can conclude that social science claims about understanding it are unwarranted. Psychologists and other social scientists can, however, use their ceteris paribus knowledge of important characteristics to influence human behavior and important social outcomes.",10.2307/3791451,1995,0,0.0,0.0,0.0,gcg,0.0,1,0
173,The Wisdom of Crowds in Matters of Taste,"Decision makers can often improve the accuracy of their judgments on factual matters by consulting ""crowds"" of others for their respective opinions. In this article, we investigate whether decision makers could similarly draw on crowds to improve the accuracy of their judgments about their own tastes and hedonic experiences. We present a theoretical model that states that accuracy gains from consulting a crowd's judgments of taste depend on the interplay among taste discrimination, crowd diversity, and the similarity between the crowd's preferences and those of the decision maker. The model also delineates the boundary conditions for such ""crowd wisdom."" Evidence supporting our hypotheses was found in two laboratory studies in which decision makers made judgments about their own enjoyment of musical pieces and short films. Our findings suggest that although different people may have different preferences and inclinations, their judgments of taste can benefit from the wisdom of crowds.",10.1287/mnsc.2016.2660,2018,0,0.0,0.0,0.0,nw,0.0,1,0
174,Investigating the added value of integrating human judgement into statistical demand forecasting systems,"Whilst the research literature points towards the benefits of a statistical approach, business practice continues in many cases to rely on judgmental approaches for demand forecasting. In today's dynamic environment, it is especially relevant to consider a combination of both approaches. However, the question remains as to how this combination should occur. This study compares two different ways of combining statistical and judgmental forecasting, employing real-life data from an international publishing company that produces weekly forecasts on regular and exceptional products. Two forecasting methodologies that are able to include human judgment are compared. In a 'restrictive judgement' model, expert predictions are incorporated as restrictions on the forecasting model. In an 'integrative judgment' model, this information is taken into account as a predictive variable in the demand forecasting process. The proposed models are compared on error metrics and analysed with regard to the properties of the adjustments (direction, size) and of the forecast itself (volatility, periodicity). The integrative approach has a positive effect on accuracy in all scenarios. However, in those cases where the restrictive approach proved to be beneficial, the integrative approach limited these beneficial effects. The study links with demand planning by using the forecasts as input for an optimization model to determine the ideal number of SKUs per Point of Sale (PoS), making a distinction between SKU forecasts and SKU per PoS forecasts. Importantly, this enables performance to be expressed as a measure of profitability, which proves to be higher for the integrative approach than for the restrictive approach.",10.1016/j.ijpe.2017.05.016,2017,1,1.0,0.0,0.0,gcg,1.0,1,1
175,Using fuzzy logic to generate conditional probabilities in Bayesian belief networks: a case study of ecological assessment,"The survival of rare animals is an important concern in an environmental impact assessment. However, it is very difficult to quantitatively predict the possible effect that a development project has on rare animals, and there is a heavy reliance on expert knowledge and judgment. In order to improve the credibility of expert judgment, this study uses Bayesian belief networks (BBN) to visually represent expert knowledge and to clearly explain the inference process. For the case study, the primary difficulty is in determining a large amount of conditional probabilities in the BBN, because there is a lack of sufficient data concerning rare animals. Therefore, a new method that uses fuzzy logic to systematically generate these probabilities is proposed. The combination of the BBN and the fuzzy logic system is used to assess the possible future population status of the Pheasant-tailed jacana and the associated probabilities, which have been affected by the construction of the Taiwan High-Speed Rail. The analysis shows that a restoration program would successfully preserve the species, because in the restoration area, the BBN model predicts that there is a 75.49 % probability that the species will flourish in the future.",10.1007/s13762-013-0459-x,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
176,Building an Expert-Judgment-Based Model of Mangrove Fisheries,"Mangroves are critically important habitats for fisheries, both for their resident fish, crustacean, and mollusk populations and as nursery grounds for the target species of offshore fisheries. However, the spatial variation in the benefits provided by mangroves to fisheries is poorly understood. Based on expert knowledge of mangrove ecology and fisheries biology, we developed a preliminary model of the spatial distribution of benefits to fisheries from mangroves. The preliminary model covers the environment factors that determine the amount of fish, crustaceans, mollusks, and other fishery target species produced by mangrove areas (termed ""potential fish production"") and the socioeconomic variables that determine the level of fishing in any given location. The combination of these two outputs gives the predicted catch. Potential fish production is predicted to be highest where there is high freshwater and nutrient input to mangroves, such as in large estuaries. At large seascape scales, total mangrove area is also an important driver. Fishing effort is highest close to human populations, which provide both the fishers and the markets for their catch. The model is qualitative and has not been parameterized with field data and, as such, should only be considered as a first step towards understanding the spatial variation in the benefits that mangroves provide to fisheries.",0,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
177,The Development of Progress Plans Using a Performance-Based Expert Judgment Model to Assess Technical Performance and Risk,"Systems engineers are routinely tasked with facilitating the delicate balance between cost, schedule, and technical performance in acquisition programs that are continuously subjected to various outside influences. While there are several quantitative methods to estimate acquisition program cost and schedule performance as well as identify their risks (e.g., Earned Value Management), the estimation of technical performance and technical risk is generally heuristic in nature. In order to monitor the progress of the technical aspects of an acquisition program, the systems engineering discipline utilizes the process of tracking Technical Measures to gain insight into the design and development, to assess risks and issues, and to evaluate the likelihood of realizing objectives. However, with the diversity of so many technical programs, the estimation and risk analysis of technical performance in technology acquisition programs rely on the opinions of experts because the identification and application of relevant quantitative data for constructive modeling is not practical. The Expert-weighted Technical Risk Index methodology proposed in this article introduces a well-established method for mathematically combining expert judgment into the realm of systems engineering to develop predictive progress plans for technical performance estimation and risk analysis. (c) 2013 Wiley Periodicals, Inc.",10.1002/sys.21273,2014,0,0.0,0.0,0.0,gcg,0.0,1,0
178,No-reference image quality assessment based on natural scene statistics and gradient magnitude similarity,"The goal of no-reference/blind image quality assessment (NR-IQA) is to devise a perceptual model that can accurately predict the quality of a distorted image as human opinions, in which feature extraction is an important issue. However, the features used in the state-of-the-art ""general purpose"" NR-IQA algorithms are usually natural scene statistics (NSS) based or are perceptually relevant; therefore, the performance of these models is limited. To further improve the performance of NR-IQA, we propose a general purpose NR-IQA algorithm which combines NSS-based features with perceptually relevant features. The new method extracts features in both the spatial and gradient domains. In the spatial domain, we extract the point-wise statistics for single pixel values which are characterized by a generalized Gaussian distribution model to form the underlying features. In the gradient domain, statistical features based on neighboring gradient magnitude similarity are extracted. Then a mapping is learned to predict quality scores using a support vector regression. The experimental results on the benchmark image databases demonstrate that the proposed algorithm correlates highly with human judgments of quality and leads to significant performance improvements over state-of-the-art methods. (C) 2014 Society of Photo-Optical Instrumentation Engineers (SPIE)",10.1117/1.OE.53.11.113110,2014,0,0.0,0.0,0.0,nw,0.0,1,0
179,Retinal Image Quality Assessment using Generic Features,"Retinal image quality assessment is an important step in automated eye disease diagnosis. Diagnosis accuracy is highly dependent on the quality of retinal images, because poor image quality might prevent the observation of significant eye features and disease manifestations. A robust algorithm is therefore required in order to evaluate the quality of images in a large database. We developed an algorithm for retinal image quality assessment based on generic features that is independent from segmentation methods. It exploits the local sharpness and texture features by applying the cumulative probability of blur detection metric and run-length encoding algorithm, respectively. The quality features are combined to evaluate the image's suitability for diagnosis purposes. Based on the recommendations of medical experts and our experience, we compared a global and a local approach. A support vector machine with radial basis functions was used as a nonlinear classifier in order to classify images to gradable and ungradable groups. We applied our methodology to 65 images of size 2592x1944 pixels that had been graded by a medical expert. The expert evaluated 38 images as gradable and 27 as ungradable. The results indicate very good agreement between the proposed algorithm's predictions and the medical expert's judgment: the sensitivity and specificity for the local approach are respectively 92% and 94%. The algorithm demonstrates sufficient robustness to identify relevant images for automated diagnosis.",10.1117/12.2043325,2014,0,0.0,0.0,0.0,ngr,0.0,1,0
180,Multi-Entity Bayesian Networks Learning For Hybrid Variables In Situation Awareness,"Over the past two decades, machine learning has led to substantial changes in Data Fusion Systems throughout the world. One of the most important application areas for data fusion is situation awareness. Situation Awareness is perception of elements in the environment, comprehension of the current situation, and projection of future status before decision making. Traditional fusion systems focus on lower levels of the JDL hierarchy, leaving higher-level fusion and situation awareness largely to unaided human judgment. This becomes untenable in today's increasingly data-rich environments, characterized by information and cognitive overload. Higher-level fusion to support situation awareness requires semantically rich representations amenable to automated processing. Multi-Entity Bayesian Networks (MEBN) combine First-Order Logic with Bayesian Networks for representing and reasoning about uncertainty in complex, knowledge-rich domains. MEBN goes beyond standard Bayesian networks to enable reasoning about an unknown number of entities interacting with each other in various types of relationships, a key requirement for PSAW. A MEBN model can be constructed manually by a domain expert or automatically by a machine learning algorithm. A discrete MEBN learning algorithm was recently developed. However, many real world variables are continuous. This paper presents a hybrid (both discrete and continuous variables) MEBN learning algorithm. The method is evaluated on a case study from the PROGNOS, predictive situation awareness system.",0,2013,0,0.0,0.0,0.0,gcg,0.0,1,0
181,Reduced Summation with Common Features in Causal Judgments,"In three experiments human participants received training in a causal judgment task. After learning which patterns were associated with an outcome, participants rated the likelihood of the outcome in the presence of a novel combination of the patterns. The first two experiments used two conditions in which two visual patterns were associated with the outcome. In one condition these patterns shared a common feature. The third experiment only used the common feature condition. According to an elemental theory (Rescorla & Wagner, 1972) the response to the novel test pattern should have exceeded that made to the individual training patterns, a summation effect, and this effect should have been reduced by the addition of a common feature. Summation was observed but since the common feature condition abolished, rather than merely reduced, summation the results were not consistent with the Rescorla-Wagner Model (RWM) nor with a configural alternative (Pearce, 1994). Instead, it is necessary to consider models which allow the possibility of both elemental and configural strategies in causal learning. The Replaced Elements Model (Wagner, 2003) is a development of the RWM which can best predict the patterns of summation and summation failure in these experiments.",10.1027/1618-3169/a000030,2010,0,0.0,0.0,0.0,gcg,0.0,1,0
182,Errors associated with simple versus realistic models,"This paper addresses the relative errors associated with simple versus realistic (or science-based) models. We take the perspective of trying to predict what the model will predict as we begin to build the model. Any model building process can get the model ""wrong"" to a greater or lesser extent by making a theoretical mistake in constructing the model. In addition, every model needs data of some sort, whether it be obtained by experiments, surveys or expert judgment, and the data collection process is filled with error sources. This paper suggests a hypothesis that 1. simple models have a larger variance in their predication of a result than do more realistic models (something most people intuitively agree to), and 2. more realistic models still have a significant probability of an error because the errors in the model building process will result in a probability distribution that ought to be bimodal, trimodal, or higher multimodal. The paper provides evidence to support these statements and draws conclusions about what types of models to generate and when.",10.1007/s10588-008-9047-x,2009,0,0.0,0.0,0.0,gcg,0.0,1,0
183,BAYESIAN ESTIMATION OF HISPANIC FERTILITY HAZARDS FROM SURVEY AND POPULATION DATA,"Previous studies have demonstrated both large gains in efficiency and reductions in bias by incorporating population information in regression estimation with sample survey data. These studies, however, assumed that the population values are exact. This assumption is relaxed here through a Bayesian extension of constrained maximum likelihood estimation applied to U.S. Hispanic fertility. The Bayesian approach allows for the use of both auxiliary survey data and expert judgment in making adjustments to published Hispanic Population fertility rates, and for the estimation of uncertainty about these adjustments. Compared with estimation from sample survey data only, the Bayesian constrained estimator results in much greater precision in the age pattern of the baseline fertility hazard and therefore of the predicted values for any given combination of socioeconomic variables. The use of population data in combination with survey data may therefore be highly advantageous even when the population data are known to have significant levels of nonsampling error.",10.1353/dem.0.0041,2009,0,0.0,0.0,0.0,gcg,0.0,1,0
184,The influence of lateral implicit visual affective stimuli on the evaluation of neutral stimuli in humans,"It is now well established that implicit affect influences explicit judgments. Findings from neurobiological studies indicate a relationship between the functioning of the human cerebral hemispheres and emotions. The aim of the present research was to examine: (1) the direction of influence on neutral targets of suboptimal primes exposed for a duration of 16 ms, (2) whether the influence of affective suboptimal primes on neutral targets depends on the hemisphere to which the prime is directed. We predicted that affective primes exposed centrally would influence the evaluation of neutral target stimuli in a direction opposite to that of their explicit effect. Second, we posited that the influence of primes on the evaluation of neutral target stimuli would be different depending on the visual field in which the primes were exposed. We present combined data from four experiments, conducted in a visual affective priming paradigm. Neutral target stimuli (ideographs exposed for a duration of 2 seconds) were sub-optimally primed by photographs of faces expressing joy or disgust exposed in either the LVF, RVF or CVF. Subjects were asked ""to state how negative/positive the character trait that is represented by a given ideograph is"". The hypotheses were supported. The evaluation of ideographs after negative priming was more positive than the evaluation of ideographs after positive priming (indicating a contrast effect). This effect appeared only when affective priming stimuli were exposed in the central visual field. The evaluation of ideographs differed depending on the visual field of prime exposure conditions: exposure of affective primes in the right visual field resulted in more positive evaluations of ideographs than ideographs following primes in the left visual field.",0,2007,0,0.0,0.0,0.0,nw,0.0,1,0
185,Policy capturing with ridge regression,"A policy capturing method combining human judgment with ridge regression is offered which results in superior judgment policy models. The new method (termed smart ridge regression) was tested against four others in seven judgment policy capturing applications. Performance criteria were two cross-validation indices: cross-validated multiple correlation and mean squared error of prediction of new judgments. Smart ridge regression was found to outperform ordinary least squares regression and conventional ridge regression, as well as subjective weighting and equal weighting of cues. (C) 1996 Academic Press, Inc.",10.1006/obhd.1996.0097,1996,0,0.0,1.0,0.0,ngr,1.0,0,0
186,Multivariate models using MCMCBayes for web-browser vulnerability discovery,"Vulnerabilities that enable well-known exploit techniques are preventable, but their public discovery continues in software. Vulnerability discovery modeling (VDM) techniques were proposed to assist managers with decisions, but do not include influential variables describing the software release (SR) (e.g., code size and complexity characteristics) and security assessment profile (SAP) (e.g., security team size or skill). Consequently, they have been limited to modeling discoveries over time for SR and SAP scenarios of unique products, whose results are not readily comparable without making assumptions that equate all SR and SAP combinations under study. This article introduces a groundbreaking capability that allows forecasting expected discoveries over time for arbitrary SR and SAP combinations, thus enabling managers to better understand the effects of influential variables they control on the phenomenon. To do this, we use variables that describe arbitrary SR and SAP combinations and construct VDM extensions that parametrically scale results from a defined baseline SR and SAP to the arbitrary SR and SAP of interest. Scaling parameters are estimated using expert judgment data gathered with a novel pairwise comparison approach. These data are then used to demonstrate predictions and how multivariate VDM techniques could be used by software-makers.",10.1016/j.ress.2018.03.024,2018,0,0.0,0.0,0.0,ngr,0.0,1,0
187,Judgmental selection of forecasting models,"In this paper, we explored how judgment can be used to improve the selection of a forecasting model. We compared the performance of judgmental model selection against a standard algorithm based on information criteria. We also examined the efficacy of a judgmental model-build approach, in which experts were asked to decide on the existence of the structural components (trend and seasonality) of the time series instead of directly selecting a model from a choice set. Our behavioral study used data from almost 700 participants, including forecasting practitioners. The results from our experiment suggest that selecting models judgmentally results in performance that is on par, if not better, to that of algorithmic selection. Further, judgmental model selection helps to avoid the worst models more frequently compared to algorithmic selection. Finally, a simple combination of the statistical and judgmental selections and judgmental aggregation significantly outperform both statistical and judgmental selections.",10.1016/j.jom.2018.05.005,2018,0,1.0,0.0,0.0,gcg,1.0,0,0
188,Making do with less: must sparse data preclude informed harvest strategies for European waterbirds?,"The demography of many European waterbirds is not well understood because most countries have conducted little monitoring and assessment, and coordination among countries on waterbird management has little precedent. Yet intergovernmental treaties now mandate the use of sustainable, adaptive harvest strategies, whose development is challenged by a paucity of demographic information. In this study, we explore how a combination of allometric relationships, fragmentary monitoring and research information, and expert judgment can be used to estimate the parameters of a theta-logistic population model, which in turn can be used in a Markov decision process to derive optimal harvesting strategies. We show how to account for considerable parametric uncertainty, as well as for different management objectives. We illustrate our methodology with a poorly understood population of Taiga Bean Geese (Anser fabalis fabalis), which is a popular game bird in Fennoscandia. Our results for Taiga Bean Geese suggest that they may have demographic rates similar to other, well-studied species of geese, and our model-based predictions of population size are consistent with the limited monitoring information available. Importantly, we found that by using a Markov decision process, a simple scalar population model may be sufficient to guide harvest management of this species, even if its demography is age structured. Finally, we demonstrated how two different management objectives can lead to very different optimal harvesting strategies, and how conflicting objectives may be traded off with each other. This approach will have broad application for European waterbirds by providing preliminary estimates of key demographic parameters, by providing insights into the monitoring and research activities needed to corroborate those estimates, and by producing harvest management strategies that are optimal with respect to the managers' objectives, options, and available demographic information.",10.1002/eap.1659,2018,1,0.0,0.0,1.0,nw,1.0,1,1
189,Differential hemispheric and visual stream contributions to ensemble coding of crowd emotion,"In crowds, where scrutinizing individual facial expressions is inefficient, humans can make snap judgments about the prevailing mood by reading 'crowd emotion'. We investigated how the brain accomplishes this feat in a set of behavioural and functional magnetic resonance imaging studies. Participants were asked to either avoid or approach one of two crowds of faces presented in the left and right visual hemifields. Perception of crowd emotion was improved when crowd stimuli contained goal-congruent cues and was highly lateralized to the right hemisphere. The dorsal visual stream was preferentially activated in crowd emotion processing, with activity in the intraparietal sulcus and superior frontal gyrus predicting perceptual accuracy for crowd emotion perception, whereas activity in the fusiform cortex in the ventral stream predicted better perception of individual facial expressions. Our findings thus reveal significant behavioural differences and differential involvement of the hemispheres and the major visual streams in reading crowd versus individual face expressions.",10.1038/s41562-017-0225-z,2017,0,0.0,0.0,0.0,ngr,0.0,1,0
190,Fuzzy Randomness Simulation of Long-Term Infrastructure Projects,"The conventional simulation model used in the prediction of long-term infrastructure development systems such as public-private partnership (PPP)-build-operate-transfer (BOT) projects assumes single probabilistic values for all of the input variables. Traditionally, all the input risks and uncertainties in Monte Carlo simulation (MCS) are modeled based on probability theory. Its result is shown by a probability distribution function (PDF) and a cumulative distribution function (CDF), which are utilized for analyzing and decision making. In reality, however, some of the variables are estimated based on expert judgment and others are derived from historical data. Further, the parameters' data of the probability distribution for the simulation model input are subject to change and difficult to predict. Therefore, a simulation model that is capable of handling both types of fuzzy and probabilistic input variables is needed and vital. Recently fuzzy randomness, which is an extension of classical probability theory, provides additional features and improvements for combining fuzzy and probabilistic data to overcome aforementioned shortcomings. Fuzzy randomness-Monte Carlo simulation (FR-MCS) technique is a hybrid simulation method used for risk and uncertainty evaluation. The proposed approach permits any type of risk and uncertainty in the input values to be explicitly defined prior to the analysis and decision making. It extends the practical use of the conventional MCS by providing the capability of choosing between fuzzy sets and probability distributions. This is done to quantify the input risks and uncertainties in a simulation. A new algorithm for generating fuzzy random variables is developed as part of the proposed FR-MCS technique based on the a-cut. FR-MCS output results are represented by fuzzy probability and the decision variables are modeled by fuzzy CDF. The FR-MCS technique is demonstrated in a PPP-BOT case study. The FR-MCS results are compared with those obtained from conventional MCS. It is shown that the FR-MCS technique facilitates decision making for both the public and private sectors' decision makers involved in PPP-BOT projects. This is done by determining a negotiation bound for negotiable concession items (NCIs) instead of precise values as are used in conventional MCS results. This approach prevents prolonged and costly negotiations in the development phase of PPP-BOT projects by providing more flexibility for decision makers. Both parties could take advantage of this technique at the negotiation table. (C) 2017 American Society of Civil Engineers.",10.1061/AJRUA6.0000902,2017,0,0.0,0.0,0.0,gcg,0.0,1,0
191,Spicy Adjectives and Nominal Donkeys: Capturing Semantic Deviance Using Compositionality in Distributional Spaces,"Sophisticated senator and legislative onion. Whether or not you have ever heard of these things, we all have some intuition that one of them makes much less sense than the other. In this paper, we introduce a large dataset of human judgments about novel adjective-noun phrases. We use these data to test an approach to semantic deviance based on phrase representations derived with compositional distributional semantic methods, that is, methods that derive word meanings from contextual information, and approximate phrase meanings by combining word meanings. We present several simple measures extracted from distributional representations of words and phrases, and we show that they have a significant impact on predicting the acceptability of novel adjective-noun phrases even when a number of alternative measures classically employed in studies of compound processing and bigram plausibility are taken into account. Our results show that the extent to which an attributive adjective alters the distributional representation of the noun is the most significant factor in modeling the distinction between acceptable and deviant phrases. Our study extends current applications of compositional distributional semantic methods to linguistically and cognitively interesting problems, and it offers a new, quantitatively precise approach to the challenge of predicting when humans will find novel linguistic expressions acceptable and when they will not.",10.1111/cogs.12330,2017,0,0.0,0.0,0.0,ngr,0.0,1,0
192,"Use of expert knowledge to anticipate the future: Issues, analysis and directions","Unless an anticipation problem is routine and short-term, and objective data are plentiful, expert judgment will be needed. Risk assessment is analogous to anticipating the future, in that models need to be developed and applied to data. Since objective data are often scanty, expert knowledge elicitation (EKE) techniques have been developed for risk assessment that allow models to be developed and parametrized using expert judgment with minimal cognitive and social biases. Here, we conceptualize how EKE can be developed and applied to support anticipation of the future. Accordingly, we begin by defining EKE as a complete process, which involves considering experts as a source of data, and comprises various methods for ensuring the quality of this data, including selecting the best experts, training experts in the normative aspects of anticipation, and combining judgments from several experts, as well as eliciting unbiased estimates and constructs from experts. We detail various aspects of the papers that constitute this special issue and analyse them in terms of the stages of the EKE future-anticipation process that they address. We also identify the remaining gaps in our knowledge. Our conceptualization of EKE with the aim of supporting anticipation of the future is compared and contrasted with the extant research on judgmental forecasting.",10.1016/j.ijforecast.2016.11.001,2017,0,0.0,1.0,0.0,ngr,1.0,0,0
193,Analysis of nutrition judgments using the Nutrition Facts Panel,"Consumers' judgments and choices of the nutritional value of food products (cereals and snacks) were studied as a function of using information in the Nutrition Facts Panel (NFP, National Labeling and Education Act, 1990). Brunswiles lens model (Brunswik, 1955; Cooksey, 1996; Hammond, 1955; Stewart, 1988) served as the theoretical and analytical tool for examining the judgment process. Lens model analysis was further enriched with the criticality of predictors' technique developed by Azen, Budescu, & Reiser (2001). Judgment accuracy was defined as correspondence between consumers' judgments and the nutritional quality index, NuVal (R), obtained from an expert system. The study also examined several individual level variables (e.g., age, gender, BMI, educational level, health status, health beliefs, etc.) as predictors of lens model indices that measure judgment consistency, judgment accuracy, and knowledge of the environment. Results showed varying levels of consistency and accuracy depending on the food product, but generally the median values of the lens model statistics were moderate. Judgment consistency was higher for more educated individuals; judgment accuracy was predicted from a combination of person level characteristics, and individuals who reported having regular meals had models that were in greater agreement with the expert's model. Conclusions: Lens model methodology is a useful tool for understanding how individuals perceive the nutrition in foods based on the NFP label. Lens model judgment indices were generally low, highlighting that the benefits of the complex NFP label may be more modest than what has been previously assumed. (C) 2016 Elsevier Ltd. All rights reserved.",10.1016/j.appet.2016.05.014,2016,0,0.0,0.0,0.0,gcg,0.0,1,0
194,Expert System for Ice Hockey Game Prediction: Data Mining with Human Judgment,This paper describes an expert system to predict National Hockey League (NHL) game outcome. A new method based on both data and judgments is used to estimate the hockey game performance. There are many facts and judgments that could influence an outcome. We employed the support vector machine to determine the importance of these factors before we incorporate them into the prediction system. Our system combines data and judgments and used them to predict the win-lose outcome of all the 89 post-season games before they took place. The accuracy of our prediction with the combined factors was 77.5%. This is to date the best accuracy reported of hockey games prediction.,10.1142/S0219622016400022,2016,1,1.0,0.0,0.0,gcg,1.0,1,1
195,Dynamic simulation metamodeling using MARS: A case of radar simulation,"Dynamic system simulations require relating the inputs to the multivariate output which can be a function of time space coordinates. In this work, we propose a methodology for the metamodeling of dynamic simulation models via Multivariate Adaptive Regression Splines (MARS). To handle incomplete output processes, where the simulation model does not produce an output in some steps due to missing inputs, we have devised a two-stage metamodeling scheme. The methodology is demonstrated on a dynamic radar simulation model. The prediction performance of the resulting metamodel is tested with four different sampling techniques (i.e., designs) and 16 sample sizes. We also investigate the effect of alternative coordinate system representations on the metamodeling performance. The results suggest that MARS is an effective method for metamodeling dynamic simulations, particularly, when expert judgment is not readily available. Results also show that there are interactions between the coordinate systems and sampling techniques, and some design-representation-size combinations are very promising in the metamodeling of radar simulations. (C) 2016 International Association for Mathematics and Computers in Simulation (IMACS). Published by Elsevier B.V. All rights reserved.",10.1016/j.matcom.2016.01.005,2016,0,0.0,0.0,0.0,gcg,0.0,1,0
196,Gist Representations and Communication of Risks about HIV-AIDS: A Fuzzy-Trace Theory Approach,"As predicted by fuzzy-trace theory, people with a range of training-from untrained adolescents to expert physicians-are susceptible to biases and errors in judgment and perception of HIV-AIDS risk. To explain why this occurs, we introduce fuzzy-trace theory as a theoretical perspective that describes these errors to be a function of knowledge deficits, gist-based representation of risk categories, retrieval failure for risk knowledge, and processing interference (e. g., base-rate neglect) in combining risk estimates. These principles explain how people perceive HIV-AIDS risk and why they take risks with potentially lethal outcomes, often despite rote (verbatim) knowledge. For example, people inappropriately generalize the wrong gist about condoms' effectiveness against fluid-borne disease to diseases that are transferred skin-to-skin, such as HPV. We also describe how variation in processing in adolescence (e. g., more verbatim processing compared to adults) can be a route to risk-taking that explains key aspects of why many people are infected with HIV in youth, as well as how interventions that emphasize bottom-line gists communicate risks effectively.",10.2174/1570162X13666150511142748,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
197,Consumer reactions to professionals who use decision aids,"Purpose - The purpose of this study is to investigate consumer reactions to professionals who use decision aids to make recommendations. The authors propose that people react negatively to decision aids only when they are used in place of human expert judgment. When used in combination with expert judgment, decision aids are not perceived negatively and may even enhance service evaluations. Design/methodology/approach - Three online experiments are presented. Participants indicated their perceptions regarding the recommendation strategy of professionals and their impressions of these professionals using one of three strategies: one based on expertise only, one based on decision aids only and a combination of the two (hybrid approach). Both within and between-subjects designs were used. Findings - Contrary to previous research that has found a negative reaction to professionals who use decision aids, the authors find that consumers actually appreciate these professionals, as long as the use of decision aids does not replace expert judgment. The authors also find that when people are given the opportunity to compare a pure expert judgment approach with a hybrid approach (decision aid in combination with expert judgment), they prefer the latter. Research limitations/implications - Although findings should extend to various contexts, this research is limited to the three contexts examined and to the type of use of decision aid described. Practical implications - It has significant practical implication, as decision aids have been shown to improve decision accuracy, but previous research had indicated that consumers view these professionals in a negative way. The current research more clearly delineates the situations under which negative reactions are likely to occur and makes recommendations regarding circumstances in which reactions are actually quite positive. Originality/value - Reactions to professionals using decision aids have been investigated outside the marketing literature. However, this is the first work to show that consumers actually have positive reactions to professionals using decision aids, as long as they do not replace expert judgment.",10.1108/EJM-07-2013-0390,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
198,Comparative Analysis of Data Collection Methods for Individualized Modeling of Radiologists' Visual Similarity Judgments in Mammograms,"Rationale and Objectives: We conducted an Observer study to investigate how the data collection method affects the efficacy of modeling individual radiologists' judgments regarding the perceptual similarity of breast masses on mammograms. Materials and Methods: Six observers of varying experience levels in breast imaging were recruited to assess the perceptual similarity of mammographic masses. The observers' subjective judgments were collected using (i) a rating method, (ii) a preference method, and (Hi) a hybrid method combining rating and ranking. Personalized user models were developed with the collected data to predict observers' opinions. The relative efficacy of each data collection method was assessed based on the classification accuracy of the resulting user models. Results: The average accuracy of the user models derived from data collected with the hybrid method was 55.5 +/- 1.5%. The models were significantly more accurate (P < .0005) than those derived from the rating (45.3 +/- 3.5%) and the preference (40.8 +/- 5%) methods. On average, the rating data collection method was significantly faster than the other two methods (P < .0001). No time advantage was observed between the preference and the hybrid methods. Conclusions: A hybrid method combining rating and ranking is an intuitive and efficient way for collecting subjective similarity judgments to model human perceptual opinions with a higher accuracy than other, more commonly used data collection methods.",10.1016/j.acra.2013.08.002,2013,0,0.0,0.0,0.0,ngr,0.0,1,0
199,Combinatorial Prediction Markets: An Experimental Study,"Prediction markets produce crowdsourced probabilistic forecasts through a market mechanism in which forecasters buy and sell securities that pay off when events occur. Prices in a prediction market can be interpreted as consensus probabilities for the corresponding events. There is strong empirical evidence that aggregate forecasts tend to be more accurate than individual forecasts, and that prediction markets are among the most accurate aggregation methods. Combinatorial prediction markets allow forecasts not only on base events, but also on conditional events (e.g., ""A if B"") and/or Boolean combinations of events. Economic theory suggests that the greater expressivity of combinatorial prediction markets should improve accuracy by capturing dependencies among related questions. This paper describes the DAGGRE combinatorial prediction market and reports on an experimental study to compare combinatorial and traditional prediction markets. The experiment challenged participants to solve a ""whodunit"" murder mystery by using a prediction market to arrive at group consensus probabilities for characteristics of the murderer, and to update these consensus probabilities as clues were revealed. A Bayesian network was used to generate the ""ground truth"" scenario and to provide ""gold standard"" probabilistic predictions. The experiment compared predictions using an ordinary flat prediction market with predictions using a combinatorial market. Evaluation metrics include accuracy of participants' predictions and the magnitude of market updates. The murder mystery scenario provided a more concrete, realistic, intuitive, believable, and dynamic environment than previous empirical work on combinatorial prediction markets.",0,2013,0,0.0,0.0,0.0,ngr,0.0,1,0
200,AGGREGATION OF FORECASTS FROM MULTIPLE SIMULATION MODELS,"When faced with output from multiple simulation models, a decision maker must aggregate the forecasts provided by each model. This problem is made harder when the models are based on similar assumptions or use overlapping input data. This situation is similar to the problem of expert judgment aggregation where experts provide a forecast distribution based on overlapping information, but only samples from the output distribution are obtained in the simulation case. We propose a Bayesian method for aggregating forecasts from multiple simulation models. We demonstrate the approach using a climate change example, an area often informed by multiple simulation models.",0,2013,1,0.0,0.0,0.0,nw,0.0,0,0
201,Combination and Selection of Traffic Safety Expert Judgments for the Prevention of Driving Risks,"In this paper, we describe a new framework to combine experts' judgments for the prevention of driving risks in a cabin truck. In addition, the methodology shows how to choose among the experts the one whose predictions fit best the environmental conditions. The methodology is applied over data sets obtained from a high immersive cabin truck simulator in natural driving conditions. A nonparametric model, based in Nearest Neighbors combined with Restricted Least Squared methods is developed. Three experts were asked to evaluate the driving risk using a Visual Analog Scale (VAS), in order to measure the driving risk in a truck simulator where the vehicle dynamics factors were stored. Numerical results show that the methodology is suitable for embedding in real time systems.",10.3390/s121114711,2012,1,0.0,0.0,0.0,gcg,0.0,0,0
202,A Demand Forecasting Methodology for Fuzzy Environments,"Several supply chain and production planning models in the literature assume the demands are fuzzy but most of them do not offer a specific technique to derive the fuzzy demands. In this study, we propose a methodology to obtain a fuzzy-demand forecast that is represented by a possibilistic distribution. The fuzzy-demand forecast is found by aggregating forecasts based on different sources; namely statistical forecasting methods and experts' judgments. In the methodology, initially, the forecast derived from the statistical forecasting techniques and experts' judgments are represented by triangular possibilistic distributions. Subsequently, those results are combined by using weights assigned to each of them. A new objective weighting approach is used to find the weights. The proposed methodology is illustrated by an example and a sensitivity analysis is provided.",0,2010,1,0.0,0.0,0.0,gcg,0.0,0,0
203,Untitled,0,10.1287/deca.1090.0137,2009,0,0.0,0.0,0.0,ngr,0.0,1,0
204,Predicting Risk in Missions under Sea Ice with Autonomous Underwater Vehicles,"Autonomous Underwater Vehicles (AUVs) have a future as effective platforms for multi-disciplinary science research and monitoring in the polar oceans. However, operation under ice may involve significant risk to the vehicle. A risk assessment and management process that balances the risk appetite of the responsible owner with the reliability of the vehicle and the probability of loss has been proposed. A critical step in the process of assessing risk is based on expert judgment of the fault history of the vehicle, and what affect faults or incidents have on the probability of loss. However, this subjective expert judgment is sensitive to the nature of sea ice cover. In contrast to the simple, yet high risk, case of operation under an ice shelf, sea ice offers a complex risk environment. Furthermore, the risk is modified by the characteristics of the support vessel, especially its ice-breaking capability. We explore how the ASPeCt sea ice characterization protocol and probability distributions of ice thickness and concentration can be used within a rigorous process to quantify risk given a range of sea ice conditions and with ships of differing ice capabilities. A solution founded on a Bayesian Belief Network approach is proposed, where the results of the expert judgment elicitation is taken as a reference. The design of the network topology captures the causal effects of the environment separately on the vehicle and on the ship, and combines these to produce the output. Complementary expert knowledge is included within the conditional probability tables of the Bayesian Belief Network. Using expert judgment on the fault history of the Autosub3 vehicle and sea ice data gathered in the Arctic and Antarctic by its predecessor, Autosub2, examples are provided of how risk is modified by the sea ice environment.",10.1109/AUV.2008.5290536,2008,0,0.0,0.0,1.0,nw,1.0,0,0
205,Identifying potential health care innovations for the future elderly,"We used a method that combined literature review and expert judgment to assess potential medical innovations for older adults. We evaluated innovations in four domains: cardiovascular disease, cancer, the biology of aging, and neurologic disease. The innovations can be categorized by common themes: improved disease prevention, better detection of subclinical or early clinical disease, and treatments for established disease. We report the likelihood, potential impact, and potential cost implications for thirty-four innovations, and we revisit this forecast five years later. Many of the innovations have the potential to greatly affect the costs and outcomes of health care.",10.1377/hlthaff.W5.R67,2005,0,0.0,0.0,0.0,gcg,0.0,1,0
206,Bayesian network models of portfolio risk and return,"A Bayesian network is a tool for modeling large multivariate probability models and fur making inferences from such models. A Bayesian network combines traditional quantitative analysis with expert judgement in an intuitive, graphical representation. In this paper, we show how to use Bayesian networks to model portfolio risk and return. Traditional financial models emphasize the historical relationship between portfolio return and market return. In practice, to forecast portfolio return? financial analysts include expert subjective judgement about other factors that may affect the portfolio. These judgmental factors include special knowledge about the stocks in the portfolio that is not captured in the historical quantitative analysis. We show how a Bayesian network can be used to represent a traditional financial model of portfolio return. Then we show how expert subjective judgement can be included in the Bayesian network model. The output of the model is the posterior marginal probability distribution of the portfolio return, This posterior return distribution can be used to obtain expected return, return variance, and value-at-risk.",0,2000,0,0.0,0.0,0.0,gcg,0.0,1,0
207,Accounting for expert-to-expert variability: A potential source of bias in performance assessments of high-level radioactive waste repositories,"Expert judgments enter several aspects of many scientific endeavors. They are typically employed to interpret data, predict systems' behaviour and assess uncertainties. In particular, expert judgments are expected to be a relevant source of data for use in performance assessments of high-level radioactive waste repositories. In this paper we consider the task of aggregating the judgments provided by experts with the objective of emphasizing a potential source of bias that might affect the results of the analysis. Typically, the analysts combine mathematical and behavioral schemes to obtain the aggregate measures desired for decision-making purposes. Within this approach to data aggregation, mathematical models are used as tools for sensitivity analysis and they should account for between-expert (expert-to-expert) as well as within-expert variability. A practical example regarding a formal expert judgment elicitation exercise for the future climate at Yucca Mountain vicinity is presented. (C) 1997 Elsevier Science Ltd.",10.1016/S0306-4549(96)00052-7,1997,1,1.0,0.0,0.0,gcg,1.0,1,1
208,A novel hybrid approach for landslide susceptibility mapping integrating analytical hierarchy process and normalized frequency ratio methods with the cloud model,"Landslides, which could cause huge losses of lives or property damages, result from several different environmental factors whose influences on landslides are very complex. Therefore, it is essential to understand the relationships between these environmental factors and landslides. Thus, the integration of the analytical hierarchy process (AHP) with the normalized frequency ratio (NFR) is evaluated for landslide susceptibility analyses. However, in addition to these complex relationships, the randomness and fuzziness always affect landslide susceptibility mapping. This study introduces the cloud model (CM) to improve the integrated AHP-NFR method, and proposes a novel hybrid AHP-NFR-CM method for landslide susceptibility analyses, which can better address issues of the randomness and fuzziness. Firstly, ten environmental parameters are selected as landslide impact factors, and their values for all the landslides identified in the study area are obtained through the remote sensing (RS) and geographical information system (GIS) technologies. The AHP method is used to obtain the weight of each landslide impact factor, and the NFR method is used to obtain the weight of each subclass in each landslide impact factor, which can reflect the relationship between the landslide impact factor and landslide occurrence. After applying an appropriate compositional operation between the weights of the landslide impact factors and the weights of the subclasses of the impact factors, a landslide susceptibility index (LSI) for each grid divided via the attribution-based spatial information multi-grid method (ASIMG) can be computed. To solve the inevitable issues of randomness and fuzziness in landslide susceptibility analyses, a cloud model that uses three numerical features (expectation, entropy and hyper-entropy) to represent the intension of the concept, is adopted to improve the methods of AHP and NFR. The relative importance of two landslide impact factors is scaled with the cloud model rather than the Saaty criteria. Pair-wise comparison matrixes of landslide impact factors given by each expert are described by the normal cloud model, and the floating cloud model is used to aggregate all experts' judgments. The weight of each landslide impact factor is also expressed with the cloud model rather than a certain value. In improving the NFR, the weight of each subclass of each landslide impact factor is expressed with the cloud model rather than a certain value. In the improvement of the landslide susceptibility results, the domain of landslide risk assessment results is also displayed with the cloud model instead of a series of definite intervals. As the study area examined is large, several grids would need to be divided, meaning that it would take a considerable amount time to subject the entire study area to landslide susceptibility mapping. Thus, we propose a new attribute-based spatial information multi-grid (ASIMG) division method and introduce grid-computing technology to improve the calculation efficiency during the process. Finally, the proposed hybrid AHP-NFR-CM-ASIMG approach is validated and applied in the study area. It's concluded that the new integration of AHP and NFR methods with the cloud model can consider both randomness and fuzziness and therefore can increase the robustness of landslide susceptibility analyses, while the ASIMG technology can enhance the calculation efficiency in regional landslide susceptibility mapping. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.geomorph.2018.10.024,2019,0,0.0,0.0,0.0,ngr,0.0,1,0
209,Interval type-2 fuzzy logic and its application to occupational safety risk performance in industries,"In this paper, we have developed an interval type-2 fuzzy logic controller (T2FLC) approach for assessment of the risks that workers expose to at construction sites. Using this novel approach, past accident data, subjective judgments of experts, and the current safety level of a construction site are to be combined. The method is then implemented on a tunneling construction site and risk level for all type of accidents is formulated. In T2FLC assists to trace inputs and outputs in a well-organized manner for building the inferences train so that various types of risk assessment can be predicted in industry. Finally, a comparative study has been successfully performed with type-1 and type-2 fuzzy dataset for improving risk assessment that can be easily determined in the type-2 fuzzy prediction model for improving accuracy. Validity of the proposed model is done with the help of statistical analysis and multiple linear regressions.",10.1007/s00500-017-2860-8,2019,1,0.0,0.0,1.0,nw,1.0,1,1
210,Applying a Global Sensitivity Analysis Workflow to Improve the Computational Efficiencies in Physiologically-Based Pharmacokinetic Modeling,"Traditionally, the solution to reduce parameter dimensionality in a physiologically-based pharmacokinetic (PBPK) model is through expert judgment. However, this approach may lead to bias in parameter estimates and model predictions if important parameters are fixed at uncertain or inappropriate values. The purpose of this study was to explore the application of global sensitivity analysis (GSA) to ascertain which parameters in the PBPK model are non-influential, and therefore can be assigned fixed values in Bayesian parameter estimation with minimal bias. We compared the elementary effect-based Morris method and three variance-based Sobol indices in their ability to distinguish ""influential"" parameters to be estimated and ""non-influential"" parameters to be fixed. We illustrated this approach using a published human PBPK model for acetaminophen (APAP) and its two primary metabolites APAP-glucuronide and APAP-sulfate. We first applied GSA to the original published model, comparing Bayesian model calibration results using all the 21 originally calibrated model parameters (OMP, determined by ""expert judgment""-based approach) vs. the subset of original influential parameters (OIP, determined by GSA from the OMP). We then applied GSA to all the PBPK parameters, including those fixed in the published model, comparing the model calibration results using this full set of 58 model parameters (FMP) vs. the full set influential parameters (FIP, determined by GSA from FMP). We also examined the impact of different cut-off points to distinguish the influential and non-influential parameters. We found that Sobol indices calculated by eFAST provided the best combination of reliability (consistency with other variance-based methods) and efficiency (lowest computational cost to achieve convergence) in identifying influential parameters. We identified several originally calibrated parameters that were not influential, and could be fixed to improve computational efficiency without discernable changes in prediction accuracy or precision. We further found six previously fixed parameters that were actually influential to the model predictions. Adding these additional influential parameters improved the model performance beyond that of the original publication while maintaining similar computational efficiency. We conclude that GSA provides an objective, transparent, and reproducible approach to improve the performance and computational efficiency of PBPK models.",10.3389/fphar.2018.00588,2018,0,0.0,0.0,0.0,ngr,0.0,1,0
211,A Weighted Rank aggregation approach towards crowd opinion analysis,"In crowd opinion aggregation models, the expertise of annotators plays an important role to derive the appropriate judgment. It is seen that in most of the aggregation methods annotators' accuracy and bias are considered as two important features and based on it the priority of annotators is assigned. But instead of relying upon these limited features, the quality of annotators can be suitably exploited using rank-based features to further improve the prediction. Basically, the annotators are ranked according to various features and therefrom multiple separate rankings are produced. These rankings, if properly weighted, can lead to obtain the final aggregated ranking in a better way. In this paper, we have developed a novel weighted rank aggregation approach and applied the same on three artificially generated ranking datasets with varying noise. Moreover, the comparative effectiveness of the proposed method is demonstrated by applying it on three Amazon Mechanical Turk datasets. (C) 2018 Elsevier B.V. All rights reserved.",10.1016/j.knosys.2018.02.005,2018,0,0.0,0.0,0.0,ngr,0.0,1,0
212,Structured Approaches to Violence Risk Assessment: A Critical Review,"This article critically reviews the theory, methodology, and empirical evidence pertaining to the various approaches employed by mental health professionals in expert testimony regarding the probability of future violence for people within the jurisprudence system. Many professionals rely on unstructured clinical assessment, which allows the evaluator to fully capitalize on their clinical experience but is vulnerable to cognitive and situational biases that negatively affect the validity and reliability of the evaluation. In contrast, actuarial assessment involves statistical estimation of violence risk based on certain combinations of criminogenic variables derived from prospective analysis of recidivism in various offender groups. Structured professional judgment relies on professional expertise with a structured checklist application, and thus attempts to minimize the limitations of unstructured clinical and actuarial assessment while retaining the strengths of each. Although an improvement on unstructured assessment, structured applications have significant limitations, highlighting the importance of a multimethod approach to violence risk assessment.",10.3928/00485713-20170803-02,2017,0,0.0,0.0,0.0,ngr,0.0,1,0
213,The role of human fatigue in the uncertainty of measurement,"Risk of human error in measurement and testing is the result of the causal combination of factors and events that are involved in the process. This paper presents how to model technical and human errors and how these could interact in order to influences the reliability of measurement/test. Human errors were designed according with a System Dynamics approach with factors and states those are part of human's state and ability to handle with the process and procedures and instruments. Technical errors were related to the environment, its organization and suitability with standards. Human and Technical factors have been therefore integrated in order to predict states affecting the consistency of measure and uncertainty in range. Optimal combination of factors - based on a System Dynamics simulation and expert judgments - has been proposed according with a sampling analysis. (C) 2017 The Authors. Published by Elsevier B.V.",10.1016/j.promfg.2017.09.092,2017,0,0.0,0.0,0.0,ngr,0.0,1,0
214,Optimal ordering policy for newsvendor models with bidirectional changes in demand using expert judgment,"Demand forecast is a critical determinant of order quantity under newsvendor problem (NVP) framework and warrants major revision in the event of changing circumstances or happening of some unforeseen events having potential to alter the demand. Retailers of single period products such as fashion apparels are required to pass their orders far ahead of selling seasons and apply preseason two-stage ordering procedure, where an initial order (first stage) is followed by a final confirmed order (second stage). The enterprise forecasting experts may get additional information related to the occurrence of some unforeseen events that may significantly impact the initial demand estimation. In this paper, the potential impact of such events is combined using a weight factor to obtain revised demand forecasts. In this context, this paper develops inventory models under NVP framework to determine the optimal order quantity and weight factor on the basis of revised forecasts. Considering the bidirectional changes in demand, we formulate a unique objective function that operates as a profit maximization function for the positive demand adjustment and turns into a cost minimization function for the negative demand adjustment. Models developed without constraints at first instance are extended subsequently by incorporating constraints of budget limits, storage space capacity and required service level. Near closed form expressions of decision variables for four demand distributions with multiplicative demand forms are presented. The results demonstrate economic benefits of using revised demand through models developed, negative impact of constraints, and role of demand distribution entropy in determining the order size and expected profit.",10.1007/s12597-016-0248-7,2016,0,0.0,0.0,0.0,ngr,0.0,1,0
215,Comparing models for quantitative risk assessment: an application to the European Registry of foreign body injuries in children,"Risk Assessment is the systematic study of decisions subject to uncertain consequences. An increasing interest has been focused on modeling techniques like Bayesian Networks since their capability of (1) combining in the probabilistic framework different type of evidence including both expert judgments and objective data; (2) overturning previous beliefs in the light of the new information being received and (3) making predictions even with incomplete data. In this work, we proposed a comparison among Bayesian Networks and other classical Quantitative Risk Assessment techniques such as Neural Networks, Classification Trees, Random Forests and Logistic Regression models. Hybrid approaches, combining both Classification Trees and Bayesian Networks, were also considered. Among Bayesian Networks, a clear distinction between purely data-driven approach and combination of expert knowledge with objective data is made. The aim of this paper consists in evaluating among this models which best can be applied, in the framework of Quantitative Risk Assessment, to assess the safety of children who are exposed to the risk of inhalation/insertion/aspiration of consumer products. The issue of preventing injuries in children is of paramount importance, in particular where product design is involved: quantifying the risk associated to product characteristics can be of great usefulness in addressing the product safety design regulation. Data of the European Registry of Foreign Bodies Injuries formed the starting evidence for risk assessment. Results showed that Bayesian Networks appeared to have both the ease of interpretability and accuracy in making prediction, even if simpler models like logistic regression still performed well.",10.1177/0962280213476167,2016,0,0.0,0.0,1.0,nw,1.0,0,0
216,Robust modeling of continuous 4-D affective space from EEG recording,"The inherent intangible nature, complexity, context-specific interpretations of emotions make it difficult to quantify and model affective space. Dimensional theory is one of the effective methods to describe and model emotions. Despite recent advances in affective computing, modeling continuous affective space remains a challenge. Here, we present a computational framework to study the role of functional areas of brain and band frequencies in modeling 4-D continuous affective space (Valence, Arousal, Like and Dominance). In particular, we used Electroencephalogram (EEG) recordings and adopted a recursive feature elimination (RFE) approach to select band frequencies and electrode locations (functional areas) that are most relevant for predicting affective space. Empirical analyses on DEAP dataset [1] reveals that only a small number of locations (7-12) and certain band frequencies carry most discriminative information. Using the selected features, we modeled 4-D affective space using Support Vector Regression (SVR). Regression analysis show that Root Mean Square Error (RMSE) for Valence, Arousal, Dominance, Like are 1.40, 1.23, 1.24 and, 1.24, respectively. Besides SVR, the performance of feature fusion and ensemble classifiers were also compared to determine the robust model against technical noise and individual variations. It was observed that the prediction accuracy of the final model is up to 37% better than human judgment evaluated on same data set. Spillover effect of our approach may include design of task-specific (i.e., emotion, memory capacity) EEG headset with a minimal number of electrodes.",10.1109/ICMLA.2016.154,2016,0,0.0,0.0,0.0,ngr,0.0,1,0
217,Weighting Components of a Composite Score Using Naive Expert Judgments About Their Relative Importance,"A common problem that arises in testingas well as other contexts such as candidate selectionis how to combine various scores into a weighted composite that reflects expert judgments about each component's relative importance. For experts to provide nominal weights explicitly, they must fully account for the variances of the components, the covariances among components, and the reliability of each component. This task can be challenging, and in many cases, experts may have greater success making simple judgments about component importance without regard for the variances, covariances, and reliabilities. In this article, it is shown how to estimate the requisite nominal weights when only these kinds of naive judgments are available, and the analytical solution is demonstrated with a small simulation study. Results from the simulation suggest that the proposed estimators could yield more valid composite scores in practice.",10.1177/0146621615584703,2015,1,1.0,0.0,0.0,gcg,1.0,1,1
218,"Calibration, sharpness and the weighting of experts in a linear opinion pool","Linear opinion pools are the most common form of aggregating the probabilistic judgments of multiple experts. Here, the performance of such an aggregation is examined in terms of the calibration and sharpness of the component judgments. The performance is measured through the average quadratic score of the aggregate. Trade-offs between calibration and sharpness are examined and an expression for the optimal weighting of two dependent experts in a linear combination is given. Circumstances where one expert would be disqualified are investigated. Optimal weights for the multiple, dependent experts are found through a concave quadratic program.",10.1007/s10479-015-1846-0,2015,1,0.0,0.0,1.0,nw,1.0,1,1
219,Automated Classification of Electrically-evoked Compound Action Potentials,"Electrically-evoked compound action potentials (ECAPs) is an objective measure of peripheral neural encoding of electrical stimulation delivered by cochlear implants (CIs) at the auditory nerve level. ECAPs play a key role in automated CI fitting and outcome diagnosis, as long as presence of genuine ECAP is accurately detected automatically. Combination of ECAP amplitudes and signal-to-noise ratio are shown to efficiently detect true responses, by comparing them to subjective visual expert judgments. Corresponding optimal thresholds were calculated from Receiver-Operating-Characteristic curves. This was conducted separately on three artifact rejection methods: alternate polarity, masker-probe and modified-masker-probe. This model resulted in sensitivity and specificity error of 3.3% in learning, 3.5% in testing and 5.0% in verification. It was found that the following combination of ECAP amplitude and signal-to-noise ratio would be accurate predictors: 22 mu V and 1.3 dB SNR thresholds for alternate polarity, 35 mu V and -0.2 dB for masker-probe and 44 mu V and -0.2 dB for modified-masker-probe.",0,2015,0,0.0,0.0,0.0,nw,0.0,1,0
220,Weighted Diagnostic Criteria for Developmental Dysplasia of the Hip,"Objective To establish clinical diagnostic criteria for developmental dysplasia of the hip (DDH) that model the practices of expert clinicians. Study design Of 23 clinical criteria for the diagnosis of DDH, ranked in order of diagnostic importance by international consensus, the 7 most highly ranked were placed in all possible combinations to create unique case vignettes. Twenty-six experts rated 52 vignettes for the presence of DDH. We modeled the data to determine which of the 7 criteria were associated with a clinician's opinion that the vignette represented DDH. From the resulting regression coefficients, for each vignette we calculated a probability of DDH. An independent panel rated the same vignettes using a visual analog scale response. We correlated the visual analog scale ratings with probabilities derived from the model. Results Our model identified 4 of 7 criteria as predictive of DDH (P < .001): Ortolani/Barlow test (beta = 3.26), limited abduction (beta = 1.48), leg length discrepancy (beta = 0.74), and first-degree family history of DDH (beta = 1.39). There was substantial correlation between the probability of DDH predicted by the model and that derived from an independent expert panel (r = 0.73; P < .001). Conclusion Weighted clinical criteria for inferring the likelihood of DDH produced consistent results in the judgment of 2 separate groups of experts. Using these weights, nonexperts could establish the probability of DDH in a manner approaching the practice of clinical experts.",10.1016/j.jpeds.2014.08.023,2014,0,0.0,1.0,0.0,ngr,1.0,0,0
221,Assimilation and contrast effects: The role of self-construal and regulatory focus as moderators in collectivistic cultures of honour,"Human judgments are context dependent. When answering a question about one's overall satisfaction with life, a previous question about one's romantic life might pose redundancy problems influencing one's judgment of life satisfaction, something known as item order effects. However, in order to detect such redundancy, one needs to pay attention to the context of the conversation. Any variable that influences the amount of attention given the context of the conversation can determine whether the presumed redundancy is detected or not. In three studies, two experiments and one correlational study, we tested the influence of induced self-construal (study 1) and self-regulatory focus (study 2) and self-regulatory focus measured as an individual difference variable (study 3) as moderators of context effects among college students from Mexico. In study 1, participants induced to have an independent mindset were less likely to detect the redundancy posed by two questions, resulting, as predicted, in a contrast effect. In study 3, participants with lower levels of prevention focus were less likely to detect the redundancy posed by the same two questions as study 1, resulting, as predicted, in an assimilation effect. The implications of the results were discussed within the framework of the inclusion/exclusion model.",10.1002/ijop.12023,2014,0,0.0,0.0,0.0,nw,0.0,1,0
222,POWER-SPECTRAL ANALYSIS OF HEAD MOTION SIGNAL FOR BEHAVIORAL MODELING IN HUMAN INTERACTION,"We examine whether head motion can be used for predicting human expert's judgments of behavioral characteristics relevant to the couples therapy domain. Specifically we predict ""high"" or ""low"" presence of several behavioral characteristics such as ""Blame"" that are discerned by human experts, through data-driven clustering of the head motion signal based on power-spectral features. We employ the distribution of motion samples in each cluster for behavior judgment prediction. We find clustering horizontal and vertical motion separately is superior to combined clustering in predicting behavior. The performance of gender-specific and gender-independent clustering of head motion is comparable in average while different for each gender. The proposed power-spectral features outperform linear prediction features in average. Using data from a clinical study of distressed couples, we empirically show that the derived clusters quantize head motion into meaningful types that relate to interpretable behavior characteristics. These findings demonstrate the feasibility of inferring behavior characteristics from head motion signals.",0,2014,0,0.0,0.0,0.0,ngr,0.0,1,0
223,Investment Decision Support for Engineering Projects Based on Risk Correlation Analysis,"Investment decisions are usually made on the basis of the subjective judgments of experts subjected to the information gap during the preliminary stages of a project. As a consequence, a series of errors in risk prediction and/or decision-making will be generated leading to out of control investment and project failure. In this paper, the variable fuzzy set theory and intelligent algorithms integrated with case-based reasoning are presented. The proposed algorithm manages the numerous fuzzy concepts and variable factors of a project and also sets up the decision-making process in accordance with past cases and experiences. Furthermore, it decreases the calculation difficulty and reduces the decision-making reaction time. Three types of risk correlations combined with different characteristics of engineering projects are summarized, and each of these correlations is expounded at the project investment decision-making stage. Quantitative and qualitative change theories of variable fuzzy sets are also addressed for investment risk warning. The approach presented in this paper enables the risk analysis in a simple and intuitive manner and realizes the integration of objective and subjective risk assessments within the decision-makers' risk expectation.",10.1155/2012/242187,2012,0,0.0,0.0,0.0,ngr,0.0,1,0
224,Efficient neural-network-based no-reference approach to an overall quality metric for JPEG and JPEG2000 compressed images,"Reliably assessing overall quality of JPEG/JPEG2000 coded images without having the original image as a reference is still challenging, mainly due to our limited understanding of how humans combine the various perceived artifacts to an overall quality judgment. A known approach to avoid the explicit simulation of human assessment of overall quality is the use of a neural network. Neural network approaches usually start by selecting active features from a set of generic image characteristics, a process that is, to some extent, rather ad hoc and computationally extensive. This paper shows that the complexity of the feature selection procedure can be considerably reduced by using dedicated features that describe a given artifact. The adaptive neural network is then used to learn the highly nonlinear relationship between the features describing an artifact and the overall quality rating. Experimental results show that the simplified feature selection procedure, in combination with the neural network, indeed are able to accurately predict perceived image quality of JPEG/JPEG2000 coded images. (C) 2011 SPIE and IS&T. [DOI: 10.1117/1.3664181]",10.1117/1.3664181,2011,0,0.0,0.0,0.0,gcg,0.0,1,0
225,Event-related brain responses as correlates of changes in predictive and affective values of conditioned stimuli,"Previous evidence suggests that the judged predictive strength of one cue may be influenced by the predictive strengths of other pretrained cues (prediction errors). In the present study, we examined affective ratings and event-related brain responses from 18 healthy participants during an aversive conditioning task in which affective values of previously trained conditioned stimuli were modified through a blocking procedure. The task was divided into two phases. During the training phase, single stimulus A (e.g., red square) was always followed by aversive picture stimuli, while single stimulus B (e.g., yellow square) was signaling the absence of aversive stimulation. During the blocking phase, compound stimuli consisted of the combination of one single trained stimulus (A or B) and one new somatosensory stimulus were also followed by the presence of aversive stimulation. Results indicated that single stimulus A elicited greater ERP amplitudes and theta power, and was rated as more unpleasant than single stimulus B during the training phase. Moreover, single stimulus B elicited greater ERP amplitudes than stimulus A, as well as greater theta power and more unpleasant ratings during the blocking as compared with the training phase. By contrast, no changes in ERP amplitudes and theta power were observed for stimulus A. Our findings provide neurophysiological and behavioral evidence for an increased affective processing of conditioned stimuli when compound stimuli were introduced, but only if the target CS was previously trained to signal the absence of aversive stimulation. (C) 2011 Elsevier B.V. All rights reserved.",10.1016/j.brainres.2011.07.049,2011,0,0.0,0.0,0.0,gcg,0.0,1,0
226,FUZZY INDICES OF ECOLOGICAL CONDITIONS: REVIEW OF TECHNIQUES AND APPLICATIONS,"The impact of human activities on natural ecosystems is becoming more and more severe, with consequences that are difficult to forecast for middle-term and long-term periods. Hence, routine monitoring and quality assessment is a necessary practice for effective environmental management. However, the definition of techniques and criteria for monitoring is not a trivial task: identification of new indices of environmental quality is a hot research topic in the field of applied ecology. Some critical issues of ecological quality assessment are represented by the different sources of uncertainty involved in the measurement, modelling and classification processes. Measurements of environmental variables are often imprecise due to natural variability and background noise; interactions between single variables are complex, as is the behaviour of natural ecosystems. Moreover, the translation of quantitative measurements into quality classes requires a subjective classification: the definition of threshold values of acceptability depends on the personal knowledge of the scientist who designs the model. The management of all of these potential sources of uncertainty requires suitable instruments. Fuzzy logic represents an effective technique for coping with such problems, as it allows the use of information that other methods cannot include, such as individual knowledge and experience, to combine quantitative and qualitative data in modeling ecological complexity. Fuzzy logic is a mathematical formalism that translates expert judgment expressed in linguistic terms into precise numbers, so the laws of fuzziness are scientifically sound; at the same time, it allows avoidance of artificial precision and producing results that are more similar to the real world. For all of these reasons, since the middle 1990s fuzzy indices of ecological conditions have been proposed and tested in air, water and soil environments. Fuzzy techniques have been applied at different stages of the assessment process, as the coding of species' biological traits, the processing of qualitative data, the definition of ecological rules, and the multicriteria evaluation of ecosystems. The objective of this chapter is to review the most relevant fuzzy techniques applied to the ecological quality assessment, providing a rational classification of such techniques, in order to allow a qualitative comparison of the considered approaches. Moreover, this chapter aims to highlight the most promising applications of fuzzy logic to ecological research in order to show the benefits of fuzzy-based approaches when coping with the high complexity of ecosystems.",0,2009,0,0.0,0.0,0.0,gcg,0.0,1,0
227,An Acceptability Predictor for Websites,"User acceptance is a high priority for website design and implementation. Two significant, but largely separate. approaches to acceptability are: First, the Web Accessibility Initiative (WAI) has explored the measurement of technical features of a website to gauge its accessibility. Second, human judgments about acceptability are obtained from intended users or experts. The present work explores the important question of how best to combine these two methods. Experiment One required new users to explore automatic website evaluation systems. They found two of four systems difficult or impossible to use and system Outputs difficult to understand. Experiment Two combines formal properties and user judgments, using an automatic system to predict user judgments from formal website properties. A simple system was able to predict user judgments within 91% accuracy. Clearly, user judgments about websites can be predicted reliably, a result of value to designers.",0,2009,1,0.0,0.0,1.0,nw,1.0,1,1
228,Life-cycle performance of structures: combining expert judgment and results of inspection,"Current bridge management systems base decisions on the results of visual inspections. These systems consider visual inspection results as accurate and disregard any further information available. In the present study, the result of each inspection is considered as a random variable, dependent of a wide range of factors, that can be integrated with other sources of information, including expert judgment and results of other inspections. The combination of different sources of information results in reliable posterior information and allows more accurate predictions of future deterioration. In the present paper, performance of an existing structure is obtained in terms of the condition index, which describes the effects of deterioration as can be seen by an inspector, and the safety index, which measures the safety margin of the structure. The reduction in uncertainty associated with periodical inspections is considered through updating of performance profiles. The updating of the condition index is direct, as new information on this parameter is collected by the inspector. In terms of safety, however, only indirect information is collected and the uncertainty reduction associated with an inspection is significantly lower. Several realistic examples show the impact of inspections on the predicted life-cycle performance of structures.",0,2008,1,0.0,0.0,0.0,ngr,0.0,0,0
229,A hybrid econometric-AI ensemble learning model for Chinese foreign trade prediction,"Due to the complexity of economic system, the interactive effects of economic variables or factors on Chinese foreign trade make the prediction of China's foreign trade extremely difficult. To analyze the relationship between economic variables and foreign trade, this study proposes a novel nonlinear ensemble learning methodology hybridizing nonlinear econometric model and artificial neural networks (ANN) for Chinese foreign trade prediction. In this proposed learning approach, an important econometrical model, the co-integration-based error correction vector auto-regression (EC-VAR) model is first used to capture the impacts of the economic variables on Chinese foreign trade from a multivariate analysis perspective. Then an ANN-based EC-VAR model is used to capture the nonlinear patterns hidden between foreign trade and economic factors. Subsequently, for introducing the effects of irregular events on foreign trade, the text mining and expert's judgmental adjustments are also incorporated into the nonlinear ANN-based EC-VAR model. Finally, all economic variables, the outputs of linear and nonlinear EC-VAR models and judgmental adjustment model are used as another neural network inputs for ensemble prediction purpose. For illustration, the proposed ensemble learning methodology integrating econometric techniques and artificial intelligence (AI) methods is applied to Chinese export trade prediction problem.",0,2007,1,0.0,0.0,0.0,ngr,0.0,0,0
230,"Optimal combined load, forecast based on the improved analytic hierarchy process","Different forecasting methods can lead to very different results in power system load forecast. In order to improve forecasting accuracy, combined forecasting methods have been introduced in solving the engineering forecasting problems. In this paper, the conception of soft method is first presented in power system load forecast. The computing and analyzing methods in soft science such as decision-making analyses have been introduced to solve load forecast that are unstructured problems of multi-factors. The combined forecasting problem is treated as multi-hierarchies and multi-factors evaluation by composing qualitative analyses and quantitative calculation. In addition, the experiences and judgments of experts will be collected to implement judgment matrices in group decision-making. The soft method based on improved analytic hierarchy process (AHP) is proposed to carry out long-middle term load combined forecast in this paper. A hierarchy structure has been established by analyzing various factors that affect the load forecast. It is the key to determine the combined weight coefficients in the optimal combined forecasting method. Fuzzy complementary judgment matrixes of pairwise comparison will be formed by expert in each hierarchy and be converted to a fuzzy consistent matrix. The eigenvector can be calculated using its general formula and be regarded as weight coefficient in combined forecasting. The combined forecast methods based on the improved AHP is of clear hierarchy structure, sufficient judgment information and simple calculation formula. The forecasting examples show that this method is practical, convenient and accurate.",0,2002,1,0.0,0.0,1.0,nw,1.0,1,1
231,Principles of forecasting - A short overview,"Forecasting procedures are needed when there is uncertainty about the future. In our contribution we discuss some principles that can help to make more accurate forecasts and help to better assess the uncertainty associated with forecasts. We mainly discuss statistical forecasting procedures, but other principles based on experts (judgmental forecasting) and integrating and combining approaches are also mentioned. We show some results of forecasting in two different application areas.",0,1999,1,0.0,0.0,0.0,nw,0.0,0,0
232,The human factor in supply chain forecasting: A systematic review,Demand forecasts are the lifeblood of supply chains. Academic literature and common industry practices indicate that demand forecasts are often subject to human interventions. Judgmental forecasting or judgmental forecast adjustments can cause both positive and negative repercussions to the rest of the supply chain. This paper provides the first systematic literature review of judgmental forecasting and adjustments focusing on key features that impact various decisions in supply chains. A carefully assembled and shortlisted literature pool is analyzed for systematic mapping of the published works using bibliometric tools. The primary sub streams of research within the broader scope of the field are synthesized from a rigorous keyword cluster analysis and a thorough discussion is presented. Our review concludes by encapsulating the key learnings from four decades of academic research in judgmental forecasting and suggests future research avenues to expand our understanding of the role of humans in demand forecasting and supply chain decision-making. (C) 2018 Elsevier B.V. All rights reserved.,10.1016/j.ejor.2018.10.028,2019,0,0.0,0.0,0.0,nw,0.0,1,0
233,Bayesian-model averaging using MCMCBayes for web-browser vulnerability discovery,"Most software vulnerabilities are preventable, but they continue to be present in software releases. When Blackhats, or malicious researchers, discover vulnerabilities, they often release corresponding exploit software and malware. Therefore, customer confidence could be reduced if vulnerabilities-or discoveries of them-are not prevented, mitigated, or addressed. In addressing this, managers must choose which alternatives will provide maximal impact and could use vulnerability discovery modeling techniques to support their decision-making process. Applications of these techniques have used traditional approaches to analysis and, despite the dearth of data, have not included information from experts. This article takes an alternative approach, applying Bayesian methods to modeling the vulnerability-discovery phenomenon. Relevant data was obtained from security experts in structured workshops and from public databases. The open-source framework, MCMCBayes, was developed to automate performing Bayesian model averaging via power-posteriors. It combines predictions of interval-grouped discoveries by performance-weighting results from six variants of the non-homogeneous Poisson process (NHPP), two regression models, and two growth-curve models. The methodology is applicable to software-makers and persons interested in applications of expert-judgment elicitation or in using Bayesian analysis techniques with phenomena having non-decreasing counts over time.",10.1016/j.ress.2018.11.030,2019,0,0.0,0.0,0.0,gcg,0.0,1,0
234,Stochastic control and optimization in retail distribution: an empirical study of a Korean fashion company,"Considerable research has recently been conducted on improving the business performance of the fashion industry through supply-chain streamlining. In addition, the accuracy enhancement of sales forecasting by using statistical methods and machine learning algorithms to minimize inventory and improve profitability has been investigated. However, few studies have focused on solving the initial distribution problem to reduce logistical costs and loss of sales opportunities. This study solves the mathematical problems related to initial distributions of fashion products using stochastic control and optimization by conducting an empirical study using real data from a leading Korean fashion company. The initial distribution of a small quantity of items among numerous shops and the distribution of special sizes produced in small quantities were examined. Monte-Carlo simulations and Lebesgue's convergence theorem were considered useful for determining initial distributions of stock produced in small quantities. Furthermore, optimal initial distributions can be achieved when experience-based expert judgment is combined with mathematical modeling using stochastic control and optimization.",10.1080/00405000.2018.1478268,2019,0,0.0,0.0,1.0,nw,1.0,0,0
235,Unpredictable and competitive cues affect prosocial behaviors and judgments,"Why natural selection would favor thoughts or behaviors that benefit others at the cost of oneself (prosociality) in humans is an intriguing question. The present studies explored two kinds of cues representing overarching environmental factors that might affect prosociality: unpredictability, which represents the variability of extrinsic threats, and competition, which represents the relevance of others performance to one's fitness. In three experiments, we also took into account the interaction between the two environmental factors and two moderators, namely resource availability and prosocial thinking types. In each experiment, participants were exposed to cues of unpredictability and/or competition before assessment of spontaneous prosocial behaviors (Studies 1 and 2) or prosocial judgments in dual-choice dilemmas (Study 3). Results showed that unpredictable cues generally led to lower prosocial behaviors and fewer prosocial judgments (Studies 2 & 3). In contrast, competitive cues led to lower prosocial behaviors among individuals with resource disadvantages (Study 1), and when combined with unpredictable cues (Study 2). However, competition also led to higher prosocial behaviors among individuals with resource advantages (Study 1) and more prosocial judgments in response to rational, utilitarian dilemmas (Study 3). Taken together, these results indicated that human prosociality is affected by environmental factors in predictable ways.",10.1016/j.paid.2018.10.006,2019,0,0.0,0.0,0.0,gcg,0.0,1,0
236,Development of a Group Judgment Process for Forecasts of Health Care Innovations,"IMPORTANCE Health care costs have increased substantially over the past few decades, in part owing to the development and diffusion of new medical treatments. Forecasting potential future technologic innovations can allow for more informed planning. OBJECTIVE To assess the predictive validity of a structured formal method for forecasting future technologic innovations in health care. DESIGN, SETTING, AND PARTICIPANTS This pilot study combined an untested, unvalidated combination of a consensus process and group judgment process to evaluate forecasts made in 2001 for technologic innovations by 2021 in Alzheimer disease (AD) and cardiovascular disease (CVD). Six experts in AD and 7 experts in CVD composed the judgment group. The study was conducted in 2017-2018. MAIN OUTCOMES AND MEASURES Year 2001 forecasts for 2021 that were judged by experts as being close to correct, directionally correct, or not correct, as well as innovations that occurred since 2001 that were not predicted. RESULTS Four forecasts of innovations in AD, each considered to be between 30% and 40% likely to be achieved by 2021, were judged to be close to correct. One forecast was considered to be directionally correct, with a likelihood of occurrence of 40%, in that it was overoptimistic. One innovation that occurred was missed: new imaging techniques (amyloid beta plaque and tau tangle positron emission tomographic imaging). Five forecasts of CVD innovations were considered to be at least 50% likely to occur by 2021, and of these, 2 were judged to be close to correct, 1 was judged as being directionally correct, and 2 were judged as being not correct (although in one of these forecasts, the overarching innovation has been achieved but with a different noninvasive imaging modality). Of 7 additional forecasts considered to be less likely to be achieved by 2021, 4 were judged to be close to correct and 3 were judged as being directionally correct. Two innovations occurred but were missed: transcatheter aortic valve replacement and cardiac resynchronization therapy. Across both conditions, 15 of 17 innovations forecasted were judged to be close to correct or directionally correct, 2 were judged to be incorrect, and there were 3 missed innovations. CONCLUSIONS AND RELEVANCE Expert elicitation provided a useful, but not fully accurate, lens into future innovation.",10.1001/jamanetworkopen.2018.5108,2018,0,0.0,1.0,0.0,ngr,1.0,0,0
237,Bayesian Analytics for Estimating Risk Probability in PPP Waste-to-Energy Projects,"Appropriate risk analysis and management is critical to the overall success in public-private partnership (PPP) projects, in which one of the key issues lies in an accurate estimation of the risk occurrence probability. Traditionally, this probability is estimated either relying on experts' judgments or historical data. The estimation may not be accurate due to the subjective nature of the former and the data sparsity of the latter. In this research, a Bayesian analytic approach is taken to forecast risk occurrence probability, combining experts' judgments and historical data. This Bayesian approach consists of four main steps: (1) data collection, (2) modeling prior probability, (3) modeling posterior probability, and (4) multiupdating and analytics. This approach can achieve a more accurate estimation of risk occurrence probability compared with only relying on experts' judgments or historical data because the subjectivity of experts' judgments is mitigated by incorporating observed real data, and the data sparsity is supplemented by experts' judgments. This model is applied to forecast the probability of several critical risks in PPP waste-to-energy (WTE) incineration projects in China, and the results demonstrate its feasibility and applicability for targeted solutions in risk response and allocation.",10.1061/(ASCE)ME.1943-5479.0000658,2018,1,0.0,1.0,0.0,ngr,1.0,1,1
238,A technical framework for automatic perceptual evaluation of singing quality,"Human experts evaluate singing quality based on many perceptual parameters such as intonation, rhythm, and vibrato, with reference to music theory. We proposed previously the Perceptual Evaluation of Singing Quality (PESnQ) framework that incorporated acoustic features related to these perceptual parameters in combination with the cognitive modeling concept of the telecommunication standard Perceptual Evaluation of Speech Quality to evaluate singing quality. In this study, we present further the study of the PESnQ framework to approximate the human judgments. First, we find that a linear combination of the individual perceptual parameter human scores can predict their overall singing quality judgment. This provides us with a human parametric judgment equation. Next, the prediction of the individual perceptual parameter scores from the PESnQ acoustic features show a high correlation with the respective human scores, which means more meaningful feedback to learners. Finally, we compare the performance of early fusion and late fusion of the acoustic features in predicting the overall human scores. We find that the late fusion method is superior to that of the early fusion method. This work underlines the importance of modeling human perception in automatic singing quality assessment.",10.1017/ATSIP.2018.10,2018,0,0.0,0.0,0.0,gcg,0.0,1,0
239,The Value of Performance Weights and Discussion in Aggregated Expert Judgments,"In risky situations characterized by imminent decisions, scarce resources, and insufficient data, policymakers rely on experts to estimate model parameters and their associated uncertainties. Different elicitation and aggregation methods can vary substantially in their efficacy and robustness. While it is generally agreed that biases in expert judgments can be mitigated using structured elicitations involving groups rather than individuals, there is still some disagreement about how to best elicit and aggregate judgments. This mostly concerns the merits of using performance-based weighting schemes to combine judgments of different individuals (rather than assigning equal weights to individual experts), and the way that interaction between experts should be handled. This article aims to contribute to, and complement, the ongoing discussion on these topics.",10.1111/risa.12992,2018,1,0.0,0.0,1.0,nw,1.0,1,1
240,"Predicting elections: Experts, polls, and fundamentals","This study analyzes the relative accuracy of experts, polls, and the so-called 'fundamentals' in predicting the popular vote in the four U.S. presidential elections from 2004 to 2016. Although the majority (62%) of 452 expert forecasts correctly predicted the directional error of polls, the typical expert's vote share forecast was 7% (of the error) less accurate than a simple polling average from the same day. The results further suggest that experts follow the polls and do not sufficiently harness information incorporated in the fundamentals. Combining expert forecasts and polls with a fundamentals-based reference class forecast reduced the error of experts and polls by 24% and 19%, respectively. The findings demonstrate the benefits of combining forecasts and the effectiveness of taking the outside view for debiasing expert judgment.",0,2018,0,0.0,0.0,1.0,nw,1.0,0,0
241,A Novel Hybrid ABC-PSO Algorithm for Effort Estimation of Software Projects Using Agile Methodologies,"In modern software development processes, software effort estimation plays a crucial role. The success or failure of projects depends greatly on the accuracy of effort estimation and schedule results. Many studies focused on proposing novel models to enhance the accuracy of predicted results; however, the question of accurate estimation of effort has been a challenging issue with regards to researchers and practitioners, especially when it comes to projects using agile methodologies. This study aims at introducing a novel formula based on team velocity and story point factors. The parameters of this formula are then optimized by employing swarm optimization algorithms. We also propose an improved algorithm combining the advantages of the artificial bee colony and particle swarm optimization algorithms. The experimental results indicated that our approaches outperformed methods in other studies in terms of the accuracy of predicted results.",10.1515/jisys-2016-0294,2018,0,0.0,1.0,0.0,ngr,1.0,0,0
242,Automatic Chinese Multiple Choice Question Generation Using Mixed Similarity Strategy,"Automatic question generation can help teachers to save the time necessary for constructing examination papers. Several approaches were proposed to automatically generate multiple-choice questions for vocalbuary assessment or grammar exercises. However, most of these studies focused on generating questions in English with a certain similarity strategy. This paper presents a mixed similarity strategy which generates Chinese multiple choice distractors with a statistical regression model including orthographic, phonological and semantic features, i.e., features that were shown in previous psycholinguistics studies to contribute to character recognition. In a first experiment, we evaluated the predictive power of the proposed features in measuring Chinese character similarity. One of the significant experimental results showed that the combination of the four proposed categories of features (structure, semantic radical, stroke and meaning) accounts for 62.5 percent of the variance in the human judgments of character similarity. In the second experiment, a user study was conducted to evaluate the quality of system-generated questions using a test item analysis method. Two hundred ninety-six Chinese primary school students (10-11-year-old) participated in this study. We have compared the mixed strategy with another three common distractor generation strategies, orthographic strategy, semantic strategy, and phonological strategy One of important findings suggested that the mixed strategy significantly outperformed other three strategies in terms of the distractor usefulness and has a highest discrimination power among four strategies.",10.1109/TLT.2017.2679009,2018,0,0.0,0.0,0.0,nw,0.0,1,0
243,MACHINE LEARNING FOR ALLOY COMPOSITION AND PROCESS OPTIMIZATION,"The drive for greater efficiency in turbomachinery has led to increasingly stringent specifications for the materials used. Current methods for optimizing alloy composition and processing to meet these requirements typically rely on a combination of expert judgment and trial and error. Machine learning offers an alternative approach that leverages data resources to significantly accelerate the optimization timeline through systematic data-informed decision making. In this paper, we demonstrate the effectiveness of machine learning methods for three different alloys classes: aluminum alloys, nickel-based superalloys, and shape memory alloys. In the first two alloy classes, models are built for the alloy mechanical properties based on the composition and processing information. In the case of shape memory alloys, a model is trained to predict the austenite to marten site transformation temperatures. In addition to achieving high baseline performance, we leverage recent methodological developments to provide well-calibrated, heteroscedastic uncertainty estimates with each prediction. By wrapping these models in an inverse design routine that takes full advantage of uncertainty information, we are able to demonstrate the feasibility of designing new alloys to meet prescribed specifications. The results indicate that this approach has the potential to fundamentally change how new structural and functional alloys are developed.",0,2018,0,0.0,0.0,0.0,ngr,0.0,1,0
244,Computational Scaling of Shape Similarity That Has Potential for Neuromorphic Implementation,"Current methods for encoding and comparing shapes are computationally demanding and are not suitable for image processing in small portable devices. Here, we describe a simple scan encoding method for transcribing shape information into a 1 -D summary. Summaries were derived from an inventory of unknown shapes, and these values were used to scale the degree of similarity of pair combinations. The scale values provided a significant level of prediction of human judgments in a match recognition task, suggesting substantial correspondence with human perception of shape similarity. Similarity scores derived with the Procrustes method did not predict human judgments.",10.1109/ACCESS.2018.2853656,2018,0,0.0,0.0,0.0,ngr,0.0,1,0
245,Recommending Outfits from Personal Closet,"We consider grading a fashion outfit for recommendation, where we assume that users have a closet of items and we aim at producing a score for an arbitrary combination of items in the closet. The challenge in outfit grading is that the input to the system is a bag of item pictures that are unordered and vary in size. We build a deep neural network-based system that can take variable-length items and predict a score. We collect a large number of outfits from a popular fashion sharing website, Polyvore, and evaluate the performance of our grading system. We compare our model with a random-choice baseline, both on the traditional classification evaluation and on people's judgment using a crowdsourcing platform. With over 84% in classification accuracy and 91% matching ratio to human annotators, our model can reliably grade the quality of an outfit. We also build an outfit recommender on top of our grader to demonstrate the practical application of our model for a personal closet assistant.",10.1109/WACV.2018.00036,2018,0,1.0,0.0,0.0,gcg,1.0,0,0
246,Combination of nuclear NF-kappa B/p65 localization and gland morphological features from surgical specimens is predictive of early biochemical recurrence in prostate cancer patients,"Identifying patients who are high-risk for biochemical recurrence (BCR) following radical prostatectomy could enable direction of adjuvant therapy to those patients while sparing low-risk patients the side effects of treatment. Current BCR prediction tools require human judgment, limiting repeatability and accuracy. Quantitative histomorphometry (QH) is the extraction of quantitative descriptors of morphology and texture from digitized tissue slides. These features are used in conjunction with machine learning classifiers for disease diagnosis and prediction. Features quantifying gland orientation disorder have been found to be predictive of BCR. Separately, staining intensity of NF-kappa B protein family member RelA/p65, which regulates cell growth, apoptosis, and angiogensis, has been connected to BCR. In this study we combine nuclear NF-kappa B/p65 and H&E gland morphology features to structurally and functionally characterize prostate cancer. This enables description of cancer phenotypes according to cellular molecular profile and social behavior. We collected radical prostatectomy specimens from 21 patients, 7 of whom experienced BCR (prostate specific antigen > .2 ng/ml) within two years of surgery. Our goal was to demonstrate the value of combining morphological and functional information for BCR prediction. Firstly, we used the top two features from each stain channel via the Wilcoxon rank-sum test using a leave-one-out cross validation approach in conjunction with a linear discriminant analysis classifier. Secondly we used the product of the posterior class probabilities from each classifier to produce an aggregate classifier. Accuracy was 0.76 with H&E features alone, 0.71 with NF-kappa B/p65 features alone, and 0.81 via the aggregate model.",10.1117/12.2292652,2018,0,0.0,0.0,0.0,nw,0.0,1,0
247,Characterization of Precipitation through Copulas and Expert Judgement for Risk Assessment of Infrastructure,"In this paper two methodologies are investigated that contribute to better assessment of risks related to extreme rainfall events. Firstly, one-parameter bivariate copulas are used to analyze rain gauge data in the Netherlands. Out of three models considered, the Gumbel copula, which indicates upper tail dependence, represents the data most accurately for all 33 stations in the Netherlands. Seasonal variability is noticeable, with rank correlation reaching maximum in winter and minimum in summer as well as other temporal and spatial patterns. Secondly, an expert judgment elicitation was undertaken. The experts' opinions were combined using Cooke's classical method in order to obtain estimates of future changes in precipitation patterns. Experts predicted mostly an approximate 10% increase in rain amount, duration, intensity and the dependence between amount and duration. The results were in line with official national climate change scenarios, based on numerical modelling. Applicability of both methods was presented based on an example of an existing tunnel in the Netherlands, contributing to better estimates of the tunnel's limit state function and therefore the probability of failure. (c) 2017 American Society of Civil Engineers.",10.1061/AJRUA6.0000914,2017,1,0.0,0.0,1.0,nw,1.0,1,1
248,Source code size prediction using use case metrics: an empirical comparison with use case points,"Software source code size, in terms of source lines of code (SLOC), is an important parameter of many parametric software development effort estimation methods. In this paper, we investigate empirically the early prediction of SLOC for object-oriented software using use case metrics. We used different modeling techniques to build the prediction models. We used the univariate logistic regression and the simple linear regression methods to evaluate the individual effect of each use case metric on SLOC, and the multivariate logistic regression and the multiple linear regression methods to explore the combined effect of the use case metrics on SLOC. We also used in the study different machine learning methods (k-NN, na < ve Bayes, C4.5, random forest, and multilayer perceptron neural network). The prediction models were evaluated using the receiver operating characteristic analysis, particularly the area under the curve measure, and leave-one-out cross validation. An empirical study, using data collected from five open source Java projects, is reported in the paper. The use case metrics have been compared to the well-known use case points method. Results provide evidence that the use case metrics-based approach gives a more accurate prediction of SLOC than the use case points-based approach.",10.1007/s11334-016-0285-7,2017,0,0.0,0.0,0.0,gcg,0.0,1,0
249,A balanced approach to central counterparty margining,"This paper is meant to serve as a comparison of the approaches and margin models employed by central clearing counterparties (CCPs). We look at two different approaches: fully automated margin models and margin models that incorporate expert judgment. The latter type of model utilizes value-at-risk (VaR) methodologies, and the outputs of these VaR methodologies are subject to manual review before the final parameters are determined. The former type of model does not involve manual review or any alteration of the parameters. While these approaches may differ greatly, there are components of each that are critical to prudent margin setting. This paper concludes that a combination of data-driven models and applied expert judgment most appropriately captures the risks in centrally cleared markets, emphasizing the importance of expert judgment in margin setting. It is important to outline the goals of initial margin as a risk management tool before selecting any approach to margining. The foremost goal of initial margin is to cover potential future exposures, whether on a product-or portfolio-level basis. This is typically demonstrated through the coverage of losses achieving, at a minimum, the common 99% standard set out in the Principles for Financial Market Infrastructures (PFMIs) by the Committee on Payments and Market Infrastructures (CPMI) and International Organization of Securities Commissions (IOSCO) in April 2012. In addition to achieving coverage, the objective of a margining framework is predictability and stability.",10.21314/JFMI.2017.080,2017,0,0.0,0.0,0.0,gcg,0.0,1,0
250,Deriving the probability of a linear opinion pooling method being superior to a set of alternatives,"Linear opinion pools are a common method for combining a set of distinct opinions into a single succinct opinion, often to be used in a decision making task. In this paper we consider a method, termed the Plug-in approach, for determining the weights to be assigned in this linear pool, in a manner that can be deemed as rational in some sense, while incorporating multiple forms of learning over time into its process. The environment that we consider is one in which every source in the pool is herself a decision maker (DM), in contrast to the more common setting in which expert judgments are amalgamated for use by a single DM. We discuss a simulation study that was conducted to show the merits of our technique, and demonstrate how theoretical probabilistic arguments can be used to exactly quantify the probability of this technique being superior (in terms of a probability density metric) to a set of alternatives. Illustrations are given of simulated proportions converging to these true probabilities in a range of commonly used distributional cases.",10.1016/j.ress.2016.10.008,2017,1,0.0,0.0,0.0,gcg,0.0,0,0
251,Doctoral Advisor or Medical Condition: Towards Entity-Specific Rankings of Knowledge Base Properties,"In knowledge bases such as Wikidata, it is possible to assert a large set of properties for entities, ranging from generic ones such as name and place of birth to highly profession-specific or background-specific ones such as doctoral advisor or medical condition. Determining a preference or ranking in this large set is a challenge in tasks such as prioritisation of edits or natural-language generation. Most previous approaches to ranking knowledge base properties are purely data-driven, that is, as we show, mistake frequency for interestingness. In this work, we have developed a human-annotated dataset of 350 preference judgments among pairs of knowledge base properties for fixed entities. From this set, we isolate a subset of pairs for which humans show a high level of agreement (87.5% on average). We show, however, that baseline and state-of-the-art techniques achieve only 61.3% precision in predicting human preferences for this subset. We then develop a technique based on a combination of general frequency, applicability to similar entities and semantic similarity that achieves 74% precision. The preference dataset is available at https://www. kaggle. com/srazniewski/wikidatapropertyranking.",10.1007/978-3-319-69179-4_37,2017,0,0.0,0.0,0.0,ngr,0.0,1,0
252,A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering,"While deep convolutional neural networks frequently approach or exceed human-level performance in benchmark tasks involving static images, extending this success to moving images is not straightforward. Video understanding is of interest for many applications, including content recommendation, prediction, summarization, event/object detection, and understanding human visual perception. However, many domains lack sufficient data to explore and perfect video models. In order to address the need for a simple, quantitative benchmark for developing and understanding video, we present MovieFIB, a fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired. In addition to presenting statistics and a description of the dataset, we perform a detailed analysis of 5 different models' predictions, and compare these with human performance. We investigate the relative importance of language, static (2D) visual features, and moving (3D) visual features; the effects of increasing dataset size, the number of frames sampled; and of vocabulary size. We illustrate that: this task is not solvable by a language model alone; our model combining 2D and 3D visual information indeed provides the best result; all models perform significantly worse than human-level. We provide human evaluation for responses given by different models and find that accuracy on the MovieFIB evaluation corresponds well with human judgment. We suggest avenues for improving video models, and hope that the MovieFIB challenge can be useful for measuring and encouraging progress in this very interesting field.",10.1109/CVPR.2017.778,2017,0,0.0,0.0,0.0,nw,0.0,1,0
253,Bayesian network models for making maintenance decisions from data and expert judgment,"To maximize asset reliability cost-effectively, maintenance should be scheduled based on the likely deterioration of an asset. A number of types of statistical model have been proposed for predicting this but they have important practical limitations. We present a Bayesian network model that can be used for maintenance decision support to overcome these limitations. The model extends an existing statistical model of asset deterioration, but shows how i) failure data from related groups of asset can be combined, ii) data on the condition of assets available from their periodic inspection can be used iii) expert knowledge of the causes deterioration can be combined with statistical data to adjust predictions and iv) the uncertain effects of maintenance actions can be modelled. We show how the model could be used for a range of decision problems, given typical data likely to be available in practice.",0,2017,0,0.0,1.0,0.0,ngr,1.0,0,0
254,COAL-FIRED POWER IN THE NEW INDUSTRIAL DEVELOPMENT,"The role of coal as the driver of innovative electricity generation for the new industrial development is complicated. On the world industrial agenda, coal continues to dominate global power generation due to its nature - significant reserves and well-distributed deposits, stability of production, flexibility of delivery, comparatively easily predicted prices and supply. A significant drawback of coal generation that is connected with the emission of toxic particulars (CO2, sulphur dioxide, nitrogen oxide, etc.) can be eliminated by new technologies, i.e. ultra-supercritical, integrated gasification combined cycle, carbon capture and sequestration. Technological inertia and capital-intensive innovation processes determine the power generation as industry with medium-low R&D expenditures, which are roughly 1-2% of the net sales of power companies. World strategic innovators in power industry build up the technological trends of coal generation, however, recent public and private spending on R&D represents only a small fraction of capital, needed for wide scaled deployment of new coal-fired power generation technologies. Plus to that, the significant cash flow from international commercial banks has been directed to finance new coal-fired plants based on traditional old-fashioned technologies with low efficiency and weak environmental standards. Most governments in the world support traditional fossil fuels suppliers by means of energy subsidies. Emerging financial initiatives green bonds, new financial metric - are aimed at redirecting cash flow towards innovative technologies in power industry. But up to the moment, the amount of money reallocation is small and the pace of training high quality engineers and scientists is slack. Nevertheless, the intensified international cooperation between the largest coal users such as the USA and China makes quite possible the optimistic scenario for the future of a new coal generation. Education, finance and technology are the main factors of strengthening bilateral or multilateral cooperation. Russia has obtained up to the moment, by experts' judgments, well-educated engineers, valuable traditions of scientific schools and research practice in the field of coal power. That bestows Russia good premises for making up international technological consortium with other, rich in coal countries, first of all, China and India.",10.20542/0131-2227-2016-60-6-42-51,2016,0,0.0,0.0,0.0,ngr,0.0,1,0
255,Hybrid Segmentation Strategy and Multi-Agent SVMs for Corporate Risk Management in Class Imbalanced Situations,"This study introduced an emerging architecture with a segmentation strategy for the classification of highly imbalanced datasets. The segmentation strategy was specifically performed by K-means, which divided the majority class into some less imbalanced datasets and yielded more robust training data. Superior forecasting performance of the ensemble mechanism/multi-agent mechanism came with a critical drawback, which was that it lacked interpretability. The study further dealt with the obscure nature of the ensemble mechanism by LEM2 algorithm. The human-readable rules could be taken as a guideline for decision makers to make a suitable judgment in a highly competitive financial environment.",10.1520/JTE20140267,2016,0,0.0,0.0,0.0,ngr,0.0,1,0
256,Improving Forecasting Performance by Exploiting Expert Knowledge: Evidence from Guangzhou Port,"Expert knowledge has been proved by substantial studies to be contributory to higher forecasting performance; meanwhile, its application is criticized and opposed by some groups for biases and inconsistency inherent in experts' subjective judgment. This paper proposes a new approach to improving forecasting performance, which takes advantage of expert knowledge by constructing a constraint equation rather than directly adjusting the predicted values by experts. For the comparison purpose, the proposed approach, together with several widely used models including ARIMA, BP-ANN and the judgment model (JM), is applied to forecasting the container throughput of Guangzhou Port, which is one of the most important ports of China. Forecasting performances of the above models are compared and the results clearly show superiority of the proposed approach over its rivals, which implies that expert knowledge will make positive contribution as long as it is used in a right way.",10.1142/S0219622016500085,2016,1,0.0,1.0,0.0,ngr,1.0,1,1
257,Model Student Selection Using Fuzzy Logic Reasoning Approach,"In an educational institution, various students criteria contributed to the main reason the student is nominated as a model student. This includes cumulative grade point average (CGPA) of academic courses taken, co-curriculum involvement, soft skills, hard work, leadership, attitude, time management, attendance, attire, and technical skill in order to make the selection decision. The distinctive student is identified firstly based on their overall score. The problem arises when one needs to evaluate all these different criteria in order to select or nominated a model student. It is a tedious task and time consuming for examiner or reviewer to analyze students individually in order to select a student that showed desirable performance. In addition, as a human has its limitation, there is a high degree of error in judgment in decision making. With the aim of bridging this gap, fuzzy logic is applied to imitate human analytic thinking or the ability to make a decision in the model student selection process. Thus, the system is able to assist examiner or evaluator in the decision-making process of determining the model student with minimal time and error. Furthermore, the user could dynamically change the criteria based on the traits that have been set by the system. Fuzzy logic is chosen as it is a powerful mathematical tool involving ambiguous and incomplete information supported by the experience and judgment of examiner or reviewer while combining this cognitive ability in order to reach a verdict. Therefore, the outputs from the experiment conducted shows that the system provides robust output and information to the user regarding student's aptitude and ultimately aim to effectively predict human judgment.",0,2016,0,0.0,0.0,0.0,gcg,0.0,1,0
258,Uncertainty and Expert Assessment for Supporting Evaluation of Levees Safety,"In France, levees remain most of the time badly maintained; these long linear structures show signs of weaknesses on numerous occasions. Only incomplete information is usually available. The general lack of data describing the behavior of the infrastructure during unwanted events led to estimate their safety mainly from expert judgment. Thus the ability of the expert to predict the level of functioning of an infrastructure for a type of hazard and its intensity is crucial. An error of judgment can have very serious consequences and the production of reliable information requires the ability of the expert to report accurately the uncertainties in its estimations, as well as associated confidence. In order to meet this need, our research within Incertu project (French Ministry of Ecology funding) aims to produce relevant scientific approaches and tools for the collection and processing reliable experts' statements or combined with a confidence level in the context of uncertain information and input data.",10.1051/e3sconf/20160703019,2016,1,0.0,0.0,0.0,gcg,0.0,0,0
259,Combining Reference Class Forecasting with Overconfidence Theory for Better Risk Assessment of Transport Infrastructure Investments,"Assessing the risks of infrastructure investments has become a topic of growing importance. This is due to a sad record of implemented projects with cost overruns and demand shortfalls leading, in retrospect, to the finding that there is a need for better risk assessment of transport infrastructure investments. In the last decade progress has been made by dealing with this situation known as planners' optimism bias. Especially attention can be drawn to the use of reference class forecasting that has led to adjustment factors that, when used on the estimates of costs and demand, lead to cost-benefit analysis results that are modified by taking historical risk experience into account. This article seeks to add to this progress in risk assessment methodology in two ways: first it suggests to apply reference class forecasting (RCF) in a flexible way where the effort is focused on formulating the best possible reference pool of projects and second to apply overconfidence theory (OT) to interpret expert judgments (EJ) about costs and demand as relating to a specific project up for examination. By combining flexible use of RCF with EJ based on OT interpretation it is argued that the current adjustment factor methodology of RCF can be further developed. The latter is among other things made possible by the comprehensive project databases that have been developed in recent years. For this article the project database developed in the UNITE research project 2009-2013 has been employed. The presented simulation-based risk examination named SIMRISK is concluded to provide a new 'in-depth' possibility for dealing with uncertainties inherent to transport decision making based on socioeconomic analysis. In addition a further research perspective is outlined.",0,2015,0,0.0,0.0,0.0,ngr,0.0,1,0
260,Fully Automated Facial Picture Evaluation Using High Level Attributes,"People automatically and quickly judge a facial picture from its appearance. Thus, developing tools that can reproduce human judgments may help consumers in their picture selection process. Previous work mostly studied the position of facial keypoints to make predictions about specific traits: trustworthiness, likability, competence, etc. In this work, high level attributes (e.g. gender, age, smile) are automatically extracted using 3 different tools and are used to build models adapted to each trait. Models are validated on a set of synthetic images and it is shown that using attributes increases significantly the correlation between human and algorithmic evaluations. Then, a new dataset of 140 images is presented and used to demonstrate the relevance of high level attributes for evaluating faces with respect to likability and competence. A model combining both facial keypoints and attributes is finally proposed and applied to picture selection: which picture depicts the most likable face for a given person?",0,2015,0,0.0,0.0,0.0,ngr,0.0,1,0
261,Fully Automated Facial Picture Evaluation Using High Level Attributes,"People automatically and quickly judge a facial picture from its appearance. Thus, developing tools that can reproduce human judgments may help consumers in their picture selection process. Previous work mostly studied the position of facial keypoints to make predictions about specific traits: trustworthiness, likability, competence, etc. In this work, high level attributes (e.g. gender, age, smile) are automatically extracted using 3 different tools and are used to build models adapted to each trait. Models are validated on a set of synthetic images and it is shown that using attributes increases significantly the correlation between human and algorithmic evaluations. Then, a new dataset of 140 images is presented and used to demonstrate the relevance of high level attributes for evaluating faces with respect to likability and competence. A model combining both facial keypoints and attributes is finally proposed and applied to picture selection: which picture depicts the most likable face for a given person?",0,2015,0,0.0,0.0,0.0,nw,0.0,1,0
262,Fully Automated Facial Picture Evaluation Using High Level Attributes,"People automatically and quickly judge a facial picture from its appearance. Thus, developing tools that can reproduce human judgments may help consumers in their picture selection process. Previous work mostly studied the position of facial keypoints to make predictions about specific traits: trustworthiness, likability, competence, etc. In this work, high level attributes (e.g. gender, age, smile) are automatically extracted using 3 different tools and are used to build models adapted to each trait. Models are validated on a set of synthetic images and it is shown that using attributes increases significantly the correlation between human and algorithmic evaluations. Then, a new dataset of 140 images is presented and used to demonstrate the relevance of high level attributes for evaluating faces with respect to likability and competence. A model combining both facial keypoints and attributes is finally proposed and applied to picture selection: which picture depicts the most likable face for a given person?",0,2015,0,0.0,0.0,0.0,nw,0.0,1,0
263,Fully Automated Facial Picture Evaluation Using High Level Attributes,"People automatically and quickly judge a facial picture from its appearance. Thus, developing tools that can reproduce human judgments may help consumers in their picture selection process. Previous work mostly studied the position of facial keypoints to make predictions about specific traits: trustworthiness, likability, competence, etc. In this work, high level attributes (e.g. gender, age, smile) are automatically extracted using 3 different tools and are used to build models adapted to each trait. Models are validated on a set of synthetic images and it is shown that using attributes increases significantly the correlation between human and algorithmic evaluations. Then, a new dataset of 140 images is presented and used to demonstrate the relevance of high level attributes for evaluating faces with respect to likability and competence. A model combining both facial keypoints and attributes is finally proposed and applied to picture selection: which picture depicts the most likable face for a given person?",0,2015,0,0.0,0.0,0.0,nw,0.0,1,0
264,Fully Automated Facial Picture Evaluation Using High Level Attributes,"People automatically and quickly judge a facial picture from its appearance. Thus, developing tools that can reproduce human judgments may help consumers in their picture selection process. Previous work mostly studied the position of facial keypoints to make predictions about specific traits: trustworthiness, likability, competence, etc. In this work, high level attributes (e.g. gender, age, smile) are automatically extracted using 3 different tools and are used to build models adapted to each trait. Models are validated on a set of synthetic images and it is shown that using attributes increases significantly the correlation between human and algorithmic evaluations. Then, a new dataset of 140 images is presented and used to demonstrate the relevance of high level attributes for evaluating faces with respect to likability and competence. A model combining both facial keypoints and attributes is finally proposed and applied to picture selection: which picture depicts the most likable face for a given person?",0,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
265,Historical heritage factor in evaluating development prospects of the regional multicultural community,"The investigation proposes a number of approaches to assessment of the factors of historical heritage in terms of their impact on quality of living and socio-economic development of a multicultural region. The monitoring variant and the complex of methods were substantiated to give predictive assessment of the prospects of socio-economic development of a region in the analysis of historical retrospective of its development. They are based on the analysis of the category of quality of life and represent the combination of logical modeling methods (including the construction of forecast scenarios), mathematical-statistical and expert judgments. In particular, the authors propose a modified index statistical method of monitoring and forecasting of the dynamics of the quality of life of a region, taking into account a social differentiation of its territory. The authors have elaborated the quality of life (dynamics) change indicators, considering a historical retrospective. These indicators form the basis for the identification and analysis of regional development scenario variants. The paper also presents an adaptable method of expert judgments based on a questionnaire for social and economic forecasting. The Republic of Tatarstan, being one of the most dynamic multicultural regions of Russia with rich historical and cultural heritage was selected as a base region. The paper presents a number of forecast scenarios of development of the region, based on the differentiation of the quality of life of regional community, and takes into account local areas of compact settlement of various ethnic groups. The comparative evaluation of these scenarios is provided. (C) 2015 The Authors. Published by Elsevier Ltd.",10.1016/j.sbspro.2015.03.368,2015,0,0.0,0.0,0.0,nw,0.0,1,0
266,Forecasting container throughput with big data using a partially combined framework,"This study proposes a partially-combined forecasting framework for container throughput based on big data composed of structured historical data and unstructured data. Under the proposed framework, the structured data (the original time series) is firstly decomposed into linear component and nonlinear component. Seasonal auto-regression integrated moving average model (SARIMA) is adopted to capture and forecast the linear component, and a combined model, composed of least squares support vector regression (LSSVR) and artificial neural network (GP), is applied to modeling the nonlinear component. Next, unstructured data is analyzed by an expert system. With the synthesized expert judgment, the forecasts of linear and nonlinear components are integrated into a final forecast. For the illustration and verification purpose, an empirical study is conducted with the data of Qingdao Port. The results show that the model under the proposed framework significantly outperforms its competitive rivals.",0,2015,0,0.0,0.0,1.0,nw,1.0,0,0
267,Approximator: Predicting Interruptibility in Software Development with Commodity Computers,"Assessing the presence and availability of a remote colleague is key in coordination in global software development but is not easily done using existing computer-mediated channels. Previous research has shown that automated estimation of interruptibility is feasible and can achieve a precision closer to, or even better than, human judgment. However, existing approaches to assess interruptibility have been designed to rely on external sensors. In this paper, we present Approximator, a system that estimates the interruptibility of a user based exclusively on the sensing ability of commodity laptops. Experimental results show that the information aggregated from several activity monitors (i.e., key-logger, mouse-logger, and face-detection) provide useful data, which, once combined with machine learning techniques, can automatically estimate the interruptibility of users with a 78% accuracy. These early but promising results represent a starting point for designing tools with support for interruptibility capable of improving distributed awareness and cooperation to be used in global software development.",10.1109/ICGSE.2015.16,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
268,"TRACE: A Dynamic Model of Trust for People-Driven Service Engagements Combining Trust with Risk, Commitments, and Emotions","Trust is an important element of achieving secure collaboration that deals with human judgment and decision making. We consider trust as it arises in and influences people-driven service engagements. Existing approaches for estimating trust between people suffer from two important limitations. One, they consider only commitment as the primary means of estimating trust and omit additional significant factors, especially risk and emotions. Two, they typically estimate trust based either on fixed parameter models that require manual setting of parameters or based on Hidden Markov Models (HMM), which assume conditional independence and are thus ill-suited to capturing complex relationships between trust, risk, commitments, and emotions. We propose TRACE, a model based on Conditional Random Fields (CRF) that predicts trust from risk, commitments, and emotions. TRACE does not require manual parameter tuning and relaxes conditional independence assumptions among input variables. We evaluate TRACE on a dataset collected by the Intelligence Advanced Research Projects Activity (IARPA) in a human-subject study. We find that TRACE outperforms existing trust-estimation approaches and that incorporating risk, commitments, and emotions yields lower trust prediction error than incorporating commitments alone.",10.1007/978-3-662-48616-0_24,2015,0,0.0,0.0,0.0,nw,0.0,1,0
269,Full-Reference Predictive Modeling of Subjective Image Quality Assessment with ANFIS,"Digital images often undergo through various processing and distortions which subsequently impacts the perceived image quality. Predicting image quality can be a crucial step to tune certain parameters for designing more effective acquisition, transmission, and storage multimedia systems. With the huge number of images captured and exchanged everyday, automatic prediction of image quality that correlates well with human judgment is steadily gaining increased importance. In this paper, we investigate the performance of three combinations of objective metrics for image quality prediction with an adaptive neuro-fuzzy inference system (ANFIS). Images are processed to extract various attributes which are then used to build a predictive model to estimate a differential mean opinion score for different types of distortions. Using a publicly available and subjectively rated image database, the proposed method is evaluated and compared to individual metrics and an existing technique based on correlation and error measures. The results prove that the proposed method can be a promising approach for predicting subjective quality of images.",10.1007/978-3-319-25210-0_18,2015,0,0.0,0.0,0.0,nw,0.0,1,0
270,A conversational approach to social media mining: the analysis of early reactions in Twitter to the launches of new products,"Purpose - We analyze streams of microblogs in terms of both contents and distribution network to create a model of people behavior to be used as an indication for future similar events. We present our methodology and some preliminary results of its application to new movie launches through the analysis of about 2 million downloaded in the US in 72 hours around the events in a 4 months period. Design/methodology/approach - We use a combination of semantic and topological analyses that demonstrated good results identifying conversational patterns to be used not only to assess how people feel about the event but also to develop a better understanding about what people say and how people talk about the event. To obtain the what people say, the methodology and the tools automatically analyze the conversations, with the support of visualization components. The how people talk is addressed detecting patterns in the conversations and evaluating the correlation of those patterns with performance indicators. Structure and contents are analyzed through a set of semantic and topological metrics to assess the content generated in different conversations. Metrics are then collected in a single dataset, which is later used to train and test a data mining model. Originality/value - Several commercial platforms are available to retrieve and assess to some extent this collective judgment through sentiment analysis. Sentiment, however, just measures how the ""crowd"" feels about a product, but does not offer insight on the structure and the determinants of the customers' preferences nor provides indications about future behavior. Our challenge is instead to dig deeper into Twitter streams to capture and assess structured contents that are embedded in them through an analytical and quantitative approach. Practical implications - The elicitation of organized content from Twitter streams could support market analysts to achieve a better understanding of consumers' perceptions and to better manage the reach-richness trade off between qualitative and quantitative market analyses. An additional practical use of our study can be to find a way to ascertain the presence of given semantic patterns in Twitter streams that can be predictive of early market success for a new product.",0,2015,0,0.0,0.0,0.0,gcg,0.0,1,0
271,Combinatorial Prediction Markets for Fusing Information from Distributed Experts and Models,"Markets are a medium for information exchange between buyers and sellers. Prediction markets exploit the information transmission property of markets to improve forecasts of future events. Participants in a prediction market buy and sell assets that pay off if the underlying event occurs. Prices in a prediction market can be interpreted as consensus probabilities for the underlying events. Prediction markets thus provide a promising method to fuse information from a collection of human forecasters and/or computer forecasting algorithms. This paper describes the use of prediction markets as an information fusion mechanism, introduces combinatorial prediction markets as a mechanism for expressing and exploiting dependencies among base events, presents algorithms for performing basic market computations in combinatorial prediction markets, and introduces the SciCast combinatorial prediction market, a combinatorial prediction market for science and technology forecasting available at http://scicast.org.",0,2015,1,0.0,0.0,1.0,nw,1.0,1,1
272,CORRECTION OF PREDICTION MODEL OUTPUT-APPLICATION TO GENERAL CORROSION MODEL,This paper applies a newly developed methodology to calibrate the corrosion model within a structural reliability analysis. The methodology combines data from experience (measurements and expert judgment) and prediction models to adjust the structural reliability models. Two corrosion models published in the literature have been used to demonstrate the technique used for the model calibration. One model is used as a prediction for a future degradation and a second one to represent the inspection recorded data. The results of the calibration process are presented and discussed.,10.3940/rina.ijme.2014.a4.310,2014,0,0.0,0.0,1.0,nw,1.0,0,0
273,Model-Based Consensus,"The aim of the rational-consensus method is to produce ""rational consensus"", that is, ""mathematical aggregation"", by weighing the performance of each expert on the basis of his or her knowledge and ability to judge relevant uncertainties. The measurement of the performance of the experts is based on the expert's assessment of ""seed variables"". These performances are used to determine the weights of the expert's judgments in the aggregation of them. The disadvantage of the rational-consensus method in social science is the lack of agreed upon seed variables, and that it does not instead use the shared knowledge captured by models. Moreover, there seems to be sufficient evidence that combining models with expert judgments leads to better judgments. This is even more evident with respect to forecasts.",10.1007/978-3-319-08551-7_3,2014,1,1.0,0.0,0.0,gcg,1.0,1,1
274,A study on the space distribution and temporal evolvement of electric power load,"Spatial load forecasting hinges on choosing suitable load density. Because of load density is influenced by many uncertain factors, taking various objective factors into account on the basis of expert experience judgment will be a great help to improve the precision of forecasting load density. The selection of load density bases on a large number of samples. This paper establishes cloud generator to extend the data of finite samples. This paper selects the influence factors which play an important role in the load density and introduces cloud theory and analytic hierarchy process to analyze the combination weight vector of various factors. Use the ideal approximation method to select the optimal load density based on the expanded sample data and modify the load density based on the similarity method. Inputting the values of influencing factors in a given year of the area to be predicted, it can be worked out that the load density of the area in the given year. This method can not only predict the spatial distribution of the electric power load, but also predict the evolution process of the spatial load. An example demonstrates the superiority of the proposed model in spatial load forecasting.",0,2011,0,0.0,0.0,1.0,nw,1.0,0,0
275,Optimization of bridge maintenance actions considering combination of sources of information: Inspections and expert judgment,"The use of advanced probabilistic life-cycle deterioration models is fundamental in bridge maintenance and management. However, such models must be based on direct and indirect information on structural performance, including results of inspections, health monitoring, but also expert judgment and information on other similar structures. A probabilistic deterioration model, considering both condition and safety as indicators of performance, is used as a decision aid in bridge management. The data considered for defining the performance profiles over time is based on the expert opinion, resulting from observation of similar structures. In order to reduce uncertainty and correct the existing predictions, the results of direct information, in the form of visuals inspections, are combined with the initial prediction. The combination of these two sources of information is performed considering Bayesian updating techniques. Based on this new information, it is possible, considering multi-objective optimization using genetic algorithms, to define the optimal lifetime maintenance strategy for a structure. The methodology is applied to a set of bridge components. The results show the significant improvement in prediction quality obtained by combining different sources of information, even if the quality of the obtained data is limited. Moreover, the impact of the new information on the optimal maintenance strategies is quantified.",0,2010,0,0.0,0.0,1.0,nw,1.0,0,0
276,A Review of the Application of Analytic Hierarchy Process to the Planning and Operation of Electric Power Microgrids,"This paper reviews literature that identifies the need for decision-making associated with the design and operation of electric power grids including microgrids. In particular, it examines the current applications of analytic hierarchy process (AHP) as the decision-making tool for electric power grids and microgrids. AHP is a decision making tool based on expert judgments that has been successfully applied in design and operation for the power systems. One advantage of AHP is its ability to incorporate subjective constraints. The current application of A]HP to grid connected power systems includes selection of generating units, identifying protection system vulnerability, prioritizing line maintenance, determining DC deployment and configurations, analyzing energy planning under uncertainty, locating and sizing of VAR sources, value based budgeting, choosing dispatch scenarios, forecasting loads, integrated resource planning, and determining combined active and reactive dispatch AHP has been applied to islanded microgrids for load shedding and optimizing cost. This paper concludes with proposals for expanding the use of AHP for decision-making for islanded microgrids.",0,2008,0,0.0,0.0,1.0,nw,1.0,0,0
277,Probabilistic Inversion Techniques in Quantitative Risk Assessment for Power System Load Forecasting,"Expert judgment is frequently used to assess parameter values in quantitative risk assessment. Experts can however only be expected to assess observable quantities, not abstract model parameters. This means that we need a method for translating expert assessed uncertainties on model outputs into uncertainties on model parameter values. So we use Probabilistic Inversion (PI) method. The probability distribution on model parameters obtained in this way can be used in a variety of ways, but in particular in an uncertainty analysis or as a Bayes prior. In this paper probabilistic inversion problems are first defined, existing algorithms for solving such problems are also discussed and the algorithms based on iterative algorithms are introduced. Those computational algorithms have proven successful in various projects. Such techniques are indicated when we wish to quantify a model which is new and perhaps unfamiliar to the expert community. There are no measurements for estimating model parameters, and experts are typically unable to give a considered judgment. In such cases, experts are asked to quantify their uncertainty regarding variables which can be predicted by the model. Applications to power system load forecasting in NingXia province of China is discussed. This study illustrates two such techniques, Iterative Proportional Fitting (IPF) and PARmeter Fitting for Uncertain Models (PARFUM) which provide useful tools for the practicing quantities risk assessment. In addition, we also illustrate how expert judgment on predicted observable quantities in combination with probabilistic inversion may be used for model validation.",10.1109/ICINFA.2008.4608092,2008,0,0.0,1.0,0.0,ngr,1.0,0,0
278,Aggregating forecasts to obtain fuzzy demands,"There are several studies in the literature that assumes fuzzy demands in supply chain or production planning models but most of them do not mention about how to derive the fuzzy demands from statistical and judgmental forecasts. In this study we propose a methodology to aggregate the forecasts coming based on different sources; namely statistical methods as well as the experts judgments, and to obtain an aggregated demand forecast that is represented by a possibilistic distribution. Results of the statistical and judgmental forecasts are represented by triangular possibilistic distributions. Subsequently, those results are combined by using weights of each input forecast. An illustrative example is also provided.",10.1142/9789812799470_0012,2008,1,0.0,0.0,1.0,nw,1.0,1,1
279,Data Mining Techniques for Complex Formation Evaluation in Petroleum Exploration and Production: A Comparison of Feature Selection and Classification Methods,"Data mining techniques, especially classification methods, are receiving increasing attention from researchers and practitioners in the domain of petroleum exploration and production (E&P) in China. To extensively investigate the effects of feature selection and learning algorithms on the hydrocarbon reservoir prediction performance, taking three real-world multiclass problems as examples, namely formation evaluation of water-flooding interval, low resistivity reservoir, and gas zone from Chinese oil fields, this paper presents a comprehensive comparative study of both five feature selection methods including expert judgment, CFS, LVF, Relief-F, and SVM-RFE, and fourteen algorithms from five distinct kinds of classification methods including decision tree, artificial neural network, support vector machines(SVM), Bayesian network and ensemble learning. The results show that Relief-F and SVM-RFE can improve prediction performances more effectively than other methods, as well as C-SVC is the best classifier with generalization accuracy of 79.48%, 80.99%, and 75.01% respectively. Our studies suggest that the choice of classification methods should be more important than that of feature selection algorithms and the combination of SVMs and feature ranking should be preferred to other approaches for the complex reservoir evaluation using well logs.",0,2008,0,0.0,0.0,0.0,gcg,0.0,1,0
280,Reliability of vibration predictions in civil engineering applications,"The reliability of vibration predictions distinguishes itself from other reliability problems because of the highly non-linear behavior of the underlying models. Over the last two years, a combination of four institutes in the Netherlands has studied the reliability in this type of predictions. For the sake of comparison, besides sophisticated computational prediction models, also simple empirical models and expert judgment was analyzed. The paper describes the experimental set-up and the results of the project. Conclusions are drawn about the reliability of the predictions and the reduction that may be achieved from an increase in model sophistication.",0,2003,0,0.0,1.0,0.0,ngr,1.0,0,0
281,"Assessing processes in uncertain, complex physical phenomena and manufacturing","PREDICT (Performance and Reliability Evaluation with Diverse Information Combination and Tracking) is a set of structured quantitative approaches for the evaluation of system performance based on multiple information sources. The methodology integrates diverse types and sources of information, and their associated uncertainties, to develop full distributions for performance metrics, such as reliability. The successful application of PREDICT has involved system performance assessment in automotive product development, aging nuclear weapons, and fatigued turbine jet engines. In each of these applications, complex physical, mechanical and materials processes affect performance, safety and reliability assessments. Processes also include the physical actions taken during manufacturing, quality control, inspections, assembly, etc. and the steps involved in product design, development and certification. In this paper, we will examine the various types of processes involved in the decision making leading to production in an automotive system reliability example. Analysis of these processes includes not only understanding their impact on performance and reliability, but also the uncertainties associated with them. The automotive example demonstrates some of the tools used in tackling the complex problem of understanding processes.: While some tools and methods exist for understanding processes (man made and natural) and the uncertainties associated with them, many of the complex issues discussed are open for continued research efforts.",0,2002,0,0.0,0.0,0.0,nw,0.0,1,0
282,Organizing new methods for erosion and sedimentation monitoring and control,"Historic and continuing land uses at the U.S. Army's Fort Carson and the Pinon Canyon Maneuver Site (PCMS) may degrade training lands and cause erosion and increased sediment loading of local waters. A team of experts from the U.S. Geological Survey, the USDA Agricultural Research Service's Southwest Watershed Research Center, and two offices of the USDA Natural Resources Conservation Service is identifying methods and projects that can be used by Army land managers to evaluate the Army's potential contribution to the regional sediment pollution problem. Projects include site reclamation to control erosion and sediment transport; monitoring of stream flow, climate, and sediment concentrations; prediction of soil erosion; and assessment of rangeland health. Although the team has coordinated several agency projects and improved the overall approach to erosion and sedimentation monitoring and control, new research and technology will ensure additional progress in the future. This will include quantifying the impacts of military land use and training activities and maintaining this information in databases to enable the development of measurement techniques and indicators of ecosystem status, change and damage. Integrated information systems are required to combine the power of modern databases, simulation modeling, and expert judgment in designing and evaluating erosion control measures in terms of erosion and sediment transport processes. Because environmental concerns do not conform to political or administrative boundaries, the Army/ARS/USGS/NRCS team shares ideas and resources to provide both the Army and the regional community with responsible environmental management. Fort Carson's multidisciplinary team adopts the challenge of change, demonstrates new directions in managing highly complex challenges, and validates the federal government's effectiveness and ability to be efficient.",0,2000,0,0.0,0.0,1.0,nw,1.0,0,0
283,Multi-hierarchical durability assessment of existing reinforced-concrete structures,"Durability of an existing reinforced concrete structure is one of the most important problems for its users and owners. However, no systematic durability assessment approach exists currently. The assessment results vary in relation to the knowledge, expertise, and partiality of the assessor. Additionally, most assessment may not focus on all the key system parameters that affect structural durability. This paper aims at establishing a systematic durability assessment approach for R.C. Structures. The development of such an approach would help to identify all major parameters that affect structural durability and in addition, would enable assessors to collect both qualitative as well as quantitative information in a standard format for any given reinforced concrete structure. Furthermore, such a systematic assessment approach would provide a fair and objective way for analyzing and dealing with data collected by field inspection. Firstly, by Saaty's ratio scaling method, which scientifically quantifies the subjective information of expert judgments. Secondly, on basis of the properties of entropy, which combines the subjective information of expert judgments with the intrinsic information of assessment parameters. Then, taking gray relation grade as criterion, which obtains the deterioration degree and deterioration rate for a given structure. Finally, based on the assumption that deterioration varies with time exponentially, it gives the present performance and residual service life for any existing reinforced concrete structure.",0,1999,0,0.0,0.0,1.0,nw,1.0,0,0
284,An expert system to evaluate TCE sites for natural attenuation,An expert judgment site-screening tool has been developed to evaluate natural attenuation as a remedial option for sites with trichloroethene (TCE) contamination of ground water. This site-screening tool combines a causative model for natural attenuation and expert knowledge within a probabilistic causal inference framework known as a Bayesian Belief Network (BBN). The resulting expert system can be used to screen sites and to evaluate evidence about the adequacy of natural attenuation for a site or portion of a site. Comparisons between sites and between experts can help to identify key natural attenuation processes and information needs for more reliable prediction of natural attenuation.,0,1998,0,0.0,1.0,0.0,ngr,1.0,0,0
